{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invictus125/cs598-final-project/blob/main/intraoperative_hypotension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Reproduction of:\n",
        "## Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram\n",
        "\n",
        "Original paper by: Yong-Yeon Jo, Jong-Hwan Jang, Joon-myoung Kwon, Hyung-Chul Lee, Chul-Woo Jung, Seonjeong Byun, Han‐Gil Jeong\n",
        "\n",
        "Reproduction project authored by\n",
        "* Mark Bauer\n",
        "  * mbauer553\n",
        "  * markab5@illinois.edu\n",
        "* Ryan David\n",
        "  * victheone\n",
        "  * invictus125\n",
        "  * radavid2@illinois.edu\n",
        "\n",
        "This project can be found on github [here](https://github.com/invictus125/cs598-final-project).  \n",
        "\n",
        "> Note that this project uses <b>VitalDB, an open biosignal dataset.  All users must agree to the Data Use Agreement below.</b>  If after reviewing the agreement you do not comply, please do not read on and close this window.\n",
        "[Data Use Agreement](https://vitaldb.net/dataset/?query=overview&documentId=13qqajnNZzkN7NZ9aXnaQ-47NWy7kx-a6gbrcEsi-gak&sectionId=h.vcpgs1yemdb5)\n",
        "\n",
        "## Introduction\n",
        "Our project is to perform an approximate reproduction of a paper, which can be found [here](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0272055), on predicting hypotension during surgery from a combination of signals such as mean arterial blood pressure (ABP), electrocardiogram (ECG), and electroencephalogram (EEG) as opposed to ABP alone.  Predicting hypotension is important because it is correlated with many post operation complications, and is actionable.  Please read the original work if you are interested in more detail!\n",
        "\n",
        "## Scope of reproducibility\n",
        "\n",
        "//TODO"
      ],
      "metadata": {
        "id": "lqAswNcRhPzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install torcheval\n",
        "!pip install vitaldb"
      ],
      "metadata": {
        "id": "TFa5j_wCxBHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology - Data\n",
        "\n",
        "Methodology - Data\n",
        "Case: a surgery/operation\n",
        "\n",
        "Track: data observed during a case, consisting of a device and type\n",
        "\n",
        "As of the draft on 2024-04-14 we are getting only ABP data and labels looking ahead one minute.  "
      ],
      "metadata": {
        "id": "i62VgNP7ufup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from functools import partial\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import vitaldb\n",
        "import requests\n",
        "from torch import FloatTensor, BoolTensor\n",
        "\n",
        "SEGEMENT_LENGTH_SECONDS = 60\n",
        "\n",
        "ABP_TRACK = 'SNUADC/ART'\n",
        "ECG_TRACK = 'SNUADC/ECG_II'\n",
        "EEG_TRACK = 'BIS/EEG1_WAV'\n",
        "\n",
        "RELEVANT_TRACKS = [\n",
        "    ABP_TRACK,\n",
        "    ECG_TRACK,\n",
        "    EEG_TRACK,\n",
        "]\n",
        "\n",
        "def _url_to_reader(url_string):\n",
        "    response = requests.get(url_string)\n",
        "    file = StringIO(response.text)\n",
        "    return csv.DictReader(file, delimiter=',')\n",
        "\n",
        "def get_unique_vals(key, iterable):\n",
        "    return set(map(lambda item: item[key], iterable))\n",
        "\n",
        "def case_filter(case):\n",
        "    return float(case['age']) >= 18.0 and case['ane_type'] == 'General'\n",
        "\n",
        "def _case_track_filter(case_id, case_dict):\n",
        "    track_list = case_dict[case_id]['tracks']\n",
        "    return (\n",
        "        ABP_TRACK in track_list and\n",
        "        ECG_TRACK in track_list and\n",
        "        EEG_TRACK in track_list\n",
        "    )\n",
        "\n",
        "def _get_candidate_cases():\n",
        "    cases_by_id = {}\n",
        "    for case in _url_to_reader('https://api.vitaldb.net/cases'):\n",
        "        if case_filter(case):\n",
        "            case['tracks'] = {}\n",
        "            cases_by_id[case['\\ufeffcaseid']] = case\n",
        "\n",
        "    track_list_reader = _url_to_reader('https://api.vitaldb.net/trks')\n",
        "\n",
        "    for track in track_list_reader:\n",
        "        case_id = track['caseid']\n",
        "        if track['tname'] in RELEVANT_TRACKS:\n",
        "            if cases_by_id.get(case_id):\n",
        "                cases_by_id[case_id]['tracks'][track['tname']] = track['tid']\n",
        "\n",
        "    case_track_filter = partial(_case_track_filter, case_dict=cases_by_id)\n",
        "\n",
        "    return [case_id for case_id in filter(case_track_filter, cases_by_id.keys())], cases_by_id\n",
        "\n",
        "def validate_abp_segment(segment):\n",
        "    return (\n",
        "        not np.isnan(segment).mean() > 0.1 and\n",
        "        not (segment > 200).any() and\n",
        "        not (segment < 30).any() and\n",
        "        not ((np.max(segment) - np.min(segment)) < 30) and\n",
        "        not (np.abs(np.diff(segment)) > 30).any() # abrupt changes are assumed to be noise\n",
        "    )\n",
        "\n",
        "def get_data(\n",
        "    minutes_ahead,\n",
        "    abp_and_ecg_sample_rate_per_second=500,\n",
        "    eeg_sample_rate_per_second=128,\n",
        "    max_num_samples=None,\n",
        "    max_num_cases=None,\n",
        "):\n",
        "    candidate_case_ids, _ = _get_candidate_cases()\n",
        "    abps = []\n",
        "    ecgs = []\n",
        "    eegs = []\n",
        "    hypotension_event_bools = []\n",
        "\n",
        "    abp_data_in_two_seconds = 2 * abp_and_ecg_sample_rate_per_second\n",
        "\n",
        "    case_count = 0\n",
        "    for case_id in candidate_case_ids:\n",
        "        case_num_samples = 0\n",
        "        case_num_events = 0\n",
        "\n",
        "        print('Getting track data for case:', case_id)\n",
        "        case_tracks = vitaldb.load_case(int(case_id), RELEVANT_TRACKS[0:2], 1/abp_and_ecg_sample_rate_per_second)\n",
        "\n",
        "        abp_track = case_tracks[:,0]\n",
        "        # ecg_track = case_tracks[:,1]\n",
        "\n",
        "        # eeg_track = vitaldb.load_case(int(case_id), RELEVANT_TRACKS[2], 1/eeg_sample_rate_per_second).flatten()\n",
        "\n",
        "        for i in range(\n",
        "            0,\n",
        "            len(abp_track) - abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + (1 + minutes_ahead) * SEGEMENT_LENGTH_SECONDS),\n",
        "            10 * abp_and_ecg_sample_rate_per_second\n",
        "        ):\n",
        "            x_segment = abp_track[i:i + abp_and_ecg_sample_rate_per_second * SEGEMENT_LENGTH_SECONDS]\n",
        "            y_segment_start = i + abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + minutes_ahead * SEGEMENT_LENGTH_SECONDS)\n",
        "            y_segement_end = i + abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + (minutes_ahead + 1) * SEGEMENT_LENGTH_SECONDS)\n",
        "            y_segment = abp_track[y_segment_start:y_segement_end]\n",
        "\n",
        "            if validate_abp_segment(x_segment) and validate_abp_segment(y_segment):\n",
        "                abps.append(x_segment)\n",
        "\n",
        "                # 2 second moving average\n",
        "                y_numerator = np.nancumsum(y_segment, dtype=np.float32)\n",
        "                y_numerator[abp_data_in_two_seconds:] = y_numerator[abp_data_in_two_seconds:] - y_numerator[:-abp_data_in_two_seconds]\n",
        "                y_moving_avg = y_numerator[abp_data_in_two_seconds - 1:] / abp_data_in_two_seconds\n",
        "\n",
        "                is_hypotension_event = np.nanmax(y_moving_avg) < 65\n",
        "                hypotension_event_bools.append(is_hypotension_event)\n",
        "                case_num_samples = case_num_samples + 1\n",
        "                if(is_hypotension_event):\n",
        "                    case_num_events = case_num_events + 1\n",
        "                print('Valid sample detected, is hypotension event?: ', is_hypotension_event)\n",
        "            else:\n",
        "                print('Invalid sample')\n",
        "\n",
        "            at_max_samples = len(hypotension_event_bools) == max_num_samples\n",
        "            if at_max_samples:\n",
        "                break\n",
        "\n",
        "        case_count = case_count + 1\n",
        "\n",
        "        if at_max_samples or case_count == max_num_cases:\n",
        "            if at_max_samples:\n",
        "                print('Max samples reached')\n",
        "            else:\n",
        "                print('Max cases reached')\n",
        "            break\n",
        "\n",
        "    return (\n",
        "        FloatTensor(np.array(abps)),\n",
        "        FloatTensor(np.array(ecgs)),\n",
        "        FloatTensor(np.array(eegs)),\n",
        "        BoolTensor(np.array(hypotension_event_bools)),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "SXPl5lA6umX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology - Model\n",
        "Our model is an exact reproduction based on the description provided in the original paper.\n",
        "\n",
        "There is a ResNet for each of the three waveform types we handle consisting of:\n",
        "\n",
        "- A CNN encoder layer\n",
        "- 12 residual blocks, each having two convolutions and two batch normalizations. Alternating blocks will halve the length of the data using a max pooling operation\n",
        "- A fully connected output layer which flattens the channels prior to passing through a NN\n",
        "\n",
        "The model is built such that we can provide one or more ResNets and it will adapt. This is so that we can experiment with varying combinations of input data.\n",
        "\n",
        "Once the input is run through the ResNets, their output is concatenated and passed through a fully connected layer which ends with a sigmoid activation, producing the final prediction.\n"
      ],
      "metadata": {
        "id": "DzUkrVE9sERX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, dim_in, kernel_size=15, stride=1):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    padding = math.floor(kernel_size / 2.0)\n",
        "    self.conv = nn.Conv1d(1, 1, kernel_size, stride, padding=padding)\n",
        "    self.mp = nn.MaxPool1d(kernel_size, stride, padding)\n",
        "    self.fc = nn.Linear(dim_in, dim_in)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = self.conv(x)\n",
        "    x_hat = self.mp(x_hat)\n",
        "    return self.fc(x_hat)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    in_channels,\n",
        "    out_channels,\n",
        "    size_down,\n",
        "    kernel_size,\n",
        "    stride=1\n",
        "  ):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.size_down = size_down\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    padding = math.floor(kernel_size / 2.0)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm1d(in_channels)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.do = nn.Dropout()\n",
        "    self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding)\n",
        "    self.bn2 = nn.BatchNorm1d(in_channels)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.mp = nn.MaxPool1d(kernel_size, padding=padding, stride=2)\n",
        "    self.conv_for_input = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = self.bn1(x)\n",
        "    x_hat = self.act1(x_hat)\n",
        "    x_hat = self.do(x_hat)\n",
        "    x_hat = self.conv1(x_hat)\n",
        "    x_hat = self.bn2(x_hat)\n",
        "    x_hat = self.act2(x_hat)\n",
        "    x_hat = self.conv2(x_hat)\n",
        "\n",
        "    # Adjust dimensions of input if needed for the skip connection\n",
        "    x_input = None\n",
        "    if self.in_channels != self.out_channels:\n",
        "      x_input = self.conv_for_input(x)\n",
        "    else:\n",
        "      x_input = x\n",
        "\n",
        "    x_hat = x_hat + x_input\n",
        "\n",
        "    if self.size_down:\n",
        "      x_hat = self.mp(x_hat)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "class FlattenAndLinearBlock(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out):\n",
        "    super(FlattenAndLinearBlock, self).__init__()\n",
        "    self.fc = nn.Linear(dim_in, dim_out)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = torch.flatten(x, start_dim=1, end_dim=-1)\n",
        "    x_hat = self.fc(x_hat)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "class WaveformResNet(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    input_shape,\n",
        "    output_size,\n",
        "    data_type\n",
        "  ):\n",
        "    super(WaveformResNet, self).__init__()\n",
        "    self.encoder = EncoderBlock(input_shape, 15, 1)\n",
        "    self.res_in_dim = input_shape\n",
        "    self.output_size = output_size\n",
        "    self.data_type = data_type\n",
        "\n",
        "    if data_type not in ['abp', 'ecg', 'eeg']:\n",
        "      raise ValueError('Invalid data type. Must be one of [abp, ecg, eeg]')\n",
        "\n",
        "    # Set up configurations for residual blocks\n",
        "    residual_configs = []\n",
        "    linear_block_input_length = -1\n",
        "    if data_type in ['abp', 'ecg']:\n",
        "      residual_configs = [\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 1,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "      ]\n",
        "      linear_block_input_length = 469 * 6\n",
        "    else:\n",
        "      residual_configs = [\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 1,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "      ]\n",
        "      linear_block_input_length = 120 * 6\n",
        "\n",
        "    self.residuals = []\n",
        "    # Build residuals\n",
        "    for i in range(12):\n",
        "      self.residuals.append(\n",
        "        ResidualBlock(\n",
        "          size_down=residual_configs[i]['size_down'],\n",
        "          in_channels=residual_configs[i]['in_channels'],\n",
        "          out_channels=residual_configs[i]['out_channels'],\n",
        "          kernel_size=residual_configs[i]['kernel_size'],\n",
        "        )\n",
        "      )\n",
        "\n",
        "    self.fl_ln = FlattenAndLinearBlock(linear_block_input_length, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = self.encoder(x)\n",
        "\n",
        "    for i in range(len(self.residuals)):\n",
        "      x_hat = self.residuals[i](x_hat)\n",
        "\n",
        "    return self.fl_ln(x_hat)\n",
        "\n",
        "\n",
        "  def get_output_size(self):\n",
        "    return self.output_size\n",
        "\n",
        "\n",
        "class IntraoperativeHypotensionModel(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    ecg_resnet,\n",
        "    abp_resnet,\n",
        "    eeg_resnet\n",
        "  ):\n",
        "    super(IntraoperativeHypotensionModel, self).__init__()\n",
        "\n",
        "    self.ecg = ecg_resnet\n",
        "    self.abp = abp_resnet\n",
        "    self.eeg = eeg_resnet\n",
        "\n",
        "    self.fc_input_length = 0\n",
        "\n",
        "    if self.ecg is not None:\n",
        "      self.fc_input_length += self.ecg.get_output_size()\n",
        "\n",
        "    if self.abp is not None:\n",
        "      self.fc_input_length += self.abp.get_output_size()\n",
        "\n",
        "    if self.eeg is not None:\n",
        "      self.fc_input_length += self.eeg.get_output_size()\n",
        "\n",
        "    if self.fc_input_length == 0:\n",
        "      raise 'No resnet blocks provided, unable to build model'\n",
        "\n",
        "    self.fc1 = nn.Linear(self.fc_input_length, 16)\n",
        "    self.fc2 = nn.Linear(16, 1)\n",
        "    self.act = nn.Sigmoid()\n",
        "\n",
        "    self.seq = nn.Sequential(\n",
        "      self.fc1,\n",
        "      self.fc2,\n",
        "      self.act\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, abp, ecg, eeg):\n",
        "    ecg_o = torch.Tensor([])\n",
        "    abp_o = torch.Tensor([])\n",
        "    eeg_o = torch.Tensor([])\n",
        "\n",
        "    if self.ecg is not None:\n",
        "      ecg_o = self.ecg(ecg)\n",
        "\n",
        "    if self.abp is not None:\n",
        "      abp_o = self.abp(abp)\n",
        "\n",
        "    if self.eeg is not None:\n",
        "      eeg_o = self.eeg(eeg)\n",
        "\n",
        "    resnet_output = torch.concat([ecg_o, abp_o, eeg_o], dim=1)\n",
        "\n",
        "    prediction = self.seq(resnet_output)\n",
        "\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "QsU7Kh2uuGgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Methodology - Training\n",
        "Computational requirements:\n",
        "- At least 50 GB of RAM\n",
        "- GPU instance (we have been experimenting with an A100)\n",
        "\n",
        "The training of this model is fairly straightforward. The paper suggested that we should use Adam as the optimizer and BCE as the loss function, so that is what we have done.\n",
        "\n",
        "Each training epoch will also automatically run evaluation on both the train set and the validation set. See the evaluation methodology for details."
      ],
      "metadata": {
        "id": "WysHaY2luLRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCELoss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _train_one_epoch(\n",
        "  model,\n",
        "  train_data,\n",
        "  optimizer,\n",
        "  criterion\n",
        "):\n",
        "  model.train()\n",
        "  loss_history = []\n",
        "  for data in train_data:\n",
        "    optimizer.zero_grad()\n",
        "    y_hat = model(data.abp, data.ecg, data.eeg)\n",
        "    loss = criterion(torch.squeeze(y_hat, dim=-1), data.y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "  return loss_history\n",
        "\n",
        "\n",
        "def train(\n",
        "  model,\n",
        "  train_data_handle,\n",
        "  test_data_handle,\n",
        "  learning_rate=0.0001,\n",
        "  epochs=100,\n",
        "  suspend_train_epochs_threshold=5\n",
        "):\n",
        "  \"\"\"Trains an IntraoperativeHypotensionModel using the given learning rate for\n",
        "  the given number of epochs\n",
        "\n",
        "  model: the IntraoperativeHypotensionModel to train\n",
        "  train_data_handle: the dataset we will train on\n",
        "  test_data_handle: the dataset we will use for evaluation\n",
        "  learning_rate: the learning rate to use with the Adam optimizer\n",
        "  epochs: the number of epochs to train for\n",
        "  suspend_train_epochs_threshold: training will be suspended if the loss does\n",
        "    not improve for this number of epochs\n",
        "  \"\"\"\n",
        "  if model is None or train_data_handle is None or test_data_handle is None:\n",
        "    raise ValueError(\n",
        "      'model, train_data_handle, and test_data_handle are required for training'\n",
        "    )\n",
        "\n",
        "  criterion = BCELoss()\n",
        "  optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  overall_loss_history = []\n",
        "  consecutive_epochs_without_improvement = 0\n",
        "  for epoch in range(epochs):\n",
        "    print('====================================')\n",
        "    print(f'     Epoch #{epoch + 1}')\n",
        "    print('====================================')\n",
        "    loss_history = _train_one_epoch(\n",
        "      model,\n",
        "      train_data_handle,\n",
        "      optimizer,\n",
        "      criterion\n",
        "    )\n",
        "    eval_model(model, train_data_handle, 'Train')\n",
        "    # Not using performance metrics yet in this function.\n",
        "    # Potential TODO: stop training once desired performance is reached (TBD)\n",
        "    performance = eval_model(model, test_data_handle, 'Test')\n",
        "\n",
        "    if epoch > 0:\n",
        "      mean_loss = np.mean(loss_history)\n",
        "      overall_loss_history.append(mean_loss)\n",
        "      loss_change = overall_loss_history[epoch - 1] - mean_loss\n",
        "      if loss_change < 0.1:\n",
        "        consecutive_epochs_without_improvement += 1\n",
        "      else:\n",
        "        consecutive_epochs_without_improvement = 0\n",
        "\n",
        "    if consecutive_epochs_without_improvement >= suspend_train_epochs_threshold:\n",
        "      print(f'Training stopping after {epoch+1} epochs.')\n",
        "      print(f'Loss did not change for {suspend_train_epochs_threshold} epochs')\n",
        "      break"
      ],
      "metadata": {
        "id": "r_CMSJdxt3Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Methodology - Evaluation\n",
        "The original paper uses four metrics:\n",
        "- AUROC\n",
        "- AUPRC\n",
        "- Sensitivity\n",
        "- Specificity\n",
        "\n",
        "We chose to use the torcheval library for our metrics, except for binary specificity which did not appear to be present in torcheval.\n",
        "\n",
        "In light of that, we have implemented our own binary specificity. Regrettably, this does not yet work properly, but we will do our best to get it functional before our final submission.\n",
        "\n"
      ],
      "metadata": {
        "id": "AUrbzCCLttmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torcheval.metrics import BinaryAUROC, BinaryAUPRC, BinaryRecall\n",
        "\n",
        "\n",
        "def _binary_specificity(test, target):\n",
        "  # TN / TN + FP\n",
        "  pinned = torch.where(test >= 0.5, 1.0, 0.0)\n",
        "  pos = torch.where(pinned > 0, 1.0, 0.0)\n",
        "  neg = torch.where(pinned < 1, 1.0, 0.0)\n",
        "  gt_pos = torch.where(target > 0, 1.0, 0.0)\n",
        "  gt_neg = torch.where(target < 1, 1.0, 0.0)\n",
        "  tn = neg + gt_neg\n",
        "  tn = torch.sum(torch.where(tn > 1, 1.0, 0.0), dtype=torch.float)\n",
        "  fp = pos + gt_neg\n",
        "  fp = torch.sum(torch.where(fp > 1, 1.0, 0.0), dtype=torch.float)\n",
        "\n",
        "  return (tn / (tn + fp))\n",
        "\n",
        "\n",
        "def eval_model(\n",
        "  model,\n",
        "  eval_data,\n",
        "  dataset_name\n",
        "):\n",
        "  model.eval()\n",
        "\n",
        "  auroc = []\n",
        "  auprc = []\n",
        "  sensitivity = []\n",
        "  specificity = []\n",
        "\n",
        "  f_auroc = BinaryAUROC()\n",
        "  f_auprc = BinaryAUPRC()\n",
        "  f_sensitivity = BinaryRecall()\n",
        "  for data in eval_data:\n",
        "    y_hat = model(data.abp, data.ecg, data.eeg)\n",
        "    y_hat = y_hat.squeeze(-1)\n",
        "    y_hat_long = torch.where(y_hat >= 0.5, 1.0, 0.0).long()\n",
        "    target_long = data.y.long()\n",
        "\n",
        "    f_auroc.update(y_hat, data.y)\n",
        "    f_auprc.update(y_hat, data.y)\n",
        "    f_sensitivity.update(y_hat_long, target_long)\n",
        "\n",
        "    auroc.append(f_auroc.compute())\n",
        "    auprc.append(f_auprc.compute())\n",
        "    sensitivity.append(f_sensitivity.compute())\n",
        "    specificity.append(_binary_specificity(y_hat, data.y))\n",
        "\n",
        "  m_auroc = np.mean(auroc)\n",
        "  m_auprc = np.mean(auprc)\n",
        "  m_sensitivity = np.mean(sensitivity)\n",
        "  m_specificity = np.mean(specificity)\n",
        "\n",
        "  print(f'    {dataset_name} data metrics:')\n",
        "  print(f'        AUROC: {m_auroc}')\n",
        "  print(f'        AUPRC: {m_auprc}')\n",
        "  print(f'        Sensitivity: {m_sensitivity}')\n",
        "  print(f'        Specificity: {m_specificity}')\n",
        "\n",
        "  return m_auroc, m_auprc, m_sensitivity, m_specificity"
      ],
      "metadata": {
        "id": "jh1fdvuctwgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Results\n",
        "* Results\n",
        "* Analyses\n",
        "* Plans\n"
      ],
      "metadata": {
        "id": "OTM5yt6WtOdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "Jo YY, Jang JH, Kwon Jm, Lee HC, Jung CW, et al. (2022) Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram: Retrospective study. PLOS ONE 17(8): e0272055. https://doi.org/10.1371/journal.pone.0272055\n",
        "\n",
        "## Acknowledgements\n",
        "* As mentioned in the introduction, this project leveraged the open [vitaldb dataset](https://vitaldb.net/dataset/), and without it would have been impossible in its current form.\n",
        "* Significant inspiration was drawn from [vital db examples](https://github.com/vitaldb/examples)"
      ],
      "metadata": {
        "id": "XFaXjyRytA24"
      }
    }
  ]
}
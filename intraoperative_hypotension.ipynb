{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invictus125/cs598-final-project/blob/main/intraoperative_hypotension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqAswNcRhPzL"
      },
      "source": [
        "# A Reproduction of:\n",
        "## Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram\n",
        "\n",
        "Original paper by: Yong-Yeon Jo, Jong-Hwan Jang, Joon-myoung Kwon, Hyung-Chul Lee, Chul-Woo Jung, Seonjeong Byun, Han‐Gil Jeong\n",
        "\n",
        "Reproduction project authored by\n",
        "* Mark Bauer\n",
        "  * mbauer553\n",
        "  * markab5@illinois.edu\n",
        "* Ryan David\n",
        "  * victheone\n",
        "  * invictus125\n",
        "  * radavid2@illinois.edu\n",
        "\n",
        "This project can be found on github https://github.com/invictus125/cs598-final-project.  \n",
        "\n",
        "> Note that this project uses <b>VitalDB, an open biosignal dataset.  All users must agree to the Data Use Agreement below.</b>  If after reviewing the agreement you do not comply, please do not read on and close this window.\n",
        "[Data Use Agreement](https://vitaldb.net/dataset/?query=overview&documentId=13qqajnNZzkN7NZ9aXnaQ-47NWy7kx-a6gbrcEsi-gak&sectionId=h.vcpgs1yemdb5)\n",
        "\n",
        "## Introduction\n",
        "Our project is to perform an approximate reproduction of a paper, which can be found [here](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0272055), on predicting hypotension during surgery from a combination of signals such as mean arterial blood pressure (ABP), electrocardiogram (ECG), and electroencephalogram (EEG) as opposed to ABP alone.  Predicting hypotension is important because it is correlated with many post operation complications, and is actionable.  Please read the original work if you are interested in more detail!\n",
        "\n",
        "## Scope of reproducibility\n",
        "\n",
        "As of the draft on 2024-04-14, we are using what we understand to be a very close replication of the model, with a smaller openly available data set.  Our model currently executes utilizing ABP data from the dataset for predictors and labels, but the EEG and ECG data is randomly created mock data.  The original work also examined more look ahead times.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFa5j_wCxBHy",
        "outputId": "fc6b3ba6-47bd-44d9-9bb1-54464f35d397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torcheval\n",
            "  Downloading torcheval-0.0.7-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m143.4/179.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.11.0)\n",
            "Installing collected packages: torcheval\n",
            "Successfully installed torcheval-0.0.7\n",
            "Collecting vitaldb\n",
            "  Downloading vitaldb-1.4.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vitaldb) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.31.0)\n",
            "Collecting wfdb (from vitaldb)\n",
            "  Downloading wfdb-4.1.2-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.0/160.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2024.2.2)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->vitaldb) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb->vitaldb) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb->vitaldb) (2.22)\n",
            "Installing collected packages: wfdb, vitaldb\n",
            "Successfully installed vitaldb-1.4.7 wfdb-4.1.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install torcheval\n",
        "!pip install vitaldb\n",
        "\n",
        "# mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i62VgNP7ufup"
      },
      "source": [
        "## Methodology - Data\n",
        "\n",
        "Methodology - Data\n",
        "Case: a surgery/operation\n",
        "\n",
        "Track: data observed during a case, consisting of a device and type\n",
        "\n",
        "As of the draft on 2024-04-14 we are getting only ABP data and labels looking ahead one minute.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SXPl5lA6umX-"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from io import StringIO\n",
        "from os import listdir\n",
        "from torch import FloatTensor, BoolTensor\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import requests\n",
        "import vitaldb\n",
        "\n",
        "SEGEMENT_LENGTH_SECONDS = 60\n",
        "\n",
        "ABP_TRACK = 'SNUADC/ART'\n",
        "ECG_TRACK = 'SNUADC/ECG_II'\n",
        "EEG_TRACK = 'BIS/EEG1_WAV'\n",
        "\n",
        "RELEVANT_TRACKS = [\n",
        "    ABP_TRACK,\n",
        "    ECG_TRACK,\n",
        "    EEG_TRACK,\n",
        "]\n",
        "\n",
        "def _url_to_reader(url_string):\n",
        "    response = requests.get(url_string)\n",
        "    file = StringIO(response.text)\n",
        "    return csv.DictReader(file, delimiter=',')\n",
        "\n",
        "def get_unique_vals(key, iterable):\n",
        "    return set(map(lambda item: item[key], iterable))\n",
        "\n",
        "def case_filter(case):\n",
        "    return float(case['age']) >= 18.0 and case['ane_type'] == 'General'\n",
        "\n",
        "def _case_track_filter(case_id, case_dict):\n",
        "    track_list = case_dict[case_id]['tracks']\n",
        "    return (\n",
        "        ABP_TRACK in track_list and\n",
        "        ECG_TRACK in track_list and\n",
        "        EEG_TRACK in track_list\n",
        "    )\n",
        "\n",
        "def _get_candidate_cases():\n",
        "    cases_by_id = {}\n",
        "    for case in _url_to_reader('https://api.vitaldb.net/cases'):\n",
        "        if case_filter(case):\n",
        "            case['tracks'] = {}\n",
        "            cases_by_id[case['\\ufeffcaseid']] = case\n",
        "\n",
        "    track_list_reader = _url_to_reader('https://api.vitaldb.net/trks')\n",
        "\n",
        "    for track in track_list_reader:\n",
        "        case_id = track['caseid']\n",
        "        if track['tname'] in RELEVANT_TRACKS:\n",
        "            if cases_by_id.get(case_id):\n",
        "                cases_by_id[case_id]['tracks'][track['tname']] = track['tid']\n",
        "\n",
        "    case_track_filter = partial(_case_track_filter, case_dict=cases_by_id)\n",
        "\n",
        "    return [case_id for case_id in filter(case_track_filter, cases_by_id.keys())]\n",
        "\n",
        "def _get_candidate_cases_from_dir(dir_path):\n",
        "    return [f.split('.vital')[0] for f in listdir(dir_path) if '.vital' in f]\n",
        "\n",
        "def _download_vital_file(case_id):\n",
        "    vf = vitaldb.VitalFile(int(case_id), RELEVANT_TRACKS)\n",
        "    vf.to_vital(case_id+'.vital')\n",
        "\n",
        "def _get_tracks_from_vital_file(path, tracks, sample_rate):\n",
        "    vf = vitaldb.read_vital(path, tracks)\n",
        "    return vf.to_numpy(tracks, sample_rate)\n",
        "\n",
        "\n",
        "def shuffle_tensors(tensors):\n",
        "  shuffled_output = []\n",
        "  shuffled_index = np.array([i for i in range(0, len(tensors[0]))]).astype(int)\n",
        "  np.random.shuffle(shuffled_index)\n",
        "  for t in tensors:\n",
        "    if len(t) > 0:\n",
        "      new_tensor = t.detach().clone()[shuffled_index]\n",
        "      shuffled_output.append(new_tensor.squeeze(0))\n",
        "    else:\n",
        "      shuffled_output.append(t)\n",
        "\n",
        "  return shuffled_output\n",
        "\n",
        "\n",
        "def validate_abp_segment(segment):\n",
        "\n",
        "    return (\n",
        "        not np.isnan(segment).any() and\n",
        "        not (segment > 200).any() and\n",
        "        not (segment < 30).any() and\n",
        "        not ((np.max(segment) - np.min(segment)) < 30) and\n",
        "        not (np.abs(np.diff(segment)) > 30).any() # abrupt changes are assumed to be noise\n",
        "    )\n",
        "\n",
        "def download_data(num_requested_cases):\n",
        "    num_downloaded_cases = 0\n",
        "    candidate_case_ids = _get_candidate_cases()\n",
        "\n",
        "    np.random.shuffle(candidate_case_ids)\n",
        "    for case_id in candidate_case_ids:\n",
        "        print('Downloading case:', case_id)\n",
        "        _download_vital_file(case_id)\n",
        "        num_downloaded_cases = num_downloaded_cases + 1\n",
        "        at_requested = num_downloaded_cases == num_requested_cases\n",
        "        if at_requested:\n",
        "            break\n",
        "\n",
        "    if not at_requested:\n",
        "        print('Requsted cases not reached but all available cases exhausted.  ')\n",
        "\n",
        "def get_data(\n",
        "    minutes_ahead,\n",
        "    abp_and_ecg_sample_rate_per_second=500,\n",
        "    eeg_sample_rate_per_second=128,\n",
        "    case_ids=None,\n",
        "    max_num_samples=None,\n",
        "    max_num_cases=None,\n",
        "    from_dir=None,\n",
        "    shuffle_samples=False,\n",
        "):\n",
        "    if case_ids is not None:\n",
        "        candidate_case_ids = case_ids\n",
        "    elif from_dir is None:\n",
        "        candidate_case_ids = _get_candidate_cases()\n",
        "    else:\n",
        "        candidate_case_ids = _get_candidate_cases_from_dir(from_dir)\n",
        "\n",
        "    abps = []\n",
        "    ecgs = []\n",
        "    eegs = []\n",
        "    hypotension_event_bools = []\n",
        "\n",
        "    abp_data_in_two_seconds = 2 * abp_and_ecg_sample_rate_per_second\n",
        "\n",
        "    at_max = False\n",
        "\n",
        "    case_count = 0\n",
        "    np.random.shuffle(candidate_case_ids)\n",
        "    for case_id in candidate_case_ids:\n",
        "        case_num_samples = 0\n",
        "        case_num_events = 0\n",
        "\n",
        "        print('Getting track data for case:', case_id)\n",
        "        if from_dir is None:\n",
        "            case_tracks = vitaldb.load_case(int(case_id), RELEVANT_TRACKS[0:2], 1/abp_and_ecg_sample_rate_per_second)\n",
        "        else:\n",
        "            case_tracks = _get_tracks_from_vital_file(f\"{from_dir}/{case_id}.vital\", RELEVANT_TRACKS[0:2], 1/abp_and_ecg_sample_rate_per_second)\n",
        "\n",
        "        abp_track = case_tracks[:,0]\n",
        "        # ecg_track = case_tracks[:,1]\n",
        "\n",
        "        # eeg_track = vitaldb.load_case(int(case_id), RELEVANT_TRACKS[2], 1/eeg_sample_rate_per_second).flatten()\n",
        "\n",
        "        for i in range(\n",
        "            0,\n",
        "            len(abp_track) - abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + (1 + minutes_ahead) * SEGEMENT_LENGTH_SECONDS),\n",
        "            10 * abp_and_ecg_sample_rate_per_second\n",
        "        ):\n",
        "            x_segment = abp_track[i:i + abp_and_ecg_sample_rate_per_second * SEGEMENT_LENGTH_SECONDS]\n",
        "            y_segment_start = i + abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + minutes_ahead * SEGEMENT_LENGTH_SECONDS)\n",
        "            y_segement_end = i + abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + (minutes_ahead + 1) * SEGEMENT_LENGTH_SECONDS)\n",
        "            y_segment = abp_track[y_segment_start:y_segement_end]\n",
        "\n",
        "            if validate_abp_segment(x_segment) and validate_abp_segment(y_segment):\n",
        "                abps.append(x_segment)\n",
        "\n",
        "                # 2 second moving average\n",
        "                y_numerator = np.nancumsum(y_segment, dtype=np.float32)\n",
        "                y_numerator[abp_data_in_two_seconds:] = y_numerator[abp_data_in_two_seconds:] - y_numerator[:-abp_data_in_two_seconds]\n",
        "                y_moving_avg = y_numerator[abp_data_in_two_seconds - 1:] / abp_data_in_two_seconds\n",
        "\n",
        "                is_hypotension_event = np.nanmax(y_moving_avg) < 65\n",
        "                hypotension_event_bools.append(is_hypotension_event)\n",
        "                case_num_samples = case_num_samples + 1\n",
        "                if(is_hypotension_event):\n",
        "                    case_num_events = case_num_events + 1\n",
        "\n",
        "            at_max_samples = len(hypotension_event_bools) == max_num_samples\n",
        "            if at_max_samples:\n",
        "                break\n",
        "\n",
        "        case_count = case_count + 1\n",
        "        print(f\"Statistics for case: {case_id}, {case_num_samples} total valid samples, {case_num_events} positive samples\")\n",
        "\n",
        "        if at_max_samples or case_count == max_num_cases:\n",
        "            if at_max_samples:\n",
        "                print('Max samples reached')\n",
        "            else:\n",
        "                print('Max cases reached')\n",
        "            at_max = True\n",
        "            break\n",
        "\n",
        "    if (max_num_samples is not None or max_num_cases is not None) and not at_max:\n",
        "        print('Max not reached but all available cases exhausted.  ')\n",
        "\n",
        "    print('Converting and shuffling (if enabled)')\n",
        "\n",
        "    abps = FloatTensor(abps)\n",
        "    ecgs = FloatTensor(ecgs)\n",
        "    eegs = FloatTensor(eegs)\n",
        "    hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n",
        "\n",
        "    if shuffle_samples:\n",
        "      output_tensors = shuffle_tensors([abps, ecgs, eegs, hypotension_event_bools])\n",
        "    else:\n",
        "      output_tensors = [abps, ecgs, eegs, hypotension_event_bools]\n",
        "\n",
        "    return (\n",
        "        output_tensors[0].unsqueeze(1),\n",
        "        output_tensors[1].unsqueeze(1),\n",
        "        output_tensors[2].unsqueeze(1),\n",
        "        output_tensors[3]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUkrVE9sERX"
      },
      "source": [
        "## Methodology - Model\n",
        "Our model is an exact reproduction based on the description provided in the original paper.\n",
        "\n",
        "There is a ResNet for each of the three waveform types we handle consisting of:\n",
        "\n",
        "- A CNN encoder layer\n",
        "- 12 residual blocks, each having two convolutions and two batch normalizations. Alternating blocks will halve the length of the data using a max pooling operation. Per the paper, we also added skip connections by summing the input into the output in each residual block.\n",
        "- A fully connected output layer which flattens the channels prior to passing through a NN\n",
        "\n",
        "The model is built such that we can provide one or more ResNets and it will adapt. This is so that we can experiment with varying combinations of input data.\n",
        "\n",
        "Once the input is run through the ResNets, their output is concatenated and passed through a fully connected layer which ends with a sigmoid activation, producing the final prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QsU7Kh2uuGgw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, dim_in, kernel_size=15, stride=1):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    padding = math.floor(kernel_size / 2.0)\n",
        "    self.conv = nn.Conv1d(1, 1, kernel_size, stride, padding=padding)\n",
        "    self.mp = nn.MaxPool1d(kernel_size, stride, padding)\n",
        "    self.fc = nn.Linear(dim_in, dim_in)\n",
        "    torch.nn.init.normal_(self.fc.weight, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = self.conv(x)\n",
        "    x_hat = self.mp(x_hat)\n",
        "    return self.fc(x_hat)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    in_channels,\n",
        "    out_channels,\n",
        "    size_down,\n",
        "    kernel_size,\n",
        "    stride=1\n",
        "  ):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.size_down = size_down\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    padding = math.floor(kernel_size / 2.0)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm1d(in_channels)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.do = nn.Dropout()\n",
        "    self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding)\n",
        "    self.bn2 = nn.BatchNorm1d(in_channels)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.mp = nn.MaxPool1d(kernel_size, padding=padding, stride=2)\n",
        "    self.conv_for_input = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = self.bn1(x)\n",
        "    x_hat = self.act1(x_hat)\n",
        "    x_hat = self.do(x_hat)\n",
        "    x_hat = self.conv1(x_hat)\n",
        "    x_hat = self.bn2(x_hat)\n",
        "    x_hat = self.act2(x_hat)\n",
        "    x_hat = self.conv2(x_hat)\n",
        "\n",
        "    # Adjust dimensions of input if needed for the skip connection\n",
        "    x_input = None\n",
        "    if self.in_channels != self.out_channels:\n",
        "      x_input = self.conv_for_input(x)\n",
        "    else:\n",
        "      x_input = x\n",
        "\n",
        "    x_hat = x_hat + x_input\n",
        "\n",
        "    if self.size_down:\n",
        "      x_hat = self.mp(x_hat)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "class FlattenAndLinearBlock(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out):\n",
        "    super(FlattenAndLinearBlock, self).__init__()\n",
        "    self.fc = nn.Linear(dim_in, dim_out)\n",
        "    torch.nn.init.normal_(self.fc.weight, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = torch.flatten(x, start_dim=1, end_dim=-1)\n",
        "    x_hat = self.fc(x_hat)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "class WaveformResNet(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    input_shape,\n",
        "    output_size,\n",
        "    data_type\n",
        "  ):\n",
        "    super(WaveformResNet, self).__init__()\n",
        "    self.encoder = EncoderBlock(input_shape, 15, 1)\n",
        "    self.res_in_dim = input_shape\n",
        "    self.output_size = output_size\n",
        "    self.data_type = data_type\n",
        "\n",
        "    if data_type not in ['abp', 'ecg', 'eeg']:\n",
        "      raise ValueError('Invalid data type. Must be one of [abp, ecg, eeg]')\n",
        "\n",
        "    # Set up configurations for residual blocks\n",
        "    residual_configs = []\n",
        "    linear_block_input_length = -1\n",
        "    if data_type in ['abp', 'ecg']:\n",
        "      residual_configs = [\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 1,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "      ]\n",
        "      linear_block_input_length = 469 * 6\n",
        "    else:\n",
        "      residual_configs = [\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 1,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "      ]\n",
        "      linear_block_input_length = 120 * 6\n",
        "\n",
        "    self.residuals = []\n",
        "    # Build residuals\n",
        "    for i in range(12):\n",
        "      self.residuals.append(\n",
        "        ResidualBlock(\n",
        "          size_down=residual_configs[i]['size_down'],\n",
        "          in_channels=residual_configs[i]['in_channels'],\n",
        "          out_channels=residual_configs[i]['out_channels'],\n",
        "          kernel_size=residual_configs[i]['kernel_size'],\n",
        "        )\n",
        "      )\n",
        "\n",
        "    self.fl_ln = FlattenAndLinearBlock(linear_block_input_length, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x_hat = self.encoder(x)\n",
        "    x_hat = x\n",
        "\n",
        "    for i in range(len(self.residuals)):\n",
        "      x_hat = self.residuals[i](x_hat)\n",
        "\n",
        "    out = self.fl_ln(x_hat)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "  def get_output_size(self):\n",
        "    return self.output_size\n",
        "\n",
        "\n",
        "class IntraoperativeHypotensionModel(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    ecg_resnet=None,\n",
        "    abp_resnet=None,\n",
        "    eeg_resnet=None\n",
        "  ):\n",
        "    super(IntraoperativeHypotensionModel, self).__init__()\n",
        "\n",
        "    self.ecg = ecg_resnet\n",
        "    self.abp = abp_resnet\n",
        "    self.eeg = eeg_resnet\n",
        "\n",
        "    self.fc_input_length = 0\n",
        "\n",
        "    if self.ecg is not None:\n",
        "      self.fc_input_length += self.ecg.get_output_size()\n",
        "\n",
        "    if self.abp is not None:\n",
        "      self.fc_input_length += self.abp.get_output_size()\n",
        "\n",
        "    if self.eeg is not None:\n",
        "      self.fc_input_length += self.eeg.get_output_size()\n",
        "\n",
        "    if self.fc_input_length == 0:\n",
        "      raise 'No resnet blocks provided, unable to build model'\n",
        "\n",
        "    self.fc1 = nn.Linear(self.fc_input_length, 16)\n",
        "    self.fc2 = nn.Linear(16, 1)\n",
        "    self.act = nn.Sigmoid()\n",
        "    torch.nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n",
        "    torch.nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "  def forward(self, abp, ecg, eeg):\n",
        "    ecg_o = torch.Tensor([])\n",
        "    abp_o = torch.Tensor([])\n",
        "    eeg_o = torch.Tensor([])\n",
        "\n",
        "    if self.ecg is not None:\n",
        "      ecg_o = self.ecg(ecg)\n",
        "\n",
        "    if self.abp is not None:\n",
        "      abp_o = self.abp(abp)\n",
        "\n",
        "    if self.eeg is not None:\n",
        "      eeg_o = self.eeg(eeg)\n",
        "\n",
        "    resnet_output = torch.concat([ecg_o, abp_o, eeg_o], dim=1)\n",
        "\n",
        "    intermediate = self.fc1(resnet_output)\n",
        "    intermediate = self.fc2(intermediate)\n",
        "\n",
        "    prediction = self.act(intermediate)\n",
        "\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WysHaY2luLRS"
      },
      "source": [
        "## Methodology - Training\n",
        "Computational requirements:\n",
        "- At least 50 GB of RAM\n",
        "- GPU instance (we have been experimenting with an A100)\n",
        "\n",
        "The training of this model is fairly straightforward. The paper suggested that we should use Adam as the optimizer and BCE as the loss function, so that is what we have done.\n",
        "\n",
        "Each training epoch will also automatically run evaluation on both the train set and the validation set. See the evaluation methodology for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r_CMSJdxt3Fp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCELoss\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "def _extract_batch(data, batch_size, batch_number):\n",
        "  start = batch_size * batch_number\n",
        "  end = start + batch_size\n",
        "\n",
        "  if start >= len(data[3]):\n",
        "    return None\n",
        "\n",
        "  return [\n",
        "      data[0][start:end] if len(data[0]) > 0 else None,\n",
        "      data[1][start:end] if len(data[1]) > 0 else None,\n",
        "      data[2][start:end] if len(data[2]) > 0 else None,\n",
        "      data[3][start:end]\n",
        "  ]\n",
        "\n",
        "\n",
        "def _write_model_checkpoint(model, out_dir, epoch, model_title, performance):\n",
        "  dir = f'{out_dir}/{model_title}/{epoch}'\n",
        "  !mkdir -p {dir}\n",
        "  model_path = f'{dir}/model'\n",
        "  perf_path = f'{dir}/perf.txt'\n",
        "  torch.save(model.state_dict(), model_path)\n",
        "  perf_file = open(perf_path, 'w')\n",
        "  perf_file.write(f'AUROC: {performance[0]}\\nAUPRC: {performance[1]}\\nSensitivity: {performance[2]}\\nSpecificity: {performance[3]}')\n",
        "  perf_file.close()\n",
        "\n",
        "\n",
        "def _train_one_epoch(\n",
        "  model,\n",
        "  train_data,\n",
        "  optimizer,\n",
        "  criterion,\n",
        "  batch_size=32\n",
        "):\n",
        "  model.train()\n",
        "  loss_history = []\n",
        "\n",
        "  batch_num = 0\n",
        "  batch = _extract_batch(train_data, batch_size, batch_num)\n",
        "  while batch is not None:\n",
        "    optimizer.zero_grad()\n",
        "    abp = batch[0]\n",
        "    ecg = batch[1]\n",
        "    eeg = batch[2]\n",
        "    y = batch[3]\n",
        "    y_hat = model(abp, ecg, eeg)\n",
        "    y_hat = torch.squeeze(y_hat, dim=-1)\n",
        "    loss = criterion(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_loss = loss.item()\n",
        "    # print(f'\\tBatch {batch_num} loss: {batch_loss}')\n",
        "    loss_history.append(batch_loss)\n",
        "\n",
        "    batch_num += 1\n",
        "    batch = _extract_batch(train_data, batch_size, batch_num)\n",
        "\n",
        "  return loss_history\n",
        "\n",
        "\n",
        "def train(\n",
        "  model,\n",
        "  train_data_handle,\n",
        "  test_data_handle,\n",
        "  learning_rate=0.0001,\n",
        "  epochs=100,\n",
        "  suspend_train_epochs_threshold=5,\n",
        "  batch_size=32,\n",
        "  output_dir=None,\n",
        "  model_title='ioht_model'\n",
        "):\n",
        "  \"\"\"Trains an IntraoperativeHypotensionModel using the given learning rate for\n",
        "  the given number of epochs\n",
        "\n",
        "  model: the IntraoperativeHypotensionModel to train\n",
        "  train_data_handle: the dataset we will train on\n",
        "  test_data_handle: the dataset we will use for evaluation\n",
        "  learning_rate: the learning rate to use with the Adam optimizer\n",
        "  epochs: the number of epochs to train for\n",
        "  suspend_train_epochs_threshold: training will be suspended if the loss does\n",
        "    not improve for this number of epochs\n",
        "  batch_size: the size of batches to train on\n",
        "  output_dir: the path to an output directory\n",
        "  model_title: the title of the model to use when writing checkpoints\n",
        "  \"\"\"\n",
        "  if model is None or train_data_handle is None or test_data_handle is None:\n",
        "    raise ValueError(\n",
        "      'model, train_data_handle, and test_data_handle are required for training'\n",
        "    )\n",
        "\n",
        "  criterion = BCELoss()\n",
        "  optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  overall_loss_history = []\n",
        "  consecutive_epochs_without_improvement = 0\n",
        "  for epoch in range(epochs):\n",
        "    print('====================================')\n",
        "    print(f'     Epoch #{epoch + 1}')\n",
        "    print('====================================')\n",
        "\n",
        "    loss_history = _train_one_epoch(\n",
        "      model,\n",
        "      train_data_handle,\n",
        "      optimizer,\n",
        "      criterion,\n",
        "      batch_size\n",
        "    )\n",
        "    eval_model(model, train_data_handle, 'Train', batch_size)\n",
        "    # Not using performance metrics yet in this function.\n",
        "    # Potential TODO: stop training once desired performance is reached (TBD)\n",
        "    performance = eval_model(model, test_data_handle, 'Test', batch_size)\n",
        "\n",
        "    if output_dir is not None and (epoch % 5) == 0:\n",
        "      # Write a model checkpoint every 5 epochs (model checkpoints are large)\n",
        "      _write_model_checkpoint(\n",
        "        model,\n",
        "        output_dir,\n",
        "        epoch,\n",
        "        model_title,\n",
        "        performance\n",
        "      )\n",
        "\n",
        "    if epoch > 0:\n",
        "      mean_loss = np.mean(loss_history)\n",
        "      overall_loss_history.append(mean_loss)\n",
        "      loss_change = overall_loss_history[epoch - 1] - mean_loss\n",
        "      print(f'Mean loss: {mean_loss}')\n",
        "      # print(f'Loss change: {loss_change}')\n",
        "      if loss_change < 0.01:\n",
        "        consecutive_epochs_without_improvement += 1\n",
        "      else:\n",
        "        consecutive_epochs_without_improvement = 0\n",
        "      # print(f'Epochs without loss improvement: {consecutive_epochs_without_improvement}')\n",
        "\n",
        "    # if consecutive_epochs_without_improvement >= suspend_train_epochs_threshold:\n",
        "    #   print(f'Training stopping after {epoch+1} epochs.')\n",
        "    #   print(f'Loss did not change for {suspend_train_epochs_threshold} epochs')\n",
        "    #   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUrbzCCLttmN"
      },
      "source": [
        "##  Methodology - Evaluation\n",
        "The original paper uses four metrics:\n",
        "- AUROC\n",
        "- AUPRC\n",
        "- Sensitivity\n",
        "- Specificity\n",
        "\n",
        "We chose to use the torcheval library for our metrics, except for binary specificity which did not appear to be present in torcheval.\n",
        "\n",
        "In light of that, we have implemented our own binary specificity. Regrettably, this does not yet work properly, but we will do our best to get it functional before our final submission.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jh1fdvuctwgX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torcheval.metrics import BinaryAUROC, BinaryAUPRC, BinaryRecall\n",
        "\n",
        "\n",
        "def _binary_specificity(test, target):\n",
        "  # TN / TN + FP\n",
        "  pinned = torch.where(test >= 0.5, 1.0, 0.0)\n",
        "  pos = torch.where(pinned > 0, 1.0, 0.0)\n",
        "  neg = torch.where(pinned < 1, 1.0, 0.0)\n",
        "  gt_pos = torch.where(target > 0, 1.0, 0.0)\n",
        "  gt_neg = torch.where(target < 1, 1.0, 0.0)\n",
        "  tn = neg + gt_neg\n",
        "  tn = torch.sum(torch.where(tn > 1, 1.0, 0.0), dtype=torch.float)\n",
        "  fp = pos + gt_neg\n",
        "  fp = torch.sum(torch.where(fp > 1, 1.0, 0.0), dtype=torch.float)\n",
        "\n",
        "  return (tn / (tn + fp))\n",
        "\n",
        "\n",
        "def eval_model(\n",
        "  model,\n",
        "  eval_data,\n",
        "  dataset_name,\n",
        "  batch_size=32\n",
        "):\n",
        "  # Reduce memory usage during eval\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "\n",
        "    # auroc = []\n",
        "    # auprc = []\n",
        "    # sensitivity = []\n",
        "    specificity = []\n",
        "\n",
        "    f_auroc = BinaryAUROC()\n",
        "    f_auprc = BinaryAUPRC()\n",
        "    f_sensitivity = BinaryRecall()\n",
        "\n",
        "    batch_num = 0\n",
        "    batch = _extract_batch(eval_data, batch_size, batch_num)\n",
        "    while batch is not None:\n",
        "      abp = batch[0]\n",
        "      ecg = batch[1]\n",
        "      eeg = batch[2]\n",
        "      y = batch[3]\n",
        "\n",
        "      y_hat = model(abp, ecg, eeg)\n",
        "      y_hat = y_hat.squeeze(-1)\n",
        "\n",
        "      y_hat_long = torch.where(y_hat >= 0.5, 1.0, 0.0).long()\n",
        "      target_long = y.long()\n",
        "\n",
        "      # print(f'y_hat_long sum: {y_hat_long.sum()}, target_long sum: {target_long.sum()}')\n",
        "\n",
        "      f_auroc.update(y_hat, y)\n",
        "      f_auprc.update(y_hat, y)\n",
        "      f_sensitivity.update(y_hat_long, target_long)\n",
        "\n",
        "      # auroc.append(f_auroc.compute())\n",
        "      # auprc.append(f_auprc.compute())\n",
        "      # sensitivity.append(f_sensitivity.compute())\n",
        "      specificity.append(_binary_specificity(y_hat, y))\n",
        "\n",
        "      batch_num += 1\n",
        "      batch = _extract_batch(eval_data, batch_size, batch_num)\n",
        "\n",
        "    # m_auroc = np.mean(auroc)\n",
        "    # m_auprc = np.mean(auprc)\n",
        "    # m_sensitivity = np.mean(sensitivity)\n",
        "    # m_specificity = np.mean(specificity)\n",
        "    m_auroc = f_auroc.compute()\n",
        "    m_auprc = f_auprc.compute()\n",
        "    m_sensitivity = f_sensitivity.compute()\n",
        "    m_specificity = np.mean(specificity)\n",
        "\n",
        "    print(f'    {dataset_name} data metrics:')\n",
        "    print(f'        AUROC: {m_auroc}')\n",
        "    print(f'        AUPRC: {m_auprc}')\n",
        "    print(f'        Sensitivity: {m_sensitivity}')\n",
        "    print(f'        Specificity: {m_specificity}')\n",
        "\n",
        "    return (m_auroc, m_auprc, m_sensitivity, m_specificity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTM5yt6WtOdP"
      },
      "source": [
        "\n",
        "### Results\n",
        "Thus far we have the ability to build our model and obtain one type of test data in the form we need it for training and evaluation. Please execute the code cells below to see for yourself!\n",
        "\n",
        "Our plans from here until the end of the project are:\n",
        "\n",
        "- Make it possible to obtain the other two data types (ECG and EEG) and use them the same way we are able to use ABP\n",
        "- Get our custom specificity function working properly\n",
        "- Train and evaluate our model using a variety of samples\n",
        "- Train and evaluate our model using varying combinations of the input types\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKRTyrR78oeV"
      },
      "source": [
        "# Train with only ABP\n",
        "\n",
        "This next section will demonstrate training on only ABP data. It also demonstrates the scalability of our model, allowing us to only use certain resnets if we so choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-FeXoi8R83dh"
      },
      "outputs": [],
      "source": [
        "# Put together a model using only ABP\n",
        "abp_resnet = WaveformResNet(\n",
        "    input_shape=30000,\n",
        "    output_size=32,\n",
        "    data_type='abp',\n",
        ")\n",
        "\n",
        "abp_model = IntraoperativeHypotensionModel(\n",
        "    abp_resnet=abp_resnet\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UTHqJcbMlGd",
        "outputId": "31abd916-cab6-4754-a4d4-b723840d1d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 1\n",
            "Statistics for case: 1, 774 total valid samples, 71 positive samples\n",
            "Getting track data for case: 12\n",
            "Statistics for case: 12, 1538 total valid samples, 319 positive samples\n",
            "Getting track data for case: 10\n",
            "Statistics for case: 10, 1741 total valid samples, 111 positive samples\n",
            "Getting track data for case: 13\n",
            "Statistics for case: 13, 529 total valid samples, 31 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling (if enabled)\n",
            "Getting track data for case: 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-717b97be3130>:200: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  abps = FloatTensor(abps)\n",
            "<ipython-input-2-717b97be3130>:203: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Statistics for case: 16, 1042 total valid samples, 11 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling (if enabled)\n"
          ]
        }
      ],
      "source": [
        "# Load data from google drive location\n",
        "import pickle\n",
        "\n",
        "case_list_file = open(r'cases_with_positive_samples.pkl', 'rb')\n",
        "case_list = pickle.load(case_list_file)\n",
        "case_list_file.close()\n",
        "\n",
        "train_cases = case_list[0:80]\n",
        "eval_cases = case_list[80:100]\n",
        "# train_cases = case_list[0:4]\n",
        "# eval_cases = case_list[4:5]\n",
        "\n",
        "train_data = get_data(3, from_dir='./drive/MyDrive/CS598/Final_Project/Case Set/', case_ids=train_cases)\n",
        "eval_data = get_data(3, from_dir='./drive/MyDrive/CS598/Final_Project/Case Set/', case_ids=eval_cases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "id": "qw2LESWrpOgC",
        "outputId": "6e377d5a-71e9-4e2b-af5d-484bb1a4c9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "    Train data metrics:\n",
            "        AUROC: 0.7686136637891023\n",
            "        AUPRC: 0.2434867024421692\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.8880169297240103\n",
            "        AUPRC: 0.045668285340070724\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "    Train data metrics:\n",
            "        AUROC: 0.7676255453448436\n",
            "        AUPRC: 0.24139319360256195\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.8825500396790407\n",
            "        AUPRC: 0.04406774044036865\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "Mean loss: 0.519880720310741\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fdf09864e846>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mabp_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0meval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3279c535f425>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_handle, test_data_handle, learning_rate, epochs, suspend_train_epochs_threshold, batch_size, output_dir, model_title)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'===================================='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     loss_history = _train_one_epoch(\n\u001b[0m\u001b[1;32m    108\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0mtrain_data_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-3279c535f425>\u001b[0m in \u001b[0;36m_train_one_epoch\u001b[0;34m(model, train_data, optimizer, criterion, batch_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(\n",
        "    abp_model,\n",
        "    train_data,\n",
        "    eval_data,\n",
        "    batch_size=256,\n",
        "    epochs=51,\n",
        "    model_title='abp_overnight_1',\n",
        "    output_dir='./drive/MyDrive/CS598/Final_Project/model_checkpoints'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g220NASowc_o"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load dataset list and download files to google drive location\n",
        "case_list_file = open(r'cases_with_positive_samples.pkl', 'rb')\n",
        "case_list = pickle.load(case_list_file)\n",
        "case_list_file.close()\n",
        "\n",
        "for case_id in case_list:\n",
        "  _download_vital_file(case_id)\n",
        "\n",
        "!rm -rf ./drive/MyDrive/CS598/Final\\ Project/Case\\ Set\n",
        "!mkdir ./drive/MyDrive/CS598/Final\\ Project/Case\\ Set\n",
        "!mv ./*.vital ./drive/MyDrive/CS598/Final\\ Project/Case\\ Set/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I8D5OS6WpgN"
      },
      "outputs": [],
      "source": [
        "# 15000 total samples should be used for training, 1000 for validation\n",
        "train_dataset = [\n",
        "    full_data[0][0:15000].detach().clone(),\n",
        "    full_data[1].detach().clone(),\n",
        "    full_data[2].detach().clone(),\n",
        "    full_data[3][0:15000].detach().clone()\n",
        "]\n",
        "\n",
        "eval_dataset = [\n",
        "    full_data[0][15000:16000].detach().clone(),\n",
        "    full_data[1].detach().clone(),\n",
        "    full_data[2].detach().clone(),\n",
        "    full_data[3][15000:16000].detach().clone()\n",
        "]\n",
        "\n",
        "full_data = None\n",
        "\n",
        "train(abp_model, train_dataset, eval_dataset, batch_size=256, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEoz5Xt6g06L"
      },
      "outputs": [],
      "source": [
        "!mv ./abp_eval_data_set.pkl ./drive/MyDrive/CS598/Final\\ Project/ABP_Datasets/\n",
        "!mv ./abp_train_data_set.pkl ./drive/MyDrive/CS598/Final\\ Project/ABP_Datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOg9awpIyxAR"
      },
      "outputs": [],
      "source": [
        "# Read data from ABP dataset at 3-minute before event\n",
        "\n",
        "import pickle\n",
        "\n",
        "def _process_loaded_abp_data(all_data):\n",
        "  pos_indices = torch.nonzero(all_data[3] > 0.5).squeeze(-1)\n",
        "  neg_indices = torch.nonzero(all_data[3] < 0.5).squeeze(-1)\n",
        "  abp_pos = all_data[0].detach().clone()[pos_indices]\n",
        "  abp_neg = all_data[0].detach().clone()[neg_indices]\n",
        "\n",
        "  positive_samples = len(pos_indices)\n",
        "  print(f'block has {positive_samples} positives')\n",
        "  abp_neg = abp_neg[0:positive_samples]\n",
        "\n",
        "  all_abp = torch.concat([abp_neg, abp_pos])\n",
        "  all_labels = torch.concat([torch.zeros([abp_neg.size()[0]]), torch.ones([abp_pos.size()[0]])])\n",
        "  shuffled = shuffle_tensors([all_abp, all_labels])\n",
        "  return [\n",
        "    shuffled[0],\n",
        "    torch.Tensor([]),\n",
        "    torch.Tensor([]),\n",
        "    shuffled[1]\n",
        "  ]\n",
        "\n",
        "# Load dataset list and import data 10 cases at a time\n",
        "case_list_file = open(r'cases_with_positive_samples.pkl', 'rb')\n",
        "case_list = pickle.load(case_list_file)\n",
        "case_list_file.close()\n",
        "\n",
        "data_blocks = []\n",
        "!rm -rf ./load_now\n",
        "!mkdir ./load_now\n",
        "\n",
        "i = 0\n",
        "for case_id in case_list:\n",
        "  !cp ./abp_only_cases/{case_id}.vital ./load_now/\n",
        "  i += 1\n",
        "  if i == 10:\n",
        "    # Cut a block\n",
        "    block = get_data(3, from_dir='./load_now')\n",
        "\n",
        "    processed = _process_loaded_abp_data(block)\n",
        "\n",
        "    data_blocks.append(processed)\n",
        "    !rm ./load_now/*.vital\n",
        "\n",
        "    i = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUB3PvfwBNM_"
      },
      "outputs": [],
      "source": [
        "# Pickle the data blocks so we can easily load them later and not have to re-generate them\n",
        "afile = open(r'abp_train_data_sets.pkl', 'wb')\n",
        "pickle.dump(data_blocks, afile)\n",
        "afile.close()\n",
        "data_blocks = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEbEykJiCG-D"
      },
      "outputs": [],
      "source": [
        "# Load the data from the pickle file\n",
        "data_file = open(r'abp_train_data_sets.pkl', 'rb')\n",
        "data_blocks = pickle.load(data_file)\n",
        "data_file.close()\n",
        "\n",
        "# Split into train and eval sets\n",
        "eval_dataset = data_blocks[-1]\n",
        "train_dataset = data_blocks[0:-1]\n",
        "\n",
        "data_blocks = None # GC\n",
        "\n",
        "# Train block by block\n",
        "for idx in range(len(train_dataset)):\n",
        "  print(f'Training on data block {idx}!!!!')\n",
        "  train(abp_model, train_dataset[idx], eval_dataset, batch_size=32, epochs=11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "CRX9zlCMzOL9",
        "outputId": "63735263-95b5-4aa5-ed54-939a0d0f0511"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'abp_neg' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-130a677314db>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get exact number of negative samples to match positive ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mabp_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabp_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m167\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Concat and shuffle together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_abp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabp_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabp_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'abp_neg' is not defined"
          ]
        }
      ],
      "source": [
        "# # Get exact number of negative samples to match positive ones\n",
        "# abp_neg = abp_neg[0:167]\n",
        "\n",
        "# # Concat and shuffle together\n",
        "# all_abp = torch.concat([abp_neg, abp_pos])\n",
        "# all_labels = torch.concat([torch.zeros([abp_neg.size()[0]]), torch.ones([abp_pos.size()[0]])])\n",
        "# shuffled = shuffle_tensors([all_abp, all_labels])\n",
        "\n",
        "# train_set = [\n",
        "#     shuffled[0],\n",
        "#     torch.Tensor([]),\n",
        "#     torch.Tensor([]),\n",
        "#     shuffled[1]\n",
        "# ]\n",
        "\n",
        "# test_set = [\n",
        "#     shuffled[0].detach().clone(),\n",
        "#     torch.Tensor([]),\n",
        "#     torch.Tensor([]),\n",
        "#     shuffled[1].detach().clone()\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--44au29-IRD",
        "outputId": "e34d2394-3d12-436a-fd30-766f99ec06e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 3704\n",
            "Statistics for case: 3704, 1028 total valid samples, 94 positive samples\n",
            "Getting track data for case: 819\n",
            "Statistics for case: 819, 737 total valid samples, 73 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:194: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  abps = FloatTensor(abps)\n",
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        }
      ],
      "source": [
        "# Obtain ABP data to train on\n",
        "#test_set = get_data(3, max_num_samples=1000)\n",
        "\n",
        "# Get cases with good data\n",
        "_download_vital_file('819')\n",
        "_download_vital_file('3704')\n",
        "\n",
        "all_data = get_data(3, from_dir='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYl8aTvL2PdF"
      },
      "outputs": [],
      "source": [
        "# Create a good balanced dataset\n",
        "\n",
        "pos_indices = torch.nonzero(all_data[3] > 0.5).squeeze(-1)\n",
        "neg_indices = torch.nonzero(all_data[3] < 0.5).squeeze(-1)\n",
        "abp_pos = all_data[0].detach().clone()[pos_indices]\n",
        "abp_neg = all_data[0].detach().clone()[neg_indices]\n",
        "\n",
        "# Get exact number of negative samples to match positive ones\n",
        "abp_neg = abp_neg[0:167]\n",
        "\n",
        "# Concat and shuffle together\n",
        "all_abp = torch.concat([abp_neg, abp_pos])\n",
        "all_labels = torch.concat([torch.zeros([abp_neg.size()[0]]), torch.ones([abp_pos.size()[0]])])\n",
        "shuffled = shuffle_tensors([all_abp, all_labels])\n",
        "\n",
        "train_set = [\n",
        "    shuffled[0],\n",
        "    torch.Tensor([]),\n",
        "    torch.Tensor([]),\n",
        "    shuffled[1]\n",
        "]\n",
        "\n",
        "test_set = [\n",
        "    shuffled[0].detach().clone(),\n",
        "    torch.Tensor([]),\n",
        "    torch.Tensor([]),\n",
        "    shuffled[1].detach().clone()\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Uurpy92P9TXg",
        "outputId": "76cf63dd-ba70-42d3-b505-7e635d3ac2f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6892008185386658\n",
            "\tBatch 1 loss: 0.7042883038520813\n",
            "\tBatch 2 loss: 0.6887048482894897\n",
            "\tBatch 3 loss: 0.6910030245780945\n",
            "\tBatch 4 loss: 0.6846761703491211\n",
            "\tBatch 5 loss: 0.6989827752113342\n",
            "\tBatch 6 loss: 0.6940107345581055\n",
            "\tBatch 7 loss: 0.697780966758728\n",
            "\tBatch 8 loss: 0.6829289197921753\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 16\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 21\n",
            "y_hat_long sum: 40, target_long sum: 24\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 40, target_long sum: 19\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 14, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9068835883725715\n",
            "        AUPRC: 0.9036766886711121\n",
            "        Sensitivity: 1.0\n",
            "        Specificity: 0.0\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 16\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 21\n",
            "y_hat_long sum: 40, target_long sum: 24\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 40, target_long sum: 19\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 14, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9084024251466533\n",
            "        AUPRC: 0.9060680866241455\n",
            "        Sensitivity: 1.0\n",
            "        Specificity: 0.0\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6869204640388489\n",
            "\tBatch 1 loss: 0.6975041627883911\n",
            "\tBatch 2 loss: 0.6867660880088806\n",
            "\tBatch 3 loss: 0.6886695027351379\n",
            "\tBatch 4 loss: 0.6862188577651978\n",
            "\tBatch 5 loss: 0.6922420263290405\n",
            "\tBatch 6 loss: 0.689597487449646\n",
            "\tBatch 7 loss: 0.6912027597427368\n",
            "\tBatch 8 loss: 0.6874047517776489\n",
            "y_hat_long sum: 35, target_long sum: 22\n",
            "y_hat_long sum: 34, target_long sum: 16\n",
            "y_hat_long sum: 38, target_long sum: 22\n",
            "y_hat_long sum: 36, target_long sum: 21\n",
            "y_hat_long sum: 36, target_long sum: 24\n",
            "y_hat_long sum: 36, target_long sum: 17\n",
            "y_hat_long sum: 36, target_long sum: 19\n",
            "y_hat_long sum: 35, target_long sum: 17\n",
            "y_hat_long sum: 13, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9146188580408359\n",
            "        AUPRC: 0.9116359353065491\n",
            "        Sensitivity: 0.984935998916626\n",
            "        Specificity: 0.17522545158863068\n",
            "y_hat_long sum: 36, target_long sum: 22\n",
            "y_hat_long sum: 33, target_long sum: 16\n",
            "y_hat_long sum: 38, target_long sum: 22\n",
            "y_hat_long sum: 37, target_long sum: 21\n",
            "y_hat_long sum: 37, target_long sum: 24\n",
            "y_hat_long sum: 35, target_long sum: 17\n",
            "y_hat_long sum: 36, target_long sum: 19\n",
            "y_hat_long sum: 36, target_long sum: 17\n",
            "y_hat_long sum: 13, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9119220383212456\n",
            "        AUPRC: 0.9160993099212646\n",
            "        Sensitivity: 0.984935998916626\n",
            "        Specificity: 0.16088983416557312\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6850563883781433\n",
            "\tBatch 1 loss: 0.6895695924758911\n",
            "\tBatch 2 loss: 0.6853010654449463\n",
            "\tBatch 3 loss: 0.6856827735900879\n",
            "\tBatch 4 loss: 0.688294529914856\n",
            "\tBatch 5 loss: 0.684249997138977\n",
            "\tBatch 6 loss: 0.6844044327735901\n",
            "\tBatch 7 loss: 0.6831811666488647\n",
            "\tBatch 8 loss: 0.6936203837394714\n",
            "y_hat_long sum: 15, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 9, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 21\n",
            "y_hat_long sum: 17, target_long sum: 24\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 11, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.914706176847865\n",
            "        AUPRC: 0.9165877103805542\n",
            "        Sensitivity: 0.6110977530479431\n",
            "        Specificity: 0.9234481453895569\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 8, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 21\n",
            "y_hat_long sum: 17, target_long sum: 24\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 19\n",
            "y_hat_long sum: 11, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9130499879447491\n",
            "        AUPRC: 0.9178506135940552\n",
            "        Sensitivity: 0.5832011699676514\n",
            "        Specificity: 0.9234481453895569\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6829558610916138\n",
            "\tBatch 1 loss: 0.6807898283004761\n",
            "\tBatch 2 loss: 0.683302640914917\n",
            "\tBatch 3 loss: 0.682981014251709\n",
            "\tBatch 4 loss: 0.6918870210647583\n",
            "\tBatch 5 loss: 0.6753025054931641\n",
            "\tBatch 6 loss: 0.6792897582054138\n",
            "\tBatch 7 loss: 0.6750096082687378\n",
            "\tBatch 8 loss: 0.7010712623596191\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9121559389770737\n",
            "        AUPRC: 0.9117672443389893\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9141807043692212\n",
            "        AUPRC: 0.9192715883255005\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6816198825836182\n",
            "\tBatch 1 loss: 0.671542227268219\n",
            "\tBatch 2 loss: 0.6819656491279602\n",
            "\tBatch 3 loss: 0.6805471181869507\n",
            "\tBatch 4 loss: 0.6955975294113159\n",
            "\tBatch 5 loss: 0.6670814752578735\n",
            "\tBatch 6 loss: 0.6745513677597046\n",
            "\tBatch 7 loss: 0.6669188737869263\n",
            "\tBatch 8 loss: 0.7087539434432983\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9151169204373849\n",
            "        AUPRC: 0.919556736946106\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.913498847896808\n",
            "        AUPRC: 0.9182300567626953\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6805179715156555\n",
            "\tBatch 1 loss: 0.6637954115867615\n",
            "\tBatch 2 loss: 0.6807087659835815\n",
            "\tBatch 3 loss: 0.6787312626838684\n",
            "\tBatch 4 loss: 0.6993579864501953\n",
            "\tBatch 5 loss: 0.6601194739341736\n",
            "\tBatch 6 loss: 0.6711148619651794\n",
            "\tBatch 7 loss: 0.660396933555603\n",
            "\tBatch 8 loss: 0.7158055305480957\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9158318365203783\n",
            "        AUPRC: 0.9196781516075134\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9125363991579238\n",
            "        AUPRC: 0.9177239537239075\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.679352879524231\n",
            "\tBatch 1 loss: 0.658550500869751\n",
            "\tBatch 2 loss: 0.6799470782279968\n",
            "\tBatch 3 loss: 0.6764718294143677\n",
            "\tBatch 4 loss: 0.7023816108703613\n",
            "\tBatch 5 loss: 0.6544641256332397\n",
            "\tBatch 6 loss: 0.6684986352920532\n",
            "\tBatch 7 loss: 0.65559983253479\n",
            "\tBatch 8 loss: 0.7181524634361267\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9137409679360537\n",
            "        AUPRC: 0.918603777885437\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9137770403610728\n",
            "        AUPRC: 0.9169428944587708\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6785613894462585\n",
            "\tBatch 1 loss: 0.6542757153511047\n",
            "\tBatch 2 loss: 0.677804172039032\n",
            "\tBatch 3 loss: 0.6745716333389282\n",
            "\tBatch 4 loss: 0.7036718726158142\n",
            "\tBatch 5 loss: 0.6512762308120728\n",
            "\tBatch 6 loss: 0.6665207147598267\n",
            "\tBatch 7 loss: 0.6510282754898071\n",
            "\tBatch 8 loss: 0.7187858819961548\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.914417685208369\n",
            "        AUPRC: 0.9208349585533142\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9128976986388897\n",
            "        AUPRC: 0.9209595918655396\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #9\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6773391366004944\n",
            "\tBatch 1 loss: 0.6524175405502319\n",
            "\tBatch 2 loss: 0.6764297485351562\n",
            "\tBatch 3 loss: 0.6725694537162781\n",
            "\tBatch 4 loss: 0.7027281522750854\n",
            "\tBatch 5 loss: 0.6483203768730164\n",
            "\tBatch 6 loss: 0.6648508310317993\n",
            "\tBatch 7 loss: 0.6481978297233582\n",
            "\tBatch 8 loss: 0.7167852520942688\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9142161130020442\n",
            "        AUPRC: 0.9227725267410278\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9159147711451077\n",
            "        AUPRC: 0.9241769313812256\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #10\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6744926571846008\n",
            "\tBatch 1 loss: 0.65020352602005\n",
            "\tBatch 2 loss: 0.6734442114830017\n",
            "\tBatch 3 loss: 0.6699119210243225\n",
            "\tBatch 4 loss: 0.7023297548294067\n",
            "\tBatch 5 loss: 0.6456762552261353\n",
            "\tBatch 6 loss: 0.6623522639274597\n",
            "\tBatch 7 loss: 0.6446765065193176\n",
            "\tBatch 8 loss: 0.7173344492912292\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9155457717968444\n",
            "        AUPRC: 0.9226319789886475\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9140720881000186\n",
            "        AUPRC: 0.9222228527069092\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #11\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6722265481948853\n",
            "\tBatch 1 loss: 0.6484382748603821\n",
            "\tBatch 2 loss: 0.6703135967254639\n",
            "\tBatch 3 loss: 0.6670130491256714\n",
            "\tBatch 4 loss: 0.7007306218147278\n",
            "\tBatch 5 loss: 0.6420892477035522\n",
            "\tBatch 6 loss: 0.6602762937545776\n",
            "\tBatch 7 loss: 0.6416604518890381\n",
            "\tBatch 8 loss: 0.715448796749115\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9147042660408164\n",
            "        AUPRC: 0.9239981770515442\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9133784312831218\n",
            "        AUPRC: 0.9213923215866089\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #12\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6702406406402588\n",
            "\tBatch 1 loss: 0.6452990174293518\n",
            "\tBatch 2 loss: 0.6670089364051819\n",
            "\tBatch 3 loss: 0.6637771725654602\n",
            "\tBatch 4 loss: 0.6979485750198364\n",
            "\tBatch 5 loss: 0.6389375925064087\n",
            "\tBatch 6 loss: 0.6578430533409119\n",
            "\tBatch 7 loss: 0.6369155049324036\n",
            "\tBatch 8 loss: 0.7131065726280212\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9145103661882721\n",
            "        AUPRC: 0.920425295829773\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9103260071371899\n",
            "        AUPRC: 0.9209523797035217\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #13\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6671538352966309\n",
            "\tBatch 1 loss: 0.6429203748703003\n",
            "\tBatch 2 loss: 0.6627871990203857\n",
            "\tBatch 3 loss: 0.6598578095436096\n",
            "\tBatch 4 loss: 0.6972920894622803\n",
            "\tBatch 5 loss: 0.633895993232727\n",
            "\tBatch 6 loss: 0.654870867729187\n",
            "\tBatch 7 loss: 0.6311982274055481\n",
            "\tBatch 8 loss: 0.7101749777793884\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9104366484368379\n",
            "        AUPRC: 0.9194555282592773\n",
            "        Sensitivity: 0.0030673397704958916\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9120152979695652\n",
            "        AUPRC: 0.92055743932724\n",
            "        Sensitivity: 0.0030673397704958916\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #14\n",
            "====================================\n",
            "\tBatch 0 loss: 0.662881076335907\n",
            "\tBatch 1 loss: 0.639514148235321\n",
            "\tBatch 2 loss: 0.6594415903091431\n",
            "\tBatch 3 loss: 0.6560066342353821\n",
            "\tBatch 4 loss: 0.6943228840827942\n",
            "\tBatch 5 loss: 0.6285778284072876\n",
            "\tBatch 6 loss: 0.6510604619979858\n",
            "\tBatch 7 loss: 0.6250197291374207\n",
            "\tBatch 8 loss: 0.7063249945640564\n",
            "y_hat_long sum: 1, target_long sum: 22\n",
            "y_hat_long sum: 1, target_long sum: 16\n",
            "y_hat_long sum: 1, target_long sum: 22\n",
            "y_hat_long sum: 1, target_long sum: 21\n",
            "y_hat_long sum: 1, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 1, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.909001982577037\n",
            "        AUPRC: 0.9180669784545898\n",
            "        Sensitivity: 0.04630136117339134\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 1, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 2, target_long sum: 22\n",
            "y_hat_long sum: 1, target_long sum: 21\n",
            "y_hat_long sum: 1, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 1, target_long sum: 19\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 1, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9120617249046163\n",
            "        AUPRC: 0.9225659966468811\n",
            "        Sensitivity: 0.046902552247047424\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #15\n",
            "====================================\n",
            "\tBatch 0 loss: 0.657463788986206\n",
            "\tBatch 1 loss: 0.6365253329277039\n",
            "\tBatch 2 loss: 0.6516498923301697\n",
            "\tBatch 3 loss: 0.6496671438217163\n",
            "\tBatch 4 loss: 0.691287636756897\n",
            "\tBatch 5 loss: 0.6214285492897034\n",
            "\tBatch 6 loss: 0.6463429927825928\n",
            "\tBatch 7 loss: 0.6186938285827637\n",
            "\tBatch 8 loss: 0.7003628611564636\n",
            "y_hat_long sum: 3, target_long sum: 22\n",
            "y_hat_long sum: 4, target_long sum: 16\n",
            "y_hat_long sum: 4, target_long sum: 22\n",
            "y_hat_long sum: 2, target_long sum: 21\n",
            "y_hat_long sum: 3, target_long sum: 24\n",
            "y_hat_long sum: 5, target_long sum: 17\n",
            "y_hat_long sum: 3, target_long sum: 19\n",
            "y_hat_long sum: 3, target_long sum: 17\n",
            "y_hat_long sum: 3, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9057255969914273\n",
            "        AUPRC: 0.9164742231369019\n",
            "        Sensitivity: 0.1636136770248413\n",
            "        Specificity: 0.9930555820465088\n",
            "y_hat_long sum: 3, target_long sum: 22\n",
            "y_hat_long sum: 3, target_long sum: 16\n",
            "y_hat_long sum: 4, target_long sum: 22\n",
            "y_hat_long sum: 3, target_long sum: 21\n",
            "y_hat_long sum: 4, target_long sum: 24\n",
            "y_hat_long sum: 5, target_long sum: 17\n",
            "y_hat_long sum: 2, target_long sum: 19\n",
            "y_hat_long sum: 5, target_long sum: 17\n",
            "y_hat_long sum: 3, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.907437916187036\n",
            "        AUPRC: 0.91530841588974\n",
            "        Sensitivity: 0.1635439395904541\n",
            "        Specificity: 0.9930555820465088\n",
            "====================================\n",
            "     Epoch #16\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6520121693611145\n",
            "\tBatch 1 loss: 0.6297735571861267\n",
            "\tBatch 2 loss: 0.6448286771774292\n",
            "\tBatch 3 loss: 0.6432048678398132\n",
            "\tBatch 4 loss: 0.6862486600875854\n",
            "\tBatch 5 loss: 0.6146045923233032\n",
            "\tBatch 6 loss: 0.642187774181366\n",
            "\tBatch 7 loss: 0.6073592305183411\n",
            "\tBatch 8 loss: 0.6975404024124146\n",
            "y_hat_long sum: 7, target_long sum: 22\n",
            "y_hat_long sum: 9, target_long sum: 16\n",
            "y_hat_long sum: 7, target_long sum: 22\n",
            "y_hat_long sum: 5, target_long sum: 21\n",
            "y_hat_long sum: 6, target_long sum: 24\n",
            "y_hat_long sum: 9, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 19\n",
            "y_hat_long sum: 9, target_long sum: 17\n",
            "y_hat_long sum: 4, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9064030583907068\n",
            "        AUPRC: 0.9148079752922058\n",
            "        Sensitivity: 0.3506799340248108\n",
            "        Specificity: 0.9861111044883728\n",
            "y_hat_long sum: 8, target_long sum: 22\n",
            "y_hat_long sum: 9, target_long sum: 16\n",
            "y_hat_long sum: 7, target_long sum: 22\n",
            "y_hat_long sum: 5, target_long sum: 21\n",
            "y_hat_long sum: 11, target_long sum: 24\n",
            "y_hat_long sum: 8, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 19\n",
            "y_hat_long sum: 9, target_long sum: 17\n",
            "y_hat_long sum: 4, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9054873441816186\n",
            "        AUPRC: 0.9133103489875793\n",
            "        Sensitivity: 0.38356393575668335\n",
            "        Specificity: 0.9861111044883728\n",
            "====================================\n",
            "     Epoch #17\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6449339389801025\n",
            "\tBatch 1 loss: 0.6251242160797119\n",
            "\tBatch 2 loss: 0.6351718306541443\n",
            "\tBatch 3 loss: 0.6345715522766113\n",
            "\tBatch 4 loss: 0.6811243891716003\n",
            "\tBatch 5 loss: 0.6064152717590332\n",
            "\tBatch 6 loss: 0.6332703232765198\n",
            "\tBatch 7 loss: 0.5971869230270386\n",
            "\tBatch 8 loss: 0.6914825439453125\n",
            "y_hat_long sum: 10, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 8, target_long sum: 22\n",
            "y_hat_long sum: 10, target_long sum: 21\n",
            "y_hat_long sum: 14, target_long sum: 24\n",
            "y_hat_long sum: 11, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 19\n",
            "y_hat_long sum: 10, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9079264219146551\n",
            "        AUPRC: 0.9156043529510498\n",
            "        Sensitivity: 0.49272775650024414\n",
            "        Specificity: 0.9463623762130737\n",
            "y_hat_long sum: 10, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 9, target_long sum: 22\n",
            "y_hat_long sum: 9, target_long sum: 21\n",
            "y_hat_long sum: 13, target_long sum: 24\n",
            "y_hat_long sum: 10, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 19\n",
            "y_hat_long sum: 10, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9036228943583592\n",
            "        AUPRC: 0.9119967222213745\n",
            "        Sensitivity: 0.4893557131290436\n",
            "        Specificity: 0.9533068537712097\n",
            "====================================\n",
            "     Epoch #18\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6358113288879395\n",
            "\tBatch 1 loss: 0.6193191409111023\n",
            "\tBatch 2 loss: 0.6239252686500549\n",
            "\tBatch 3 loss: 0.6201116442680359\n",
            "\tBatch 4 loss: 0.6790622472763062\n",
            "\tBatch 5 loss: 0.5962127447128296\n",
            "\tBatch 6 loss: 0.6256857514381409\n",
            "\tBatch 7 loss: 0.5815078020095825\n",
            "\tBatch 8 loss: 0.6803013682365417\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 12, target_long sum: 16\n",
            "y_hat_long sum: 11, target_long sum: 22\n",
            "y_hat_long sum: 10, target_long sum: 21\n",
            "y_hat_long sum: 18, target_long sum: 24\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9055317932246313\n",
            "        AUPRC: 0.916564404964447\n",
            "        Sensitivity: 0.5964581370353699\n",
            "        Specificity: 0.9341269731521606\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 11, target_long sum: 22\n",
            "y_hat_long sum: 10, target_long sum: 21\n",
            "y_hat_long sum: 19, target_long sum: 24\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9023378882092268\n",
            "        AUPRC: 0.9100298285484314\n",
            "        Sensitivity: 0.5903105139732361\n",
            "        Specificity: 0.9341269731521606\n",
            "====================================\n",
            "     Epoch #19\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6220640540122986\n",
            "\tBatch 1 loss: 0.6124768257141113\n",
            "\tBatch 2 loss: 0.6112731099128723\n",
            "\tBatch 3 loss: 0.6101856827735901\n",
            "\tBatch 4 loss: 0.6660743355751038\n",
            "\tBatch 5 loss: 0.5836362838745117\n",
            "\tBatch 6 loss: 0.6151502728462219\n",
            "\tBatch 7 loss: 0.5657191872596741\n",
            "\tBatch 8 loss: 0.6653093695640564\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 16\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 19, target_long sum: 24\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 7, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.90227949998207\n",
            "        AUPRC: 0.9132506251335144\n",
            "        Sensitivity: 0.6342509388923645\n",
            "        Specificity: 0.8857938051223755\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 16\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 13, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 7, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.8988305840619388\n",
            "        AUPRC: 0.9081506729125977\n",
            "        Sensitivity: 0.6373183131217957\n",
            "        Specificity: 0.868727445602417\n",
            "====================================\n",
            "     Epoch #20\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6118425130844116\n",
            "\tBatch 1 loss: 0.6049607396125793\n",
            "\tBatch 2 loss: 0.591064453125\n",
            "\tBatch 3 loss: 0.59397292137146\n",
            "\tBatch 4 loss: 0.6609207987785339\n",
            "\tBatch 5 loss: 0.5649071931838989\n",
            "\tBatch 6 loss: 0.6034107804298401\n",
            "\tBatch 7 loss: 0.5433727502822876\n",
            "\tBatch 8 loss: 0.6561498641967773\n",
            "y_hat_long sum: 15, target_long sum: 22\n",
            "y_hat_long sum: 15, target_long sum: 16\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 15, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 8, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9020147318954375\n",
            "        AUPRC: 0.9098455905914307\n",
            "        Sensitivity: 0.6612875461578369\n",
            "        Specificity: 0.8577236533164978\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 15, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 8, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9025140054438228\n",
            "        AUPRC: 0.9109455943107605\n",
            "        Sensitivity: 0.686884343624115\n",
            "        Specificity: 0.8484643697738647\n",
            "====================================\n",
            "     Epoch #21\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5953096151351929\n",
            "\tBatch 1 loss: 0.5934098362922668\n",
            "\tBatch 2 loss: 0.5744529962539673\n",
            "\tBatch 3 loss: 0.5775696039199829\n",
            "\tBatch 4 loss: 0.6523662805557251\n",
            "\tBatch 5 loss: 0.5451484322547913\n",
            "\tBatch 6 loss: 0.5920466184616089\n",
            "\tBatch 7 loss: 0.5220590829849243\n",
            "\tBatch 8 loss: 0.6437917351722717\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 16, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 16, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 8, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8994549509687132\n",
            "        AUPRC: 0.9083201289176941\n",
            "        Sensitivity: 0.7393754720687866\n",
            "        Specificity: 0.8255500197410583\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 16, target_long sum: 21\n",
            "y_hat_long sum: 19, target_long sum: 24\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 17, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9002835431016211\n",
            "        AUPRC: 0.9113142490386963\n",
            "        Sensitivity: 0.6985442638397217\n",
            "        Specificity: 0.8341479301452637\n",
            "====================================\n",
            "     Epoch #22\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5775318741798401\n",
            "\tBatch 1 loss: 0.5729418992996216\n",
            "\tBatch 2 loss: 0.5543146133422852\n",
            "\tBatch 3 loss: 0.5552261471748352\n",
            "\tBatch 4 loss: 0.6446357369422913\n",
            "\tBatch 5 loss: 0.527985692024231\n",
            "\tBatch 6 loss: 0.578209638595581\n",
            "\tBatch 7 loss: 0.49590954184532166\n",
            "\tBatch 8 loss: 0.6254758834838867\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 16, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 17, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8995672395118522\n",
            "        AUPRC: 0.9081029891967773\n",
            "        Sensitivity: 0.739317774772644\n",
            "        Specificity: 0.8202590346336365\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 17, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.8977549728615387\n",
            "        AUPRC: 0.9066032767295837\n",
            "        Sensitivity: 0.7765670418739319\n",
            "        Specificity: 0.8202590346336365\n",
            "====================================\n",
            "     Epoch #23\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5586413145065308\n",
            "\tBatch 1 loss: 0.553929328918457\n",
            "\tBatch 2 loss: 0.5273860692977905\n",
            "\tBatch 3 loss: 0.5337191224098206\n",
            "\tBatch 4 loss: 0.6379135847091675\n",
            "\tBatch 5 loss: 0.5060160160064697\n",
            "\tBatch 6 loss: 0.5605292320251465\n",
            "\tBatch 7 loss: 0.47192078828811646\n",
            "\tBatch 8 loss: 0.6171318888664246\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 19, target_long sum: 17\n",
            "y_hat_long sum: 18, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8999552290367203\n",
            "        AUPRC: 0.9075493216514587\n",
            "        Sensitivity: 0.7990299463272095\n",
            "        Specificity: 0.8092553019523621\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9035299355516708\n",
            "        AUPRC: 0.9131190776824951\n",
            "        Sensitivity: 0.781860888004303\n",
            "        Specificity: 0.8061688542366028\n",
            "====================================\n",
            "     Epoch #24\n",
            "====================================\n",
            "\tBatch 0 loss: 0.540097713470459\n",
            "\tBatch 1 loss: 0.5453150868415833\n",
            "\tBatch 2 loss: 0.5060952305793762\n",
            "\tBatch 3 loss: 0.514571487903595\n",
            "\tBatch 4 loss: 0.6237150430679321\n",
            "\tBatch 5 loss: 0.48014673590660095\n",
            "\tBatch 6 loss: 0.5526663064956665\n",
            "\tBatch 7 loss: 0.4371268153190613\n",
            "\tBatch 8 loss: 0.5707822442054749\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 17, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9096165155834309\n",
            "        AUPRC: 0.9152199625968933\n",
            "        Sensitivity: 0.7988736629486084\n",
            "        Specificity: 0.8145463466644287\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 17, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 19, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9059674847431809\n",
            "        AUPRC: 0.9114459156990051\n",
            "        Sensitivity: 0.778083086013794\n",
            "        Specificity: 0.8092553019523621\n",
            "====================================\n",
            "     Epoch #25\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5129109621047974\n",
            "\tBatch 1 loss: 0.5320796370506287\n",
            "\tBatch 2 loss: 0.4859645366668701\n",
            "\tBatch 3 loss: 0.49320918321609497\n",
            "\tBatch 4 loss: 0.6200492978096008\n",
            "\tBatch 5 loss: 0.46122536063194275\n",
            "\tBatch 6 loss: 0.5245465636253357\n",
            "\tBatch 7 loss: 0.4072629511356354\n",
            "\tBatch 8 loss: 0.5563286542892456\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9074245551209363\n",
            "        AUPRC: 0.913905680179596\n",
            "        Sensitivity: 0.7802093625068665\n",
            "        Specificity: 0.8154281377792358\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 18, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9064246080279639\n",
            "        AUPRC: 0.9141232967376709\n",
            "        Sensitivity: 0.7802093625068665\n",
            "        Specificity: 0.8050857782363892\n",
            "====================================\n",
            "     Epoch #26\n",
            "====================================\n",
            "\tBatch 0 loss: 0.49690312147140503\n",
            "\tBatch 1 loss: 0.5002130270004272\n",
            "\tBatch 2 loss: 0.4583618640899658\n",
            "\tBatch 3 loss: 0.46201497316360474\n",
            "\tBatch 4 loss: 0.6252663731575012\n",
            "\tBatch 5 loss: 0.43490082025527954\n",
            "\tBatch 6 loss: 0.5175111889839172\n",
            "\tBatch 7 loss: 0.38453200459480286\n",
            "\tBatch 8 loss: 0.5224553942680359\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9108956653065243\n",
            "        AUPRC: 0.916391909122467\n",
            "        Sensitivity: 0.7933604717254639\n",
            "        Specificity: 0.8105972409248352\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.903133487921726\n",
            "        AUPRC: 0.9096946716308594\n",
            "        Sensitivity: 0.7993379235267639\n",
            "        Specificity: 0.8092553019523621\n",
            "====================================\n",
            "     Epoch #27\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4713056981563568\n",
            "\tBatch 1 loss: 0.4852832853794098\n",
            "\tBatch 2 loss: 0.43334245681762695\n",
            "\tBatch 3 loss: 0.45268183946609497\n",
            "\tBatch 4 loss: 0.6166642904281616\n",
            "\tBatch 5 loss: 0.4112322926521301\n",
            "\tBatch 6 loss: 0.5099173188209534\n",
            "\tBatch 7 loss: 0.3541201949119568\n",
            "\tBatch 8 loss: 0.4999852776527405\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9091484493691434\n",
            "        AUPRC: 0.9145628809928894\n",
            "        Sensitivity: 0.7830312848091125\n",
            "        Specificity: 0.8047492504119873\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.907201729012272\n",
            "        AUPRC: 0.9122954607009888\n",
            "        Sensitivity: 0.7875897884368896\n",
            "        Specificity: 0.7974799275398254\n",
            "====================================\n",
            "     Epoch #28\n",
            "====================================\n",
            "\tBatch 0 loss: 0.45703190565109253\n",
            "\tBatch 1 loss: 0.45623263716697693\n",
            "\tBatch 2 loss: 0.42124828696250916\n",
            "\tBatch 3 loss: 0.4326518177986145\n",
            "\tBatch 4 loss: 0.5946008563041687\n",
            "\tBatch 5 loss: 0.39314547181129456\n",
            "\tBatch 6 loss: 0.48073118925094604\n",
            "\tBatch 7 loss: 0.3258168697357178\n",
            "\tBatch 8 loss: 0.49800100922584534\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.915470572373176\n",
            "        AUPRC: 0.9206973314285278\n",
            "        Sensitivity: 0.7788721919059753\n",
            "        Specificity: 0.8044244050979614\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9155131609101826\n",
            "        AUPRC: 0.9178063869476318\n",
            "        Sensitivity: 0.8202460408210754\n",
            "        Specificity: 0.8047492504119873\n",
            "====================================\n",
            "     Epoch #29\n",
            "====================================\n",
            "\tBatch 0 loss: 0.44141101837158203\n",
            "\tBatch 1 loss: 0.44576215744018555\n",
            "\tBatch 2 loss: 0.411935955286026\n",
            "\tBatch 3 loss: 0.4177750051021576\n",
            "\tBatch 4 loss: 0.5975330471992493\n",
            "\tBatch 5 loss: 0.3745512068271637\n",
            "\tBatch 6 loss: 0.4692893922328949\n",
            "\tBatch 7 loss: 0.31565457582473755\n",
            "\tBatch 8 loss: 0.47539201378822327\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9173901206434635\n",
            "        AUPRC: 0.9202038645744324\n",
            "        Sensitivity: 0.7882551550865173\n",
            "        Specificity: 0.8071992993354797\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9175676107754693\n",
            "        AUPRC: 0.91779625415802\n",
            "        Sensitivity: 0.8040256500244141\n",
            "        Specificity: 0.8079267740249634\n",
            "====================================\n",
            "     Epoch #30\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4238317012786865\n",
            "\tBatch 1 loss: 0.42817822098731995\n",
            "\tBatch 2 loss: 0.3993373513221741\n",
            "\tBatch 3 loss: 0.41644197702407837\n",
            "\tBatch 4 loss: 0.5928016304969788\n",
            "\tBatch 5 loss: 0.3553703725337982\n",
            "\tBatch 6 loss: 0.4571201205253601\n",
            "\tBatch 7 loss: 0.2850829064846039\n",
            "\tBatch 8 loss: 0.4402788281440735\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9223497176903298\n",
            "        AUPRC: 0.9256170988082886\n",
            "        Sensitivity: 0.7957514524459839\n",
            "        Specificity: 0.7952887415885925\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 24, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9205916544068877\n",
            "        AUPRC: 0.9243068695068359\n",
            "        Sensitivity: 0.8119109272956848\n",
            "        Specificity: 0.8133146166801453\n",
            "====================================\n",
            "     Epoch #31\n",
            "====================================\n",
            "\tBatch 0 loss: 0.40575703978538513\n",
            "\tBatch 1 loss: 0.40882283449172974\n",
            "\tBatch 2 loss: 0.3769167959690094\n",
            "\tBatch 3 loss: 0.38330045342445374\n",
            "\tBatch 4 loss: 0.6149474382400513\n",
            "\tBatch 5 loss: 0.348533570766449\n",
            "\tBatch 6 loss: 0.4583815932273865\n",
            "\tBatch 7 loss: 0.2713106572628021\n",
            "\tBatch 8 loss: 0.4211786091327667\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9256290322691668\n",
            "        AUPRC: 0.9285816550254822\n",
            "        Sensitivity: 0.8161505460739136\n",
            "        Specificity: 0.8216009736061096\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9238509482589762\n",
            "        AUPRC: 0.9263980388641357\n",
            "        Sensitivity: 0.7925508618354797\n",
            "        Specificity: 0.8276636004447937\n",
            "====================================\n",
            "     Epoch #32\n",
            "====================================\n",
            "\tBatch 0 loss: 0.40394920110702515\n",
            "\tBatch 1 loss: 0.39052993059158325\n",
            "\tBatch 2 loss: 0.37605565786361694\n",
            "\tBatch 3 loss: 0.39274030923843384\n",
            "\tBatch 4 loss: 0.5977856516838074\n",
            "\tBatch 5 loss: 0.3231494128704071\n",
            "\tBatch 6 loss: 0.4454365372657776\n",
            "\tBatch 7 loss: 0.26916977763175964\n",
            "\tBatch 8 loss: 0.38943150639533997\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.923293780403125\n",
            "        AUPRC: 0.924525260925293\n",
            "        Sensitivity: 0.8227059841156006\n",
            "        Specificity: 0.8080811500549316\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9255179297456037\n",
            "        AUPRC: 0.9279183149337769\n",
            "        Sensitivity: 0.8100029826164246\n",
            "        Specificity: 0.8207986950874329\n",
            "====================================\n",
            "     Epoch #33\n",
            "====================================\n",
            "\tBatch 0 loss: 0.38184699416160583\n",
            "\tBatch 1 loss: 0.38256606459617615\n",
            "\tBatch 2 loss: 0.3720191717147827\n",
            "\tBatch 3 loss: 0.36135175824165344\n",
            "\tBatch 4 loss: 0.5956486463546753\n",
            "\tBatch 5 loss: 0.3198111951351166\n",
            "\tBatch 6 loss: 0.41644731163978577\n",
            "\tBatch 7 loss: 0.25092363357543945\n",
            "\tBatch 8 loss: 0.3977125585079193\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9298979798138206\n",
            "        AUPRC: 0.9276628494262695\n",
            "        Sensitivity: 0.8074328899383545\n",
            "        Specificity: 0.8228327035903931\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9264019916782574\n",
            "        AUPRC: 0.9299004673957825\n",
            "        Sensitivity: 0.8425882458686829\n",
            "        Specificity: 0.8157530426979065\n",
            "====================================\n",
            "     Epoch #34\n",
            "====================================\n",
            "\tBatch 0 loss: 0.36873772740364075\n",
            "\tBatch 1 loss: 0.3749043345451355\n",
            "\tBatch 2 loss: 0.34286636114120483\n",
            "\tBatch 3 loss: 0.3531152606010437\n",
            "\tBatch 4 loss: 0.5901861190795898\n",
            "\tBatch 5 loss: 0.2955051064491272\n",
            "\tBatch 6 loss: 0.4149653911590576\n",
            "\tBatch 7 loss: 0.2443433701992035\n",
            "\tBatch 8 loss: 0.3669602572917938\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9366084554044496\n",
            "        AUPRC: 0.9346143007278442\n",
            "        Sensitivity: 0.8141166567802429\n",
            "        Specificity: 0.8095801472663879\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 22, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9322428934496172\n",
            "        AUPRC: 0.9293645024299622\n",
            "        Sensitivity: 0.8339041471481323\n",
            "        Specificity: 0.8392980098724365\n",
            "====================================\n",
            "     Epoch #35\n",
            "====================================\n",
            "\tBatch 0 loss: 0.36754292249679565\n",
            "\tBatch 1 loss: 0.3627736568450928\n",
            "\tBatch 2 loss: 0.33578401803970337\n",
            "\tBatch 3 loss: 0.3520323634147644\n",
            "\tBatch 4 loss: 0.6031167507171631\n",
            "\tBatch 5 loss: 0.29994362592697144\n",
            "\tBatch 6 loss: 0.41411834955215454\n",
            "\tBatch 7 loss: 0.23601683974266052\n",
            "\tBatch 8 loss: 0.35793250799179077\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9334972274834674\n",
            "        AUPRC: 0.9340277910232544\n",
            "        Sensitivity: 0.8142987489700317\n",
            "        Specificity: 0.8384660482406616\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9300985337680571\n",
            "        AUPRC: 0.9306390285491943\n",
            "        Sensitivity: 0.8129301071166992\n",
            "        Specificity: 0.8462731838226318\n",
            "====================================\n",
            "     Epoch #36\n",
            "====================================\n",
            "\tBatch 0 loss: 0.36972880363464355\n",
            "\tBatch 1 loss: 0.34340900182724\n",
            "\tBatch 2 loss: 0.3554220199584961\n",
            "\tBatch 3 loss: 0.3402032256126404\n",
            "\tBatch 4 loss: 0.6039629578590393\n",
            "\tBatch 5 loss: 0.2948702871799469\n",
            "\tBatch 6 loss: 0.4092229902744293\n",
            "\tBatch 7 loss: 0.23137831687927246\n",
            "\tBatch 8 loss: 0.39961346983909607\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9342961646714034\n",
            "        AUPRC: 0.934509813785553\n",
            "        Sensitivity: 0.8336307406425476\n",
            "        Specificity: 0.8144111037254333\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9324726064536354\n",
            "        AUPRC: 0.9316632747650146\n",
            "        Sensitivity: 0.7956344485282898\n",
            "        Specificity: 0.8144111037254333\n",
            "====================================\n",
            "     Epoch #37\n",
            "====================================\n",
            "\tBatch 0 loss: 0.34632936120033264\n",
            "\tBatch 1 loss: 0.34182876348495483\n",
            "\tBatch 2 loss: 0.3272763788700104\n",
            "\tBatch 3 loss: 0.3421041667461395\n",
            "\tBatch 4 loss: 0.5906454920768738\n",
            "\tBatch 5 loss: 0.2842285633087158\n",
            "\tBatch 6 loss: 0.39186030626296997\n",
            "\tBatch 7 loss: 0.21082703769207\n",
            "\tBatch 8 loss: 0.349656879901886\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 23, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9343020626985599\n",
            "        AUPRC: 0.931919515132904\n",
            "        Sensitivity: 0.7964224219322205\n",
            "        Specificity: 0.8354592323303223\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9363861074314968\n",
            "        AUPRC: 0.9349743723869324\n",
            "        Sensitivity: 0.8129301071166992\n",
            "        Specificity: 0.842941403388977\n",
            "====================================\n",
            "     Epoch #38\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3505795896053314\n",
            "\tBatch 1 loss: 0.3147304356098175\n",
            "\tBatch 2 loss: 0.3276650309562683\n",
            "\tBatch 3 loss: 0.3412100672721863\n",
            "\tBatch 4 loss: 0.6018750071525574\n",
            "\tBatch 5 loss: 0.2720131278038025\n",
            "\tBatch 6 loss: 0.39026397466659546\n",
            "\tBatch 7 loss: 0.2132374495267868\n",
            "\tBatch 8 loss: 0.3048141598701477\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9426085636670738\n",
            "        AUPRC: 0.9422325491905212\n",
            "        Sensitivity: 0.820276141166687\n",
            "        Specificity: 0.839763879776001\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9422427361324642\n",
            "        AUPRC: 0.9387847185134888\n",
            "        Sensitivity: 0.8175389766693115\n",
            "        Specificity: 0.8555324077606201\n",
            "====================================\n",
            "     Epoch #39\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3326621651649475\n",
            "\tBatch 1 loss: 0.30952587723731995\n",
            "\tBatch 2 loss: 0.33503231406211853\n",
            "\tBatch 3 loss: 0.3480437099933624\n",
            "\tBatch 4 loss: 0.5807898044586182\n",
            "\tBatch 5 loss: 0.26113712787628174\n",
            "\tBatch 6 loss: 0.4018990993499756\n",
            "\tBatch 7 loss: 0.19625884294509888\n",
            "\tBatch 8 loss: 0.3340538442134857\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9361394688148462\n",
            "        AUPRC: 0.9356534481048584\n",
            "        Sensitivity: 0.8249130845069885\n",
            "        Specificity: 0.8320727348327637\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9389322769339719\n",
            "        AUPRC: 0.9395716190338135\n",
            "        Sensitivity: 0.8337478637695312\n",
            "        Specificity: 0.8544052243232727\n",
            "====================================\n",
            "     Epoch #40\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3297193944454193\n",
            "\tBatch 1 loss: 0.3067370653152466\n",
            "\tBatch 2 loss: 0.31248071789741516\n",
            "\tBatch 3 loss: 0.33646783232688904\n",
            "\tBatch 4 loss: 0.5968549847602844\n",
            "\tBatch 5 loss: 0.25906163454055786\n",
            "\tBatch 6 loss: 0.36026841402053833\n",
            "\tBatch 7 loss: 0.20458146929740906\n",
            "\tBatch 8 loss: 0.3077736794948578\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9369504322414455\n",
            "        AUPRC: 0.934417188167572\n",
            "        Sensitivity: 0.8315937519073486\n",
            "        Specificity: 0.8453798294067383\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9362448211918493\n",
            "        AUPRC: 0.9334294199943542\n",
            "        Sensitivity: 0.8400299549102783\n",
            "        Specificity: 0.846138060092926\n",
            "====================================\n",
            "     Epoch #41\n",
            "====================================\n",
            "\tBatch 0 loss: 0.310249924659729\n",
            "\tBatch 1 loss: 0.30674347281455994\n",
            "\tBatch 2 loss: 0.3261305093765259\n",
            "\tBatch 3 loss: 0.31613868474960327\n",
            "\tBatch 4 loss: 0.5714409351348877\n",
            "\tBatch 5 loss: 0.25908395648002625\n",
            "\tBatch 6 loss: 0.38194718956947327\n",
            "\tBatch 7 loss: 0.1884789615869522\n",
            "\tBatch 8 loss: 0.31656986474990845\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 25, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.937899173883033\n",
            "        AUPRC: 0.9352991580963135\n",
            "        Sensitivity: 0.8247063159942627\n",
            "        Specificity: 0.8366583585739136\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.943189272221164\n",
            "        AUPRC: 0.9412679672241211\n",
            "        Sensitivity: 0.8124892711639404\n",
            "        Specificity: 0.8453913331031799\n",
            "====================================\n",
            "     Epoch #42\n",
            "====================================\n",
            "\tBatch 0 loss: 0.32345086336135864\n",
            "\tBatch 1 loss: 0.2985163629055023\n",
            "\tBatch 2 loss: 0.32061073184013367\n",
            "\tBatch 3 loss: 0.30522990226745605\n",
            "\tBatch 4 loss: 0.5791003108024597\n",
            "\tBatch 5 loss: 0.25887179374694824\n",
            "\tBatch 6 loss: 0.3558381199836731\n",
            "\tBatch 7 loss: 0.19695691764354706\n",
            "\tBatch 8 loss: 0.2668020725250244\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 25, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.946022257044334\n",
            "        AUPRC: 0.9433573484420776\n",
            "        Sensitivity: 0.83878093957901\n",
            "        Specificity: 0.8483560085296631\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9448428983397531\n",
            "        AUPRC: 0.9415054321289062\n",
            "        Sensitivity: 0.8369714617729187\n",
            "        Specificity: 0.8391274213790894\n",
            "====================================\n",
            "     Epoch #43\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3035673499107361\n",
            "\tBatch 1 loss: 0.27698826789855957\n",
            "\tBatch 2 loss: 0.3157745897769928\n",
            "\tBatch 3 loss: 0.3169941306114197\n",
            "\tBatch 4 loss: 0.5854333639144897\n",
            "\tBatch 5 loss: 0.2551666498184204\n",
            "\tBatch 6 loss: 0.370851993560791\n",
            "\tBatch 7 loss: 0.19506017863750458\n",
            "\tBatch 8 loss: 0.32528403401374817\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9475821523926982\n",
            "        AUPRC: 0.9429805278778076\n",
            "        Sensitivity: 0.8282347321510315\n",
            "        Specificity: 0.8197020888328552\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9440817262900839\n",
            "        AUPRC: 0.9403026103973389\n",
            "        Sensitivity: 0.8234438300132751\n",
            "        Specificity: 0.8618624806404114\n",
            "====================================\n",
            "     Epoch #44\n",
            "====================================\n",
            "\tBatch 0 loss: 0.2941048741340637\n",
            "\tBatch 1 loss: 0.29775479435920715\n",
            "\tBatch 2 loss: 0.30417245626449585\n",
            "\tBatch 3 loss: 0.32033535838127136\n",
            "\tBatch 4 loss: 0.5784574151039124\n",
            "\tBatch 5 loss: 0.2427477091550827\n",
            "\tBatch 6 loss: 0.367704302072525\n",
            "\tBatch 7 loss: 0.17950263619422913\n",
            "\tBatch 8 loss: 0.32741260528564453\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9426163783666655\n",
            "        AUPRC: 0.941786527633667\n",
            "        Sensitivity: 0.8434412479400635\n",
            "        Specificity: 0.8668420314788818\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9447772408077721\n",
            "        AUPRC: 0.9397960305213928\n",
            "        Sensitivity: 0.8139026165008545\n",
            "        Specificity: 0.8668420314788818\n",
            "====================================\n",
            "     Epoch #45\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3096926808357239\n",
            "\tBatch 1 loss: 0.263778418302536\n",
            "\tBatch 2 loss: 0.29553166031837463\n",
            "\tBatch 3 loss: 0.28504472970962524\n",
            "\tBatch 4 loss: 0.5808922052383423\n",
            "\tBatch 5 loss: 0.2509579062461853\n",
            "\tBatch 6 loss: 0.3385808765888214\n",
            "\tBatch 7 loss: 0.1774354726076126\n",
            "\tBatch 8 loss: 0.28821372985839844\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9471531111214097\n",
            "        AUPRC: 0.9455374479293823\n",
            "        Sensitivity: 0.845410168170929\n",
            "        Specificity: 0.8615509867668152\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 23, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.948195055469161\n",
            "        AUPRC: 0.9462856650352478\n",
            "        Sensitivity: 0.8498404026031494\n",
            "        Specificity: 0.8547167181968689\n",
            "====================================\n",
            "     Epoch #46\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3007979691028595\n",
            "\tBatch 1 loss: 0.28622761368751526\n",
            "\tBatch 2 loss: 0.2894701361656189\n",
            "\tBatch 3 loss: 0.2870360016822815\n",
            "\tBatch 4 loss: 0.5581520795822144\n",
            "\tBatch 5 loss: 0.24446752667427063\n",
            "\tBatch 6 loss: 0.3361733853816986\n",
            "\tBatch 7 loss: 0.1588309407234192\n",
            "\tBatch 8 loss: 0.29155871272087097\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9510629643027093\n",
            "        AUPRC: 0.9481450319290161\n",
            "        Sensitivity: 0.8645387291908264\n",
            "        Specificity: 0.871471643447876\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9512859224850363\n",
            "        AUPRC: 0.9461638927459717\n",
            "        Sensitivity: 0.840217649936676\n",
            "        Specificity: 0.8505472540855408\n",
            "====================================\n",
            "     Epoch #47\n",
            "====================================\n",
            "\tBatch 0 loss: 0.28589928150177\n",
            "\tBatch 1 loss: 0.30362269282341003\n",
            "\tBatch 2 loss: 0.3023149073123932\n",
            "\tBatch 3 loss: 0.30406543612480164\n",
            "\tBatch 4 loss: 0.5755289196968079\n",
            "\tBatch 5 loss: 0.2294454276561737\n",
            "\tBatch 6 loss: 0.3225007653236389\n",
            "\tBatch 7 loss: 0.16644492745399475\n",
            "\tBatch 8 loss: 0.2828854024410248\n",
            "y_hat_long sum: 20, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 25, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9474305443041457\n",
            "        AUPRC: 0.9413825273513794\n",
            "        Sensitivity: 0.8670159578323364\n",
            "        Specificity: 0.8402708768844604\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9532593565682451\n",
            "        AUPRC: 0.9474031925201416\n",
            "        Sensitivity: 0.8374357223510742\n",
            "        Specificity: 0.8626474738121033\n",
            "====================================\n",
            "     Epoch #48\n",
            "====================================\n",
            "\tBatch 0 loss: 0.2669869661331177\n",
            "\tBatch 1 loss: 0.27287620306015015\n",
            "\tBatch 2 loss: 0.2839727997779846\n",
            "\tBatch 3 loss: 0.2775915563106537\n",
            "\tBatch 4 loss: 0.5511963367462158\n",
            "\tBatch 5 loss: 0.23044192790985107\n",
            "\tBatch 6 loss: 0.3559674620628357\n",
            "\tBatch 7 loss: 0.1623958796262741\n",
            "\tBatch 8 loss: 0.2960177958011627\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9520786878701338\n",
            "        AUPRC: 0.9482333064079285\n",
            "        Sensitivity: 0.8434412479400635\n",
            "        Specificity: 0.8716729283332825\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9563931491405278\n",
            "        AUPRC: 0.9526493549346924\n",
            "        Sensitivity: 0.8355838656425476\n",
            "        Specificity: 0.8771843314170837\n",
            "====================================\n",
            "     Epoch #49\n",
            "====================================\n",
            "\tBatch 0 loss: 0.2704758644104004\n",
            "\tBatch 1 loss: 0.26584023237228394\n",
            "\tBatch 2 loss: 0.31520920991897583\n",
            "\tBatch 3 loss: 0.27997085452079773\n",
            "\tBatch 4 loss: 0.586531937122345\n",
            "\tBatch 5 loss: 0.24528121948242188\n",
            "\tBatch 6 loss: 0.31344133615493774\n",
            "\tBatch 7 loss: 0.16601188480854034\n",
            "\tBatch 8 loss: 0.25708112120628357\n",
            "y_hat_long sum: 20, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9528879482185023\n",
            "        AUPRC: 0.9499600529670715\n",
            "        Sensitivity: 0.865164041519165\n",
            "        Specificity: 0.8551768064498901\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 24, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9557455565862397\n",
            "        AUPRC: 0.9526564478874207\n",
            "        Sensitivity: 0.8670191168785095\n",
            "        Specificity: 0.8571417927742004\n",
            "====================================\n",
            "     Epoch #50\n",
            "====================================\n",
            "\tBatch 0 loss: 0.25458478927612305\n",
            "\tBatch 1 loss: 0.27453333139419556\n",
            "\tBatch 2 loss: 0.2878008186817169\n",
            "\tBatch 3 loss: 0.2748916745185852\n",
            "\tBatch 4 loss: 0.5189120173454285\n",
            "\tBatch 5 loss: 0.24669179320335388\n",
            "\tBatch 6 loss: 0.3273248076438904\n",
            "\tBatch 7 loss: 0.14814820885658264\n",
            "\tBatch 8 loss: 0.26053696870803833\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 24, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9571599552612877\n",
            "        AUPRC: 0.9531541466712952\n",
            "        Sensitivity: 0.8471001386642456\n",
            "        Specificity: 0.867723822593689\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 24, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c81e93f84bf1>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# TEST WITH RANDOM DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-57fe25927359>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_handle, test_data_handle, learning_rate, epochs, suspend_train_epochs_threshold, batch_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Not using performance metrics yet in this function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Potential TODO: stop training once desired performance is reached (TBD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0097cb82a268>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, eval_data, dataset_name, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c5a7eef6a396>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, abp, ecg, eeg)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m       \u001b[0mabp_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meeg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c5a7eef6a396>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c5a7eef6a396>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    304\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 306\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    307\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train the model on ABP data\n",
        "\n",
        "# train_set = [\n",
        "#     all_data[0][0:4000].unsqueeze(1),\n",
        "#     all_data[1][0:4000].unsqueeze(1),\n",
        "#     all_data[2][0:4000].unsqueeze(1),\n",
        "#     all_data[3][0:4000]\n",
        "# ]\n",
        "\n",
        "# test_set = [\n",
        "#     all_data[0][4000:].unsqueeze(1),\n",
        "#     all_data[1][4000:].unsqueeze(1),\n",
        "#     all_data[2][4000:].unsqueeze(1),\n",
        "#     all_data[3][4000:]\n",
        "# ]\n",
        "\n",
        "# train(abp_model, train_set, test_set, batch_size=40, epochs=100, learning_rate=0.0001)\n",
        "\n",
        "# TRAIN ON ONLY CASE 819\n",
        "\n",
        "# new_train_set = (\n",
        "#     train_set[0].detach().clone(),\n",
        "#     train_set[1].detach().clone(),\n",
        "#     train_set[2].detach().clone(),\n",
        "#     torch.ones(train_set[3].size(), dtype=torch.float)\n",
        "# )\n",
        "\n",
        "# new_test_set = (\n",
        "#     train_set[0].detach().clone(),\n",
        "#     train_set[1].detach().clone(),\n",
        "#     train_set[2].detach().clone(),\n",
        "#     torch.ones(train_set[3].size(), dtype=torch.float)\n",
        "# )\n",
        "\n",
        "train(abp_model, train_set, test_set, batch_size=40, epochs=100)\n",
        "\n",
        "# TEST WITH RANDOM DATA\n",
        "# sample_size = 400\n",
        "# train_set_r = [\n",
        "#     torch.randn([sample_size, 1, 30000]),\n",
        "#     torch.randn([sample_size, 1, 30000]),\n",
        "#     torch.randn([sample_size, 1, 30000]),\n",
        "#     torch.where(torch.rand([sample_size]) > 0.5, 1.0, 0.0),\n",
        "# ]\n",
        "\n",
        "# train(abp_model, train_set_r, train_set_r, batch_size=40, epochs=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "co7PHmJG0Cu_",
        "outputId": "ace8dd4e-d839-45ae-e47a-afe8cd569c90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 152\n",
            "Statistics for case: 152, 428 total valid samples, 3 positive samples\n",
            "Getting track data for case: 156\n",
            "Statistics for case: 156, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 153\n",
            "Statistics for case: 153, 603 total valid samples, 6 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 156\n",
            "Statistics for case: 156, 2379 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 160\n",
            "Statistics for case: 160, 1277 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 161\n",
            "Statistics for case: 161, 1559 total valid samples, 43 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 163\n",
            "Statistics for case: 163, 265 total valid samples, 54 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 166\n",
            "Statistics for case: 166, 1466 total valid samples, 69 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 167\n",
            "Statistics for case: 167, 789 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 172\n",
            "Statistics for case: 172, 276 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 175\n",
            "Statistics for case: 175, 711 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 177\n",
            "Statistics for case: 177, 1751 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 178\n",
            "Statistics for case: 178, 1114 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 181\n",
            "Statistics for case: 181, 986 total valid samples, 17 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 183\n",
            "Statistics for case: 183, 259 total valid samples, 6 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 184\n",
            "Statistics for case: 184, 3096 total valid samples, 54 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 186\n",
            "Statistics for case: 186, 564 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 190\n",
            "Statistics for case: 190, 1171 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 191\n",
            "Statistics for case: 191, 691 total valid samples, 81 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 195\n",
            "Statistics for case: 195, 1430 total valid samples, 15 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 197\n",
            "Statistics for case: 197, 960 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 198\n",
            "Statistics for case: 198, 1257 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 199\n",
            "Statistics for case: 199, 267 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 200\n",
            "Statistics for case: 200, 677 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 202\n",
            "Statistics for case: 202, 2664 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 203\n",
            "Statistics for case: 203, 394 total valid samples, 130 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 206\n",
            "Statistics for case: 206, 642 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 207\n",
            "Statistics for case: 207, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 208\n",
            "Statistics for case: 208, 654 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 210\n",
            "Statistics for case: 210, 1509 total valid samples, 20 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 218\n",
            "Statistics for case: 218, 1237 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 221\n",
            "Statistics for case: 221, 217 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 222\n",
            "Statistics for case: 222, 627 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 229\n",
            "Statistics for case: 229, 1414 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 232\n",
            "Statistics for case: 232, 1016 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 233\n",
            "Statistics for case: 233, 958 total valid samples, 47 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 234\n",
            "Statistics for case: 234, 1232 total valid samples, 4 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 236\n",
            "Statistics for case: 236, 1764 total valid samples, 16 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 237\n",
            "Statistics for case: 237, 1859 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 239\n",
            "Statistics for case: 239, 1067 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 241\n",
            "Statistics for case: 241, 2333 total valid samples, 114 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 244\n",
            "Statistics for case: 244, 525 total valid samples, 138 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 247\n",
            "Statistics for case: 247, 1820 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 250\n",
            "Statistics for case: 250, 954 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 251\n",
            "Statistics for case: 251, 1663 total valid samples, 487 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 252\n",
            "Statistics for case: 252, 1551 total valid samples, 52 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 256\n",
            "Statistics for case: 256, 1166 total valid samples, 17 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 258\n",
            "Statistics for case: 258, 675 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 261\n",
            "Statistics for case: 261, 642 total valid samples, 27 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 263\n",
            "Statistics for case: 263, 673 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 266\n",
            "Statistics for case: 266, 1536 total valid samples, 9 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 268\n",
            "Statistics for case: 268, 418 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 269\n",
            "Statistics for case: 269, 921 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 270\n",
            "Statistics for case: 270, 184 total valid samples, 43 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 272\n",
            "Statistics for case: 272, 441 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 279\n",
            "Statistics for case: 279, 569 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 281\n",
            "Statistics for case: 281, 843 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 282\n",
            "Statistics for case: 282, 757 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 283\n",
            "Statistics for case: 283, 1143 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 286\n",
            "Statistics for case: 286, 1002 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 287\n",
            "Statistics for case: 287, 657 total valid samples, 9 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 293\n",
            "Statistics for case: 293, 698 total valid samples, 12 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 295\n",
            "Statistics for case: 295, 912 total valid samples, 18 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 296\n",
            "Statistics for case: 296, 935 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 297\n",
            "Statistics for case: 297, 889 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 300\n",
            "Statistics for case: 300, 590 total valid samples, 42 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 302\n",
            "Statistics for case: 302, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 303\n",
            "Statistics for case: 303, 1452 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 304\n",
            "Statistics for case: 304, 1105 total valid samples, 17 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 306\n",
            "Statistics for case: 306, 1644 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 308\n",
            "Statistics for case: 308, 1551 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 309\n",
            "Statistics for case: 309, 666 total valid samples, 191 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 312\n",
            "Statistics for case: 312, 1113 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 316\n",
            "Statistics for case: 316, 355 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 318\n",
            "Statistics for case: 318, 613 total valid samples, 90 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 319\n",
            "Statistics for case: 319, 292 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 321\n",
            "Statistics for case: 321, 799 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 323\n",
            "Statistics for case: 323, 787 total valid samples, 106 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 325\n",
            "Statistics for case: 325, 796 total valid samples, 39 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 326\n",
            "Statistics for case: 326, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 327\n",
            "Statistics for case: 327, 1725 total valid samples, 97 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 330\n",
            "Statistics for case: 330, 572 total valid samples, 12 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 337\n",
            "Statistics for case: 337, 592 total valid samples, 45 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 338\n",
            "Statistics for case: 338, 809 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 342\n",
            "Statistics for case: 342, 1879 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 343\n",
            "Statistics for case: 343, 859 total valid samples, 14 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 345\n",
            "Statistics for case: 345, 1709 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 347\n",
            "Statistics for case: 347, 185 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 348\n",
            "Statistics for case: 348, 820 total valid samples, 7 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 349\n",
            "Statistics for case: 349, 856 total valid samples, 464 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 351\n",
            "Statistics for case: 351, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 353\n",
            "Statistics for case: 353, 1341 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 354\n",
            "Statistics for case: 354, 1005 total valid samples, 167 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 355\n",
            "Statistics for case: 355, 741 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 357\n",
            "Statistics for case: 357, 425 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 358\n",
            "Statistics for case: 358, 979 total valid samples, 1 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 359\n",
            "Statistics for case: 359, 1417 total valid samples, 184 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 362\n",
            "Statistics for case: 362, 1509 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 363\n",
            "Statistics for case: 363, 1340 total valid samples, 48 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 367\n",
            "Statistics for case: 367, 1159 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 369\n",
            "Statistics for case: 369, 1212 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 370\n",
            "Statistics for case: 370, 329 total valid samples, 32 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 371\n",
            "Statistics for case: 371, 683 total valid samples, 6 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 375\n",
            "Statistics for case: 375, 1395 total valid samples, 391 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 380\n",
            "Statistics for case: 380, 643 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 381\n",
            "Statistics for case: 381, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 382\n",
            "Statistics for case: 382, 1460 total valid samples, 9 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 383\n",
            "Statistics for case: 383, 430 total valid samples, 1 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 384\n",
            "Statistics for case: 384, 745 total valid samples, 19 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 386\n",
            "Statistics for case: 386, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 387\n",
            "Statistics for case: 387, 621 total valid samples, 26 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 388\n",
            "Statistics for case: 388, 1057 total valid samples, 12 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 390\n",
            "Statistics for case: 390, 1841 total valid samples, 24 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 397\n",
            "Statistics for case: 397, 1486 total valid samples, 451 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 398\n",
            "Statistics for case: 398, 738 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 402\n",
            "Statistics for case: 402, 1269 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 404\n",
            "Statistics for case: 404, 460 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 405\n",
            "Statistics for case: 405, 461 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 406\n",
            "Statistics for case: 406, 1606 total valid samples, 4 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 408\n",
            "Statistics for case: 408, 717 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 409\n",
            "Statistics for case: 409, 572 total valid samples, 69 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 413\n",
            "Statistics for case: 413, 434 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 415\n",
            "Statistics for case: 415, 536 total valid samples, 39 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 416\n",
            "Statistics for case: 416, 511 total valid samples, 5 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 417\n",
            "Statistics for case: 417, 1576 total valid samples, 11 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 418\n",
            "Statistics for case: 418, 1567 total valid samples, 33 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting track data for case: 419\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-dd32d2310c12>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# Load the case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Analyze positive vs negative ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6f2b2df1b781>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(minutes_ahead, abp_and_ecg_sample_rate_per_second, eeg_sample_rate_per_second, max_num_samples, max_num_cases, from_dir)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0my_moving_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_numerator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabp_data_in_two_seconds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mabp_data_in_two_seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mis_hypotension_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_moving_avg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mhypotension_event_bools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_hypotension_event\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mcase_num_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase_num_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36m_nanmax_dispatcher\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m def _nanmax_dispatcher(a, axis=None, out=None, keepdims=None,\n\u001b[0m\u001b[1;32m    365\u001b[0m                        initial=None, where=None):\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Get cases and find their data counts\n",
        "candy_cases = _get_candidate_cases()\n",
        "left_off = rare_candy_cases[-1]\n",
        "for case_id in candy_cases:\n",
        "  if int(case_id) <= int(left_off):\n",
        "    continue\n",
        "\n",
        "  # Download the case\n",
        "  _download_vital_file(case_id)\n",
        "\n",
        "  # Load the case\n",
        "  all_data = get_data(3, from_dir='.')\n",
        "\n",
        "  # Analyze positive vs negative ratio\n",
        "  length = len(torch.flatten(all_data[3]))\n",
        "  if length > 0:\n",
        "    pos = torch.sum(all_data[3])\n",
        "    if pos > 1:\n",
        "      rare_candy_cases.append(case_id)\n",
        "      case_distribution[case_id] = { 'positive': pos, 'negative': (length - pos) }\n",
        "\n",
        "  # Remove the case file\n",
        "  !rm ./*.vital\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mh4qd7mBzKS",
        "outputId": "0bce412a-3008-4f81-c354-eed433e8e5c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1', '10', '12', '13', '16', '17', '19', '20', '24', '25', '27', '43', '49', '50', '52', '55', '58', '60', '61', '64', '66', '75', '79', '83', '84', '87', '92', '93', '94', '96', '97', '104', '105', '108', '111', '112', '116', '117', '118', '124', '135', '142', '143', '146', '148', '149', '152', '153', '161', '163', '166', '181', '183', '184', '191', '195', '197', '203', '208', '210', '233', '234', '236', '241', '244', '251', '252', '256', '261', '263', '266', '270', '272', '287', '293', '295', '300', '304', '309', '318', '323', '325', '327', '330', '337', '343', '348', '349', '354', '359', '363', '370', '371', '375', '382', '384', '387', '388', '390', '397', '406', '409', '415', '416', '417', '418']\n"
          ]
        }
      ],
      "source": [
        "# pickle the results of case analysis\n",
        "print(rare_candy_cases)\n",
        "\n",
        "afile = open(r'cases_with_positive_samples.pkl', 'wb')\n",
        "pickle.dump(rare_candy_cases, afile)\n",
        "afile.close()\n",
        "\n",
        "bfile = open(r'cases_with_positive_samples_statistics.pkl', 'wb')\n",
        "pickle.dump(case_distribution, bfile)\n",
        "bfile.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hkXFQt7LyJA",
        "outputId": "d83a0bbc-e7fa-48b8-e457-86723dc0b755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106\n"
          ]
        }
      ],
      "source": [
        "print(len(rare_candy_cases))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijm3trFE6UK6"
      },
      "outputs": [],
      "source": [
        "# Put together model as the paper describes (with all resnets)\n",
        "abp_resnet = WaveformResNet(\n",
        "    input_shape=30000,\n",
        "    output_size=32,\n",
        "    data_type='abp'\n",
        ")\n",
        "\n",
        "ecg_resnet = WaveformResNet(\n",
        "    input_shape=30000,\n",
        "    output_size=32,\n",
        "    data_type='ecg'\n",
        ")\n",
        "\n",
        "eeg_resnet = WaveformResNet(\n",
        "    input_shape=7680,\n",
        "    output_size=32,\n",
        "    data_type='eeg'\n",
        ")\n",
        "\n",
        "model = IntraoperativeHypotensionModel(\n",
        "    abp_resnet=abp_resnet,\n",
        "    ecg_resnet=ecg_resnet,\n",
        "    eeg_resnet=eeg_resnet\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFaXjyRytA24"
      },
      "source": [
        "## References\n",
        "Jo YY, Jang JH, Kwon Jm, Lee HC, Jung CW, et al. (2022) Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram: Retrospective study. PLOS ONE 17(8): e0272055. https://doi.org/10.1371/journal.pone.0272055\n",
        "\n",
        "## Acknowledgements\n",
        "* As mentioned in the introduction, this project leveraged the open [vitaldb dataset](https://vitaldb.net/dataset/), and without it would have been impossible in its current form.\n",
        "* Significant inspiration was drawn from [vital db examples](https://github.com/vitaldb/examples)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

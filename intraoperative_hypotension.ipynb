{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invictus125/cs598-final-project/blob/main/intraoperative_hypotension.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqAswNcRhPzL"
      },
      "source": [
        "# A Reproduction of:\n",
        "## Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram\n",
        "\n",
        "Original paper by: Yong-Yeon Jo, Jong-Hwan Jang, Joon-myoung Kwon, Hyung-Chul Lee, Chul-Woo Jung, Seonjeong Byun, Han‐Gil Jeong\n",
        "\n",
        "Reproduction project authored by\n",
        "* Mark Bauer\n",
        "  * mbauer553\n",
        "  * markab5@illinois.edu\n",
        "* Ryan David\n",
        "  * victheone\n",
        "  * invictus125\n",
        "  * radavid2@illinois.edu\n",
        "\n",
        "This project can be found on github https://github.com/invictus125/cs598-final-project.  \n",
        "\n",
        "> Note that this project uses <b>VitalDB, an open biosignal dataset.  All users must agree to the Data Use Agreement below.</b>  If after reviewing the agreement you do not comply, please do not read on and close this window.\n",
        "[Data Use Agreement](https://vitaldb.net/dataset/?query=overview&documentId=13qqajnNZzkN7NZ9aXnaQ-47NWy7kx-a6gbrcEsi-gak&sectionId=h.vcpgs1yemdb5)\n",
        "\n",
        "## Introduction\n",
        "Our project is to perform an approximate reproduction of a paper, which can be found [here](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0272055), on predicting hypotension during surgery from a combination of signals such as mean arterial blood pressure (ABP), electrocardiogram (ECG), and electroencephalogram (EEG) as opposed to ABP alone.  Predicting hypotension is important because it is correlated with many post operation complications, and is actionable.  Please read the original work if you are interested in more detail!\n",
        "\n",
        "## Scope of reproducibility\n",
        "\n",
        "As of the draft on 2024-04-14, we are using what we understand to be a very close replication of the model, with a smaller openly available data set.  Our model currently executes utilizing ABP data from the dataset for predictors and labels, but the EEG and ECG data is randomly created mock data.  The original work also examined more look ahead times.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFa5j_wCxBHy",
        "outputId": "3b1a53c0-4324-419f-f344-8c0cdff86e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torcheval) (4.11.0)\n",
            "Requirement already satisfied: vitaldb in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vitaldb) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vitaldb) (2.31.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.10/dist-packages (from vitaldb) (4.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->vitaldb) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vitaldb) (2024.2.2)\n",
            "Requirement already satisfied: SoundFile>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (0.12.1)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wfdb->vitaldb) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->wfdb->vitaldb) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->vitaldb) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile>=0.10.0->wfdb->vitaldb) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile>=0.10.0->wfdb->vitaldb) (2.22)\n"
          ]
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install torcheval\n",
        "!pip install vitaldb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i62VgNP7ufup"
      },
      "source": [
        "## Methodology - Data\n",
        "\n",
        "Methodology - Data\n",
        "Case: a surgery/operation\n",
        "\n",
        "Track: data observed during a case, consisting of a device and type\n",
        "\n",
        "As of the draft on 2024-04-14 we are getting only ABP data and labels looking ahead one minute.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SXPl5lA6umX-"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from io import StringIO\n",
        "from os import listdir, getcwd\n",
        "from torch import FloatTensor, BoolTensor\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import requests\n",
        "import vitaldb\n",
        "\n",
        "SEGEMENT_LENGTH_SECONDS = 60\n",
        "\n",
        "ABP_TRACK = 'SNUADC/ART'\n",
        "ECG_TRACK = 'SNUADC/ECG_II'\n",
        "EEG_TRACK = 'BIS/EEG1_WAV'\n",
        "\n",
        "RELEVANT_TRACKS = [\n",
        "    ABP_TRACK,\n",
        "    ECG_TRACK,\n",
        "    EEG_TRACK,\n",
        "]\n",
        "\n",
        "def _url_to_reader(url_string):\n",
        "    response = requests.get(url_string)\n",
        "    file = StringIO(response.text)\n",
        "    return csv.DictReader(file, delimiter=',')\n",
        "\n",
        "def get_unique_vals(key, iterable):\n",
        "    return set(map(lambda item: item[key], iterable))\n",
        "\n",
        "def case_filter(case):\n",
        "    return float(case['age']) >= 18.0 and case['ane_type'] == 'General'\n",
        "\n",
        "def _case_track_filter(case_id, case_dict):\n",
        "    track_list = case_dict[case_id]['tracks']\n",
        "    return (\n",
        "        ABP_TRACK in track_list and\n",
        "        ECG_TRACK in track_list and\n",
        "        EEG_TRACK in track_list\n",
        "    )\n",
        "\n",
        "def _get_candidate_cases():\n",
        "    cases_by_id = {}\n",
        "    for case in _url_to_reader('https://api.vitaldb.net/cases'):\n",
        "        if case_filter(case):\n",
        "            case['tracks'] = {}\n",
        "            cases_by_id[case['\\ufeffcaseid']] = case\n",
        "\n",
        "    track_list_reader = _url_to_reader('https://api.vitaldb.net/trks')\n",
        "\n",
        "    for track in track_list_reader:\n",
        "        case_id = track['caseid']\n",
        "        if track['tname'] in RELEVANT_TRACKS:\n",
        "            if cases_by_id.get(case_id):\n",
        "                cases_by_id[case_id]['tracks'][track['tname']] = track['tid']\n",
        "\n",
        "    case_track_filter = partial(_case_track_filter, case_dict=cases_by_id)\n",
        "\n",
        "    return [case_id for case_id in filter(case_track_filter, cases_by_id.keys())]\n",
        "\n",
        "def _get_candidate_cases_from_dir(dir_path):\n",
        "    return [f.split('.vital')[0] for f in listdir(dir_path) if '.vital' in f]\n",
        "\n",
        "def _download_vital_file(case_id):\n",
        "    vf = vitaldb.VitalFile(int(case_id), RELEVANT_TRACKS)\n",
        "    vf.to_vital(case_id+'.vital')\n",
        "\n",
        "def _get_tracks_from_vital_file(path, tracks, sample_rate):\n",
        "    vf = vitaldb.read_vital(path, tracks)\n",
        "    return vf.to_numpy(tracks, sample_rate)\n",
        "\n",
        "\n",
        "def shuffle_tensors(tensors):\n",
        "  shuffled_output = []\n",
        "  shuffled_index = np.array([i for i in range(0, len(tensors[0]))]).astype(int)\n",
        "  np.random.shuffle(shuffled_index)\n",
        "  for t in tensors:\n",
        "    if len(t) > 0:\n",
        "      new_tensor = t.detach().clone()[shuffled_index]\n",
        "      shuffled_output.append(new_tensor.squeeze(0))\n",
        "    else:\n",
        "      shuffled_output.append(t)\n",
        "\n",
        "  return shuffled_output\n",
        "\n",
        "\n",
        "def validate_abp_segment(segment):\n",
        "\n",
        "    return (\n",
        "        not np.isnan(segment).any() and\n",
        "        not (segment > 200).any() and\n",
        "        not (segment < 30).any() and\n",
        "        not ((np.max(segment) - np.min(segment)) < 30) and\n",
        "        not (np.abs(np.diff(segment)) > 30).any() # abrupt changes are assumed to be noise\n",
        "    )\n",
        "\n",
        "def download_data(num_requested_cases):\n",
        "    num_downloaded_cases = 0\n",
        "    candidate_case_ids = _get_candidate_cases()\n",
        "\n",
        "    np.random.shuffle(candidate_case_ids)\n",
        "    for case_id in candidate_case_ids:\n",
        "        print('Downloading case:', case_id)\n",
        "        _download_vital_file(case_id)\n",
        "        num_downloaded_cases = num_downloaded_cases + 1\n",
        "        at_requested = num_downloaded_cases == num_requested_cases\n",
        "        if at_requested:\n",
        "            break\n",
        "\n",
        "    if not at_requested:\n",
        "        print('Requsted cases not reached but all available cases exhausted.  ')\n",
        "\n",
        "def get_data(\n",
        "    minutes_ahead,\n",
        "    abp_and_ecg_sample_rate_per_second=500,\n",
        "    eeg_sample_rate_per_second=128,\n",
        "    max_num_samples=None,\n",
        "    max_num_cases=None,\n",
        "    from_dir=None,\n",
        "):\n",
        "    if from_dir is None:\n",
        "        candidate_case_ids = _get_candidate_cases()\n",
        "    else:\n",
        "        candidate_case_ids = _get_candidate_cases_from_dir(from_dir)\n",
        "\n",
        "    abps = []\n",
        "    ecgs = []\n",
        "    eegs = []\n",
        "    hypotension_event_bools = []\n",
        "\n",
        "    abp_data_in_two_seconds = 2 * abp_and_ecg_sample_rate_per_second\n",
        "\n",
        "    at_max = False\n",
        "\n",
        "    case_count = 0\n",
        "    np.random.shuffle(candidate_case_ids)\n",
        "    for case_id in candidate_case_ids:\n",
        "        case_num_samples = 0\n",
        "        case_num_events = 0\n",
        "\n",
        "        print('Getting track data for case:', case_id)\n",
        "        if from_dir is None:\n",
        "            case_tracks = vitaldb.load_case(int(case_id), RELEVANT_TRACKS[0:2], 1/abp_and_ecg_sample_rate_per_second)\n",
        "        else:\n",
        "            case_tracks = _get_tracks_from_vital_file(f\"{from_dir}/{case_id}.vital\", RELEVANT_TRACKS[0:2], 1/abp_and_ecg_sample_rate_per_second)\n",
        "\n",
        "        abp_track = case_tracks[:,0]\n",
        "        # ecg_track = case_tracks[:,1]\n",
        "\n",
        "        # eeg_track = vitaldb.load_case(int(case_id), RELEVANT_TRACKS[2], 1/eeg_sample_rate_per_second).flatten()\n",
        "\n",
        "        for i in range(\n",
        "            0,\n",
        "            len(abp_track) - abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + (1 + minutes_ahead) * SEGEMENT_LENGTH_SECONDS),\n",
        "            10 * abp_and_ecg_sample_rate_per_second\n",
        "        ):\n",
        "            x_segment = abp_track[i:i + abp_and_ecg_sample_rate_per_second * SEGEMENT_LENGTH_SECONDS]\n",
        "            y_segment_start = i + abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + minutes_ahead * SEGEMENT_LENGTH_SECONDS)\n",
        "            y_segement_end = i + abp_and_ecg_sample_rate_per_second * (SEGEMENT_LENGTH_SECONDS + (minutes_ahead + 1) * SEGEMENT_LENGTH_SECONDS)\n",
        "            y_segment = abp_track[y_segment_start:y_segement_end]\n",
        "\n",
        "            if validate_abp_segment(x_segment) and validate_abp_segment(y_segment):\n",
        "                abps.append(x_segment)\n",
        "\n",
        "                # 2 second moving average\n",
        "                y_numerator = np.nancumsum(y_segment, dtype=np.float32)\n",
        "                y_numerator[abp_data_in_two_seconds:] = y_numerator[abp_data_in_two_seconds:] - y_numerator[:-abp_data_in_two_seconds]\n",
        "                y_moving_avg = y_numerator[abp_data_in_two_seconds - 1:] / abp_data_in_two_seconds\n",
        "\n",
        "                is_hypotension_event = np.nanmax(y_moving_avg) < 65\n",
        "                hypotension_event_bools.append(is_hypotension_event)\n",
        "                case_num_samples = case_num_samples + 1\n",
        "                if(is_hypotension_event):\n",
        "                    case_num_events = case_num_events + 1\n",
        "\n",
        "            at_max_samples = len(hypotension_event_bools) == max_num_samples\n",
        "            if at_max_samples:\n",
        "                break\n",
        "\n",
        "        case_count = case_count + 1\n",
        "        print(f\"Statistics for case: {case_id}, {case_num_samples} total valid samples, {case_num_events} positive samples\")\n",
        "\n",
        "        if at_max_samples or case_count == max_num_cases:\n",
        "            if at_max_samples:\n",
        "                print('Max samples reached')\n",
        "            else:\n",
        "                print('Max cases reached')\n",
        "            at_max = True\n",
        "            break\n",
        "\n",
        "    if not at_max:\n",
        "        print('Max not reached but all available cases exhausted.  ')\n",
        "\n",
        "    print('Converting and shuffling')\n",
        "\n",
        "    abps = FloatTensor(abps)\n",
        "    ecgs = FloatTensor(ecgs)\n",
        "    eegs = FloatTensor(eegs)\n",
        "    hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n",
        "\n",
        "    shuffled_tensors = shuffle_tensors([abps, ecgs, eegs, hypotension_event_bools])\n",
        "\n",
        "    return (\n",
        "        shuffled_tensors[0].unsqueeze(1),\n",
        "        shuffled_tensors[1].unsqueeze(1),\n",
        "        shuffled_tensors[2].unsqueeze(1),\n",
        "        shuffled_tensors[3]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzUkrVE9sERX"
      },
      "source": [
        "## Methodology - Model\n",
        "Our model is an exact reproduction based on the description provided in the original paper.\n",
        "\n",
        "There is a ResNet for each of the three waveform types we handle consisting of:\n",
        "\n",
        "- A CNN encoder layer\n",
        "- 12 residual blocks, each having two convolutions and two batch normalizations. Alternating blocks will halve the length of the data using a max pooling operation. Per the paper, we also added skip connections by summing the input into the output in each residual block.\n",
        "- A fully connected output layer which flattens the channels prior to passing through a NN\n",
        "\n",
        "The model is built such that we can provide one or more ResNets and it will adapt. This is so that we can experiment with varying combinations of input data.\n",
        "\n",
        "Once the input is run through the ResNets, their output is concatenated and passed through a fully connected layer which ends with a sigmoid activation, producing the final prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QsU7Kh2uuGgw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, dim_in, kernel_size=15, stride=1):\n",
        "    super(EncoderBlock, self).__init__()\n",
        "    padding = math.floor(kernel_size / 2.0)\n",
        "    self.conv = nn.Conv1d(1, 1, kernel_size, stride, padding=padding)\n",
        "    self.mp = nn.MaxPool1d(kernel_size, stride, padding)\n",
        "    self.fc = nn.Linear(dim_in, dim_in)\n",
        "    torch.nn.init.normal_(self.fc.weight, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = self.conv(x)\n",
        "    x_hat = self.mp(x_hat)\n",
        "    return self.fc(x_hat)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    in_channels,\n",
        "    out_channels,\n",
        "    size_down,\n",
        "    kernel_size,\n",
        "    stride=1\n",
        "  ):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "\n",
        "    self.size_down = size_down\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    padding = math.floor(kernel_size / 2.0)\n",
        "\n",
        "    self.bn1 = nn.BatchNorm1d(in_channels)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.do = nn.Dropout()\n",
        "    self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size, stride, padding)\n",
        "    self.bn2 = nn.BatchNorm1d(in_channels)\n",
        "    self.act2 = nn.ReLU()\n",
        "    self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "    self.mp = nn.MaxPool1d(kernel_size, padding=padding, stride=2)\n",
        "    self.conv_for_input = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = self.bn1(x)\n",
        "    x_hat = self.act1(x_hat)\n",
        "    x_hat = self.do(x_hat)\n",
        "    x_hat = self.conv1(x_hat)\n",
        "    x_hat = self.bn2(x_hat)\n",
        "    x_hat = self.act2(x_hat)\n",
        "    x_hat = self.conv2(x_hat)\n",
        "\n",
        "    # Adjust dimensions of input if needed for the skip connection\n",
        "    x_input = None\n",
        "    if self.in_channels != self.out_channels:\n",
        "      x_input = self.conv_for_input(x)\n",
        "    else:\n",
        "      x_input = x\n",
        "\n",
        "    x_hat = x_hat + x_input\n",
        "\n",
        "    if self.size_down:\n",
        "      x_hat = self.mp(x_hat)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "class FlattenAndLinearBlock(nn.Module):\n",
        "  def __init__(self, dim_in, dim_out):\n",
        "    super(FlattenAndLinearBlock, self).__init__()\n",
        "    self.fc = nn.Linear(dim_in, dim_out)\n",
        "    torch.nn.init.normal_(self.fc.weight, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_hat = torch.flatten(x, start_dim=1, end_dim=-1)\n",
        "    x_hat = self.fc(x_hat)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "class WaveformResNet(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    input_shape,\n",
        "    output_size,\n",
        "    data_type\n",
        "  ):\n",
        "    super(WaveformResNet, self).__init__()\n",
        "    self.encoder = EncoderBlock(input_shape, 15, 1)\n",
        "    self.res_in_dim = input_shape\n",
        "    self.output_size = output_size\n",
        "    self.data_type = data_type\n",
        "\n",
        "    if data_type not in ['abp', 'ecg', 'eeg']:\n",
        "      raise ValueError('Invalid data type. Must be one of [abp, ecg, eeg]')\n",
        "\n",
        "    # Set up configurations for residual blocks\n",
        "    residual_configs = []\n",
        "    linear_block_input_length = -1\n",
        "    if data_type in ['abp', 'ecg']:\n",
        "      residual_configs = [\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 1,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 15,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "      ]\n",
        "      linear_block_input_length = 469 * 6\n",
        "    else:\n",
        "      residual_configs = [\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 1,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 2,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 7,\n",
        "          'in_channels': 2,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 4,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 4,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': True,\n",
        "        },\n",
        "        {\n",
        "          'kernel_size': 3,\n",
        "          'in_channels': 6,\n",
        "          'out_channels': 6,\n",
        "          'size_down': False,\n",
        "        },\n",
        "      ]\n",
        "      linear_block_input_length = 120 * 6\n",
        "\n",
        "    self.residuals = []\n",
        "    # Build residuals\n",
        "    for i in range(12):\n",
        "      self.residuals.append(\n",
        "        ResidualBlock(\n",
        "          size_down=residual_configs[i]['size_down'],\n",
        "          in_channels=residual_configs[i]['in_channels'],\n",
        "          out_channels=residual_configs[i]['out_channels'],\n",
        "          kernel_size=residual_configs[i]['kernel_size'],\n",
        "        )\n",
        "      )\n",
        "\n",
        "    self.fl_ln = FlattenAndLinearBlock(linear_block_input_length, output_size)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x_hat = self.encoder(x)\n",
        "    x_hat = x\n",
        "\n",
        "    for i in range(len(self.residuals)):\n",
        "      x_hat = self.residuals[i](x_hat)\n",
        "\n",
        "    out = self.fl_ln(x_hat)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "  def get_output_size(self):\n",
        "    return self.output_size\n",
        "\n",
        "\n",
        "class IntraoperativeHypotensionModel(nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    ecg_resnet=None,\n",
        "    abp_resnet=None,\n",
        "    eeg_resnet=None\n",
        "  ):\n",
        "    super(IntraoperativeHypotensionModel, self).__init__()\n",
        "\n",
        "    self.ecg = ecg_resnet\n",
        "    self.abp = abp_resnet\n",
        "    self.eeg = eeg_resnet\n",
        "\n",
        "    self.fc_input_length = 0\n",
        "\n",
        "    if self.ecg is not None:\n",
        "      self.fc_input_length += self.ecg.get_output_size()\n",
        "\n",
        "    if self.abp is not None:\n",
        "      self.fc_input_length += self.abp.get_output_size()\n",
        "\n",
        "    if self.eeg is not None:\n",
        "      self.fc_input_length += self.eeg.get_output_size()\n",
        "\n",
        "    if self.fc_input_length == 0:\n",
        "      raise 'No resnet blocks provided, unable to build model'\n",
        "\n",
        "    self.fc1 = nn.Linear(self.fc_input_length, 16)\n",
        "    self.fc2 = nn.Linear(16, 1)\n",
        "    self.act = nn.Sigmoid()\n",
        "    torch.nn.init.normal_(self.fc1.weight, mean=0.0, std=0.01)\n",
        "    torch.nn.init.normal_(self.fc2.weight, mean=0.0, std=0.01)\n",
        "\n",
        "\n",
        "  def forward(self, abp, ecg, eeg):\n",
        "    ecg_o = torch.Tensor([])\n",
        "    abp_o = torch.Tensor([])\n",
        "    eeg_o = torch.Tensor([])\n",
        "\n",
        "    if self.ecg is not None:\n",
        "      ecg_o = self.ecg(ecg)\n",
        "\n",
        "    if self.abp is not None:\n",
        "      abp_o = self.abp(abp)\n",
        "\n",
        "    if self.eeg is not None:\n",
        "      eeg_o = self.eeg(eeg)\n",
        "\n",
        "    resnet_output = torch.concat([ecg_o, abp_o, eeg_o], dim=1)\n",
        "\n",
        "    intermediate = self.fc1(resnet_output)\n",
        "    intermediate = self.fc2(intermediate)\n",
        "\n",
        "    prediction = self.act(intermediate)\n",
        "\n",
        "    return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WysHaY2luLRS"
      },
      "source": [
        "## Methodology - Training\n",
        "Computational requirements:\n",
        "- At least 50 GB of RAM\n",
        "- GPU instance (we have been experimenting with an A100)\n",
        "\n",
        "The training of this model is fairly straightforward. The paper suggested that we should use Adam as the optimizer and BCE as the loss function, so that is what we have done.\n",
        "\n",
        "Each training epoch will also automatically run evaluation on both the train set and the validation set. See the evaluation methodology for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "r_CMSJdxt3Fp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.nn import BCELoss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def _extract_batch(data, batch_size, batch_number):\n",
        "  start = batch_size * batch_number\n",
        "  end = start + batch_size\n",
        "\n",
        "  if start >= len(data[3]):\n",
        "    return None\n",
        "\n",
        "  return [\n",
        "      data[0][start:end] if len(data[0]) > 0 else None,\n",
        "      data[1][start:end] if len(data[1]) > 0 else None,\n",
        "      data[2][start:end] if len(data[2]) > 0 else None,\n",
        "      data[3][start:end]\n",
        "  ]\n",
        "\n",
        "\n",
        "def _train_one_epoch(\n",
        "  model,\n",
        "  train_data,\n",
        "  optimizer,\n",
        "  criterion,\n",
        "  batch_size=32\n",
        "):\n",
        "  model.train()\n",
        "  loss_history = []\n",
        "\n",
        "  batch_num = 0\n",
        "  batch = _extract_batch(train_data, batch_size, batch_num)\n",
        "  while batch is not None:\n",
        "    optimizer.zero_grad()\n",
        "    abp = batch[0]\n",
        "    ecg = batch[1]\n",
        "    eeg = batch[2]\n",
        "    y = batch[3]\n",
        "    y_hat = model(abp, ecg, eeg)\n",
        "    y_hat = torch.squeeze(y_hat, dim=-1)\n",
        "    loss = criterion(y_hat, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    batch_loss = loss.item()\n",
        "    print(f'\\tBatch {batch_num} loss: {batch_loss}')\n",
        "    loss_history.append(batch_loss)\n",
        "\n",
        "    batch_num += 1\n",
        "    batch = _extract_batch(train_data, batch_size, batch_num)\n",
        "\n",
        "  return loss_history\n",
        "\n",
        "\n",
        "def train(\n",
        "  model,\n",
        "  train_data_handle,\n",
        "  test_data_handle,\n",
        "  learning_rate=0.0001,\n",
        "  epochs=100,\n",
        "  suspend_train_epochs_threshold=5,\n",
        "  batch_size=32\n",
        "):\n",
        "  \"\"\"Trains an IntraoperativeHypotensionModel using the given learning rate for\n",
        "  the given number of epochs\n",
        "\n",
        "  model: the IntraoperativeHypotensionModel to train\n",
        "  train_data_handle: the dataset we will train on\n",
        "  test_data_handle: the dataset we will use for evaluation\n",
        "  learning_rate: the learning rate to use with the Adam optimizer\n",
        "  epochs: the number of epochs to train for\n",
        "  suspend_train_epochs_threshold: training will be suspended if the loss does\n",
        "    not improve for this number of epochs\n",
        "  \"\"\"\n",
        "  if model is None or train_data_handle is None or test_data_handle is None:\n",
        "    raise ValueError(\n",
        "      'model, train_data_handle, and test_data_handle are required for training'\n",
        "    )\n",
        "\n",
        "  criterion = BCELoss()\n",
        "  optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  overall_loss_history = []\n",
        "  consecutive_epochs_without_improvement = 0\n",
        "  for epoch in range(epochs):\n",
        "    print('====================================')\n",
        "    print(f'     Epoch #{epoch + 1}')\n",
        "    print('====================================')\n",
        "\n",
        "    loss_history = _train_one_epoch(\n",
        "      model,\n",
        "      train_data_handle,\n",
        "      optimizer,\n",
        "      criterion,\n",
        "      batch_size\n",
        "    )\n",
        "    eval_model(model, train_data_handle, 'Train', batch_size)\n",
        "    # Not using performance metrics yet in this function.\n",
        "    # Potential TODO: stop training once desired performance is reached (TBD)\n",
        "    performance = eval_model(model, test_data_handle, 'Test', batch_size)\n",
        "\n",
        "    if epoch > 0:\n",
        "      mean_loss = np.mean(loss_history)\n",
        "      overall_loss_history.append(mean_loss)\n",
        "      loss_change = overall_loss_history[epoch - 1] - mean_loss\n",
        "      if loss_change < 0.1:\n",
        "        consecutive_epochs_without_improvement += 1\n",
        "      else:\n",
        "        consecutive_epochs_without_improvement = 0\n",
        "\n",
        "    # if consecutive_epochs_without_improvement >= suspend_train_epochs_threshold:\n",
        "    #   print(f'Training stopping after {epoch+1} epochs.')\n",
        "    #   print(f'Loss did not change for {suspend_train_epochs_threshold} epochs')\n",
        "    #   break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUrbzCCLttmN"
      },
      "source": [
        "##  Methodology - Evaluation\n",
        "The original paper uses four metrics:\n",
        "- AUROC\n",
        "- AUPRC\n",
        "- Sensitivity\n",
        "- Specificity\n",
        "\n",
        "We chose to use the torcheval library for our metrics, except for binary specificity which did not appear to be present in torcheval.\n",
        "\n",
        "In light of that, we have implemented our own binary specificity. Regrettably, this does not yet work properly, but we will do our best to get it functional before our final submission.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jh1fdvuctwgX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torcheval.metrics import BinaryAUROC, BinaryAUPRC, BinaryRecall\n",
        "\n",
        "\n",
        "def _binary_specificity(test, target):\n",
        "  # TN / TN + FP\n",
        "  pinned = torch.where(test >= 0.5, 1.0, 0.0)\n",
        "  pos = torch.where(pinned > 0, 1.0, 0.0)\n",
        "  neg = torch.where(pinned < 1, 1.0, 0.0)\n",
        "  gt_pos = torch.where(target > 0, 1.0, 0.0)\n",
        "  gt_neg = torch.where(target < 1, 1.0, 0.0)\n",
        "  tn = neg + gt_neg\n",
        "  tn = torch.sum(torch.where(tn > 1, 1.0, 0.0), dtype=torch.float)\n",
        "  fp = pos + gt_neg\n",
        "  fp = torch.sum(torch.where(fp > 1, 1.0, 0.0), dtype=torch.float)\n",
        "\n",
        "  return (tn / (tn + fp))\n",
        "\n",
        "\n",
        "def eval_model(\n",
        "  model,\n",
        "  eval_data,\n",
        "  dataset_name,\n",
        "  batch_size=32\n",
        "):\n",
        "  model.eval()\n",
        "\n",
        "  auroc = []\n",
        "  auprc = []\n",
        "  sensitivity = []\n",
        "  specificity = []\n",
        "\n",
        "  f_auroc = BinaryAUROC()\n",
        "  f_auprc = BinaryAUPRC()\n",
        "  f_sensitivity = BinaryRecall()\n",
        "\n",
        "  batch_num = 0\n",
        "  batch = _extract_batch(eval_data, batch_size, batch_num)\n",
        "  while batch is not None:\n",
        "    abp = batch[0]\n",
        "    ecg = batch[1]\n",
        "    eeg = batch[2]\n",
        "    y = batch[3]\n",
        "\n",
        "    y_hat = model(abp, ecg, eeg)\n",
        "    y_hat = y_hat.squeeze(-1)\n",
        "\n",
        "    y_hat_long = torch.where(y_hat >= 0.5, 1.0, 0.0).long()\n",
        "    target_long = y.long()\n",
        "\n",
        "    # print(f'y_hat_long sum: {y_hat_long.sum()}, target_long sum: {target_long.sum()}')\n",
        "\n",
        "    f_auroc.update(y_hat, y)\n",
        "    f_auprc.update(y_hat, y)\n",
        "    f_sensitivity.update(y_hat_long, target_long)\n",
        "\n",
        "    auroc.append(f_auroc.compute())\n",
        "    auprc.append(f_auprc.compute())\n",
        "    sensitivity.append(f_sensitivity.compute())\n",
        "    specificity.append(_binary_specificity(y_hat, y))\n",
        "\n",
        "    batch_num += 1\n",
        "    batch = _extract_batch(eval_data, batch_size, batch_num)\n",
        "\n",
        "  m_auroc = np.mean(auroc)\n",
        "  m_auprc = np.mean(auprc)\n",
        "  m_sensitivity = np.mean(sensitivity)\n",
        "  m_specificity = np.mean(specificity)\n",
        "\n",
        "  print(f'    {dataset_name} data metrics:')\n",
        "  print(f'        AUROC: {m_auroc}')\n",
        "  print(f'        AUPRC: {m_auprc}')\n",
        "  print(f'        Sensitivity: {m_sensitivity}')\n",
        "  print(f'        Specificity: {m_specificity}')\n",
        "\n",
        "  return m_auroc, m_auprc, m_sensitivity, m_specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTM5yt6WtOdP"
      },
      "source": [
        "\n",
        "### Results\n",
        "Thus far we have the ability to build our model and obtain one type of test data in the form we need it for training and evaluation. Please execute the code cells below to see for yourself!\n",
        "\n",
        "Our plans from here until the end of the project are:\n",
        "\n",
        "- Make it possible to obtain the other two data types (ECG and EEG) and use them the same way we are able to use ABP\n",
        "- Get our custom specificity function working properly\n",
        "- Train and evaluate our model using a variety of samples\n",
        "- Train and evaluate our model using varying combinations of the input types\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with only ABP\n",
        "\n",
        "This next section will demonstrate training on only ABP data. It also demonstrates the scalability of our model, allowing us to only use certain resnets if we so choose."
      ],
      "metadata": {
        "id": "FKRTyrR78oeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put together a model using only ABP\n",
        "abp_resnet = WaveformResNet(\n",
        "    input_shape=30000,\n",
        "    output_size=32,\n",
        "    data_type='abp',\n",
        ")\n",
        "\n",
        "abp_model = IntraoperativeHypotensionModel(\n",
        "    abp_resnet=abp_resnet\n",
        ")"
      ],
      "metadata": {
        "id": "-FeXoi8R83dh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Load dataset list and download files\n",
        "case_list_file = open(r'cases_with_positive_samples.pkl', 'rb')\n",
        "case_list = pickle.load(case_list_file)\n",
        "case_list_file.close()\n",
        "\n",
        "for case_id in case_list:\n",
        "  _download_vital_file(case_id)\n",
        "\n",
        "!rm -rf ./abp_only_cases\n",
        "!mkdir ./abp_only_cases\n",
        "!mv ./*.vital ./abp_only_cases/"
      ],
      "metadata": {
        "id": "g220NASowc_o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from ABP dataset at 3-minute before event\n",
        "\n",
        "import pickle\n",
        "\n",
        "def _process_loaded_abp_data(all_data):\n",
        "  pos_indices = torch.nonzero(all_data[3] > 0.5).squeeze(-1)\n",
        "  neg_indices = torch.nonzero(all_data[3] < 0.5).squeeze(-1)\n",
        "  abp_pos = all_data[0].detach().clone()[pos_indices]\n",
        "  abp_neg = all_data[0].detach().clone()[neg_indices]\n",
        "\n",
        "  positive_samples = len(pos_indices)\n",
        "  print(f'block has {positive_samples} positives')\n",
        "  abp_neg = abp_neg[0:positive_samples]\n",
        "\n",
        "  all_abp = torch.concat([abp_neg, abp_pos])\n",
        "  all_labels = torch.concat([torch.zeros([abp_neg.size()[0]]), torch.ones([abp_pos.size()[0]])])\n",
        "  shuffled = shuffle_tensors([all_abp, all_labels])\n",
        "  return [\n",
        "    shuffled[0],\n",
        "    torch.Tensor([]),\n",
        "    torch.Tensor([]),\n",
        "    shuffled[1]\n",
        "  ]\n",
        "\n",
        "# Load dataset list and import data 10 cases at a time\n",
        "case_list_file = open(r'cases_with_positive_samples.pkl', 'rb')\n",
        "case_list = pickle.load(case_list_file)\n",
        "case_list_file.close()\n",
        "\n",
        "data_blocks = []\n",
        "!rm -rf ./load_now\n",
        "!mkdir ./load_now\n",
        "\n",
        "i = 0\n",
        "for case_id in case_list:\n",
        "  !cp ./abp_only_cases/{case_id}.vital ./load_now/\n",
        "  i += 1\n",
        "  if i == 10:\n",
        "    # Cut a block\n",
        "    block = get_data(3, from_dir='./load_now')\n",
        "\n",
        "    processed = _process_loaded_abp_data(block)\n",
        "\n",
        "    data_blocks.append(processed)\n",
        "    !rm ./load_now/*.vital\n",
        "\n",
        "    i = 0\n",
        ""
      ],
      "metadata": {
        "id": "mOg9awpIyxAR",
        "outputId": "73e8ce35-bbe3-490b-ef64-988674f9edb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 19\n",
            "Statistics for case: 19, 2355 total valid samples, 199 positive samples\n",
            "Getting track data for case: 1\n",
            "Statistics for case: 1, 774 total valid samples, 71 positive samples\n",
            "Getting track data for case: 12\n",
            "Statistics for case: 12, 1538 total valid samples, 319 positive samples\n",
            "Getting track data for case: 16\n",
            "Statistics for case: 16, 1042 total valid samples, 11 positive samples\n",
            "Getting track data for case: 13\n",
            "Statistics for case: 13, 529 total valid samples, 31 positive samples\n",
            "Getting track data for case: 20\n",
            "Statistics for case: 20, 2169 total valid samples, 2 positive samples\n",
            "Getting track data for case: 25\n",
            "Statistics for case: 25, 1193 total valid samples, 9 positive samples\n",
            "Getting track data for case: 17\n",
            "Statistics for case: 17, 1725 total valid samples, 77 positive samples\n",
            "Getting track data for case: 10\n",
            "Statistics for case: 10, 1741 total valid samples, 111 positive samples\n",
            "Getting track data for case: 24\n",
            "Statistics for case: 24, 443 total valid samples, 18 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-538bdfa2138a>:199: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block has 848 positives\n",
            "Getting track data for case: 49\n",
            "Statistics for case: 49, 606 total valid samples, 2 positive samples\n",
            "Getting track data for case: 61\n",
            "Statistics for case: 61, 642 total valid samples, 52 positive samples\n",
            "Getting track data for case: 60\n",
            "Statistics for case: 60, 1145 total valid samples, 29 positive samples\n",
            "Getting track data for case: 52\n",
            "Statistics for case: 52, 1292 total valid samples, 130 positive samples\n",
            "Getting track data for case: 58\n",
            "Statistics for case: 58, 1243 total valid samples, 291 positive samples\n",
            "Getting track data for case: 43\n",
            "Statistics for case: 43, 1157 total valid samples, 208 positive samples\n",
            "Getting track data for case: 50\n",
            "Statistics for case: 50, 1323 total valid samples, 23 positive samples\n",
            "Getting track data for case: 27\n",
            "Statistics for case: 27, 1487 total valid samples, 14 positive samples\n",
            "Getting track data for case: 55\n",
            "Statistics for case: 55, 1564 total valid samples, 13 positive samples\n",
            "Getting track data for case: 64\n",
            "Statistics for case: 64, 1114 total valid samples, 28 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 790 positives\n",
            "Getting track data for case: 93\n",
            "Statistics for case: 93, 148 total valid samples, 29 positive samples\n",
            "Getting track data for case: 87\n",
            "Statistics for case: 87, 789 total valid samples, 157 positive samples\n",
            "Getting track data for case: 79\n",
            "Statistics for case: 79, 1658 total valid samples, 48 positive samples\n",
            "Getting track data for case: 96\n",
            "Statistics for case: 96, 3152 total valid samples, 35 positive samples\n",
            "Getting track data for case: 75\n",
            "Statistics for case: 75, 2007 total valid samples, 15 positive samples\n",
            "Getting track data for case: 66\n",
            "Statistics for case: 66, 741 total valid samples, 7 positive samples\n",
            "Getting track data for case: 83\n",
            "Statistics for case: 83, 1244 total valid samples, 108 positive samples\n",
            "Getting track data for case: 84\n",
            "Statistics for case: 84, 1558 total valid samples, 12 positive samples\n",
            "Getting track data for case: 94\n",
            "Statistics for case: 94, 1853 total valid samples, 157 positive samples\n",
            "Getting track data for case: 92\n",
            "Statistics for case: 92, 239 total valid samples, 92 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 660 positives\n",
            "Getting track data for case: 108\n",
            "Statistics for case: 108, 709 total valid samples, 128 positive samples\n",
            "Getting track data for case: 97\n",
            "Statistics for case: 97, 927 total valid samples, 7 positive samples\n",
            "Getting track data for case: 116\n",
            "Statistics for case: 116, 953 total valid samples, 138 positive samples\n",
            "Getting track data for case: 112\n",
            "Statistics for case: 112, 813 total valid samples, 28 positive samples\n",
            "Getting track data for case: 117\n",
            "Statistics for case: 117, 910 total valid samples, 55 positive samples\n",
            "Getting track data for case: 105\n",
            "Statistics for case: 105, 1274 total valid samples, 118 positive samples\n",
            "Getting track data for case: 118\n",
            "Statistics for case: 118, 2452 total valid samples, 224 positive samples\n",
            "Getting track data for case: 104\n",
            "Statistics for case: 104, 771 total valid samples, 309 positive samples\n",
            "Getting track data for case: 111\n",
            "Statistics for case: 111, 1079 total valid samples, 446 positive samples\n",
            "Getting track data for case: 124\n",
            "Statistics for case: 124, 72 total valid samples, 2 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 1455 positives\n",
            "Getting track data for case: 146\n",
            "Statistics for case: 146, 858 total valid samples, 455 positive samples\n",
            "Getting track data for case: 143\n",
            "Statistics for case: 143, 945 total valid samples, 6 positive samples\n",
            "Getting track data for case: 142\n",
            "Statistics for case: 142, 1052 total valid samples, 12 positive samples\n",
            "Getting track data for case: 135\n",
            "Statistics for case: 135, 843 total valid samples, 106 positive samples\n",
            "Getting track data for case: 152\n",
            "Statistics for case: 152, 428 total valid samples, 3 positive samples\n",
            "Getting track data for case: 148\n",
            "Statistics for case: 148, 954 total valid samples, 135 positive samples\n",
            "Getting track data for case: 153\n",
            "Statistics for case: 153, 603 total valid samples, 6 positive samples\n",
            "Getting track data for case: 149\n",
            "Statistics for case: 149, 959 total valid samples, 4 positive samples\n",
            "Getting track data for case: 161\n",
            "Statistics for case: 161, 1559 total valid samples, 43 positive samples\n",
            "Getting track data for case: 163\n",
            "Statistics for case: 163, 265 total valid samples, 54 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 824 positives\n",
            "Getting track data for case: 191\n",
            "Statistics for case: 191, 691 total valid samples, 81 positive samples\n",
            "Getting track data for case: 208\n",
            "Statistics for case: 208, 654 total valid samples, 10 positive samples\n",
            "Getting track data for case: 197\n",
            "Statistics for case: 197, 960 total valid samples, 10 positive samples\n",
            "Getting track data for case: 184\n",
            "Statistics for case: 184, 3096 total valid samples, 54 positive samples\n",
            "Getting track data for case: 210\n",
            "Statistics for case: 210, 1509 total valid samples, 20 positive samples\n",
            "Getting track data for case: 181\n",
            "Statistics for case: 181, 986 total valid samples, 17 positive samples\n",
            "Getting track data for case: 195\n",
            "Statistics for case: 195, 1430 total valid samples, 15 positive samples\n",
            "Getting track data for case: 183\n",
            "Statistics for case: 183, 259 total valid samples, 6 positive samples\n",
            "Getting track data for case: 203\n",
            "Statistics for case: 203, 394 total valid samples, 130 positive samples\n",
            "Getting track data for case: 166\n",
            "Statistics for case: 166, 1466 total valid samples, 69 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 412 positives\n",
            "Getting track data for case: 263\n",
            "Statistics for case: 263, 673 total valid samples, 10 positive samples\n",
            "Getting track data for case: 241\n",
            "Statistics for case: 241, 2333 total valid samples, 114 positive samples\n",
            "Getting track data for case: 252\n",
            "Statistics for case: 252, 1551 total valid samples, 52 positive samples\n",
            "Getting track data for case: 251\n",
            "Statistics for case: 251, 1663 total valid samples, 487 positive samples\n",
            "Getting track data for case: 261\n",
            "Statistics for case: 261, 642 total valid samples, 27 positive samples\n",
            "Getting track data for case: 234\n",
            "Statistics for case: 234, 1232 total valid samples, 4 positive samples\n",
            "Getting track data for case: 244\n",
            "Statistics for case: 244, 525 total valid samples, 138 positive samples\n",
            "Getting track data for case: 233\n",
            "Statistics for case: 233, 958 total valid samples, 47 positive samples\n",
            "Getting track data for case: 236\n",
            "Statistics for case: 236, 1764 total valid samples, 16 positive samples\n",
            "Getting track data for case: 256\n",
            "Statistics for case: 256, 1166 total valid samples, 17 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 912 positives\n",
            "Getting track data for case: 295\n",
            "Statistics for case: 295, 912 total valid samples, 18 positive samples\n",
            "Getting track data for case: 293\n",
            "Statistics for case: 293, 698 total valid samples, 12 positive samples\n",
            "Getting track data for case: 270\n",
            "Statistics for case: 270, 184 total valid samples, 43 positive samples\n",
            "Getting track data for case: 287\n",
            "Statistics for case: 287, 657 total valid samples, 9 positive samples\n",
            "Getting track data for case: 266\n",
            "Statistics for case: 266, 1536 total valid samples, 9 positive samples\n",
            "Getting track data for case: 309\n",
            "Statistics for case: 309, 666 total valid samples, 191 positive samples\n",
            "Getting track data for case: 304\n",
            "Statistics for case: 304, 1105 total valid samples, 17 positive samples\n",
            "Getting track data for case: 272\n",
            "Statistics for case: 272, 441 total valid samples, 10 positive samples\n",
            "Getting track data for case: 300\n",
            "Statistics for case: 300, 590 total valid samples, 42 positive samples\n",
            "Getting track data for case: 318\n",
            "Statistics for case: 318, 613 total valid samples, 90 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 441 positives\n",
            "Getting track data for case: 327\n",
            "Statistics for case: 327, 1725 total valid samples, 97 positive samples\n",
            "Getting track data for case: 337\n",
            "Statistics for case: 337, 592 total valid samples, 45 positive samples\n",
            "Getting track data for case: 349\n",
            "Statistics for case: 349, 856 total valid samples, 464 positive samples\n",
            "Getting track data for case: 323\n",
            "Statistics for case: 323, 787 total valid samples, 106 positive samples\n",
            "Getting track data for case: 343\n",
            "Statistics for case: 343, 859 total valid samples, 14 positive samples\n",
            "Getting track data for case: 354\n",
            "Statistics for case: 354, 1005 total valid samples, 167 positive samples\n",
            "Getting track data for case: 359\n",
            "Statistics for case: 359, 1417 total valid samples, 184 positive samples\n",
            "Getting track data for case: 330\n",
            "Statistics for case: 330, 572 total valid samples, 12 positive samples\n",
            "Getting track data for case: 325\n",
            "Statistics for case: 325, 796 total valid samples, 39 positive samples\n",
            "Getting track data for case: 348\n",
            "Statistics for case: 348, 820 total valid samples, 7 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 1135 positives\n",
            "Getting track data for case: 387\n",
            "Statistics for case: 387, 621 total valid samples, 26 positive samples\n",
            "Getting track data for case: 363\n",
            "Statistics for case: 363, 1340 total valid samples, 48 positive samples\n",
            "Getting track data for case: 397\n",
            "Statistics for case: 397, 1486 total valid samples, 451 positive samples\n",
            "Getting track data for case: 375\n",
            "Statistics for case: 375, 1395 total valid samples, 391 positive samples\n",
            "Getting track data for case: 384\n",
            "Statistics for case: 384, 745 total valid samples, 19 positive samples\n",
            "Getting track data for case: 388\n",
            "Statistics for case: 388, 1057 total valid samples, 12 positive samples\n",
            "Getting track data for case: 371\n",
            "Statistics for case: 371, 683 total valid samples, 6 positive samples\n",
            "Getting track data for case: 370\n",
            "Statistics for case: 370, 329 total valid samples, 32 positive samples\n",
            "Getting track data for case: 382\n",
            "Statistics for case: 382, 1460 total valid samples, 9 positive samples\n",
            "Getting track data for case: 390\n",
            "Statistics for case: 390, 1841 total valid samples, 24 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Converting and shuffling\n",
            "block has 1018 positives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pickle the data blocks so we can easily load them later and not have to re-generate them\n",
        "afile = open(r'abp_train_data_sets.pkl', 'wb')\n",
        "pickle.dump(data_blocks, afile)\n",
        "afile.close()\n",
        "data_blocks = None"
      ],
      "metadata": {
        "id": "KUB3PvfwBNM_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data from the pickle file\n",
        "data_file = open(r'abp_train_data_sets.pkl', 'rb')\n",
        "data_blocks = pickle.load(data_file)\n",
        "data_file.close()\n",
        "\n",
        "# Split into train and eval sets\n",
        "eval_dataset = data_blocks[-1]\n",
        "train_dataset = data_blocks[0:-1]\n",
        "\n",
        "data_blocks = None # GC\n",
        "\n",
        "# Train block by block\n",
        "for idx in range(len(train_dataset)):\n",
        "  print(f'Training on data block {idx}!!!!')\n",
        "  train(abp_model, train_dataset[idx], eval_dataset, batch_size=32, epochs=11)"
      ],
      "metadata": {
        "id": "fEbEykJiCG-D",
        "outputId": "415ba123-4bb4-4c40-d517-38a9ff46550d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on data block 0!!!!\n",
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 0.7028172016143799\n",
            "\tBatch 1 loss: 0.6982393264770508\n",
            "\tBatch 2 loss: 0.6839103698730469\n",
            "\tBatch 3 loss: 0.7009466290473938\n",
            "\tBatch 4 loss: 0.6991347670555115\n",
            "\tBatch 5 loss: 0.6651118397712708\n",
            "\tBatch 6 loss: 0.6900951862335205\n",
            "\tBatch 7 loss: 0.705383837223053\n",
            "\tBatch 8 loss: 0.6577776670455933\n",
            "\tBatch 9 loss: 0.6961024403572083\n",
            "\tBatch 10 loss: 0.7036084532737732\n",
            "\tBatch 11 loss: 0.7230327129364014\n",
            "\tBatch 12 loss: 0.6965652704238892\n",
            "\tBatch 13 loss: 0.6878703236579895\n",
            "\tBatch 14 loss: 0.7338614463806152\n",
            "\tBatch 15 loss: 0.6755515933036804\n",
            "\tBatch 16 loss: 0.6897897124290466\n",
            "\tBatch 17 loss: 0.7043782472610474\n",
            "\tBatch 18 loss: 0.6712531447410583\n",
            "\tBatch 19 loss: 0.6650838851928711\n",
            "\tBatch 20 loss: 0.7071044445037842\n",
            "\tBatch 21 loss: 0.6916097402572632\n",
            "\tBatch 22 loss: 0.6873482465744019\n",
            "\tBatch 23 loss: 0.6752777099609375\n",
            "\tBatch 24 loss: 0.7110837697982788\n",
            "\tBatch 25 loss: 0.7064640522003174\n",
            "\tBatch 26 loss: 0.6777164936065674\n",
            "\tBatch 27 loss: 0.700722336769104\n",
            "\tBatch 28 loss: 0.6950640678405762\n",
            "\tBatch 29 loss: 0.6820744276046753\n",
            "\tBatch 30 loss: 0.6948636174201965\n",
            "\tBatch 31 loss: 0.7041798233985901\n",
            "\tBatch 32 loss: 0.681863009929657\n",
            "\tBatch 33 loss: 0.6875184774398804\n",
            "\tBatch 34 loss: 0.6797133684158325\n",
            "\tBatch 35 loss: 0.6874408721923828\n",
            "\tBatch 36 loss: 0.6832254528999329\n",
            "\tBatch 37 loss: 0.7028766870498657\n",
            "\tBatch 38 loss: 0.7067825198173523\n",
            "\tBatch 39 loss: 0.6612104177474976\n",
            "\tBatch 40 loss: 0.6800569891929626\n",
            "\tBatch 41 loss: 0.6637239456176758\n",
            "\tBatch 42 loss: 0.633089005947113\n",
            "\tBatch 43 loss: 0.6939460635185242\n",
            "\tBatch 44 loss: 0.676472544670105\n",
            "\tBatch 45 loss: 0.6865904927253723\n",
            "\tBatch 46 loss: 0.6932474970817566\n",
            "\tBatch 47 loss: 0.6408879160881042\n",
            "\tBatch 48 loss: 0.6231887340545654\n",
            "\tBatch 49 loss: 0.7172536253929138\n",
            "\tBatch 50 loss: 0.7014454007148743\n",
            "\tBatch 51 loss: 0.6618095636367798\n",
            "\tBatch 52 loss: 0.7151521444320679\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9024314128128078\n",
            "        AUPRC: 0.9023133516311646\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7245343927776513\n",
            "        AUPRC: 0.7140810489654541\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6599609851837158\n",
            "\tBatch 1 loss: 0.7592914700508118\n",
            "\tBatch 2 loss: 0.6411876678466797\n",
            "\tBatch 3 loss: 0.7341111898422241\n",
            "\tBatch 4 loss: 0.7195131778717041\n",
            "\tBatch 5 loss: 0.6332162618637085\n",
            "\tBatch 6 loss: 0.690932035446167\n",
            "\tBatch 7 loss: 0.7131512761116028\n",
            "\tBatch 8 loss: 0.6475414037704468\n",
            "\tBatch 9 loss: 0.6949045062065125\n",
            "\tBatch 10 loss: 0.6999756097793579\n",
            "\tBatch 11 loss: 0.7154929637908936\n",
            "\tBatch 12 loss: 0.6923824548721313\n",
            "\tBatch 13 loss: 0.6859470009803772\n",
            "\tBatch 14 loss: 0.7185484766960144\n",
            "\tBatch 15 loss: 0.6782954931259155\n",
            "\tBatch 16 loss: 0.6883853673934937\n",
            "\tBatch 17 loss: 0.6984448432922363\n",
            "\tBatch 18 loss: 0.6790671348571777\n",
            "\tBatch 19 loss: 0.6766431927680969\n",
            "\tBatch 20 loss: 0.6988130211830139\n",
            "\tBatch 21 loss: 0.6901029944419861\n",
            "\tBatch 22 loss: 0.6886475086212158\n",
            "\tBatch 23 loss: 0.6820981502532959\n",
            "\tBatch 24 loss: 0.7018232345581055\n",
            "\tBatch 25 loss: 0.6992656588554382\n",
            "\tBatch 26 loss: 0.6813257932662964\n",
            "\tBatch 27 loss: 0.695151150226593\n",
            "\tBatch 28 loss: 0.692832350730896\n",
            "\tBatch 29 loss: 0.6843112707138062\n",
            "\tBatch 30 loss: 0.6926736831665039\n",
            "\tBatch 31 loss: 0.6993420124053955\n",
            "\tBatch 32 loss: 0.6822683811187744\n",
            "\tBatch 33 loss: 0.6863938570022583\n",
            "\tBatch 34 loss: 0.6797915697097778\n",
            "\tBatch 35 loss: 0.6863946318626404\n",
            "\tBatch 36 loss: 0.6818217635154724\n",
            "\tBatch 37 loss: 0.7010525465011597\n",
            "\tBatch 38 loss: 0.7044010758399963\n",
            "\tBatch 39 loss: 0.6607316136360168\n",
            "\tBatch 40 loss: 0.6785910129547119\n",
            "\tBatch 41 loss: 0.6620486974716187\n",
            "\tBatch 42 loss: 0.6328243017196655\n",
            "\tBatch 43 loss: 0.6925493478775024\n",
            "\tBatch 44 loss: 0.6751880645751953\n",
            "\tBatch 45 loss: 0.6849640011787415\n",
            "\tBatch 46 loss: 0.690759539604187\n",
            "\tBatch 47 loss: 0.6406735181808472\n",
            "\tBatch 48 loss: 0.6239179372787476\n",
            "\tBatch 49 loss: 0.7139781713485718\n",
            "\tBatch 50 loss: 0.6981911063194275\n",
            "\tBatch 51 loss: 0.6596741676330566\n",
            "\tBatch 52 loss: 0.7117797136306763\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9056601404669878\n",
            "        AUPRC: 0.9053590893745422\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7340311025447013\n",
            "        AUPRC: 0.7277218103408813\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6577972173690796\n",
            "\tBatch 1 loss: 0.7571585178375244\n",
            "\tBatch 2 loss: 0.6396528482437134\n",
            "\tBatch 3 loss: 0.7332937717437744\n",
            "\tBatch 4 loss: 0.7187819480895996\n",
            "\tBatch 5 loss: 0.6302107572555542\n",
            "\tBatch 6 loss: 0.6889525651931763\n",
            "\tBatch 7 loss: 0.712734580039978\n",
            "\tBatch 8 loss: 0.6445359587669373\n",
            "\tBatch 9 loss: 0.6937772035598755\n",
            "\tBatch 10 loss: 0.6991786360740662\n",
            "\tBatch 11 loss: 0.7156662940979004\n",
            "\tBatch 12 loss: 0.6913759708404541\n",
            "\tBatch 13 loss: 0.684315025806427\n",
            "\tBatch 14 loss: 0.7189529538154602\n",
            "\tBatch 15 loss: 0.6755503416061401\n",
            "\tBatch 16 loss: 0.6865550875663757\n",
            "\tBatch 17 loss: 0.6982252597808838\n",
            "\tBatch 18 loss: 0.6764690279960632\n",
            "\tBatch 19 loss: 0.6742334365844727\n",
            "\tBatch 20 loss: 0.6983965635299683\n",
            "\tBatch 21 loss: 0.6881987452507019\n",
            "\tBatch 22 loss: 0.6873000264167786\n",
            "\tBatch 23 loss: 0.6803845167160034\n",
            "\tBatch 24 loss: 0.7009625434875488\n",
            "\tBatch 25 loss: 0.6982250809669495\n",
            "\tBatch 26 loss: 0.6790595054626465\n",
            "\tBatch 27 loss: 0.693170964717865\n",
            "\tBatch 28 loss: 0.6914281845092773\n",
            "\tBatch 29 loss: 0.6831663846969604\n",
            "\tBatch 30 loss: 0.6913120746612549\n",
            "\tBatch 31 loss: 0.6972881555557251\n",
            "\tBatch 32 loss: 0.6806270480155945\n",
            "\tBatch 33 loss: 0.6848055720329285\n",
            "\tBatch 34 loss: 0.6780223250389099\n",
            "\tBatch 35 loss: 0.6847343444824219\n",
            "\tBatch 36 loss: 0.6795305013656616\n",
            "\tBatch 37 loss: 0.6996421217918396\n",
            "\tBatch 38 loss: 0.7025274038314819\n",
            "\tBatch 39 loss: 0.6589380502700806\n",
            "\tBatch 40 loss: 0.6765527725219727\n",
            "\tBatch 41 loss: 0.6591967344284058\n",
            "\tBatch 42 loss: 0.6294140219688416\n",
            "\tBatch 43 loss: 0.6911770105361938\n",
            "\tBatch 44 loss: 0.673826277256012\n",
            "\tBatch 45 loss: 0.6839804649353027\n",
            "\tBatch 46 loss: 0.6894752383232117\n",
            "\tBatch 47 loss: 0.6373597979545593\n",
            "\tBatch 48 loss: 0.6212799549102783\n",
            "\tBatch 49 loss: 0.7141173481941223\n",
            "\tBatch 50 loss: 0.6971185803413391\n",
            "\tBatch 51 loss: 0.6576833128929138\n",
            "\tBatch 52 loss: 0.7112734913825989\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9081263426204248\n",
            "        AUPRC: 0.9076322913169861\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7407662410771338\n",
            "        AUPRC: 0.7389142513275146\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6556494235992432\n",
            "\tBatch 1 loss: 0.7564597129821777\n",
            "\tBatch 2 loss: 0.6373399496078491\n",
            "\tBatch 3 loss: 0.7319911122322083\n",
            "\tBatch 4 loss: 0.7162346839904785\n",
            "\tBatch 5 loss: 0.6280982494354248\n",
            "\tBatch 6 loss: 0.6862975358963013\n",
            "\tBatch 7 loss: 0.7091076374053955\n",
            "\tBatch 8 loss: 0.6436899304389954\n",
            "\tBatch 9 loss: 0.691141664981842\n",
            "\tBatch 10 loss: 0.6954150199890137\n",
            "\tBatch 11 loss: 0.7121329307556152\n",
            "\tBatch 12 loss: 0.6889283061027527\n",
            "\tBatch 13 loss: 0.6825821399688721\n",
            "\tBatch 14 loss: 0.7135486006736755\n",
            "\tBatch 15 loss: 0.6745529174804688\n",
            "\tBatch 16 loss: 0.6847671866416931\n",
            "\tBatch 17 loss: 0.6960314512252808\n",
            "\tBatch 18 loss: 0.6779685616493225\n",
            "\tBatch 19 loss: 0.6773300766944885\n",
            "\tBatch 20 loss: 0.695012092590332\n",
            "\tBatch 21 loss: 0.6857116222381592\n",
            "\tBatch 22 loss: 0.6860485076904297\n",
            "\tBatch 23 loss: 0.6802784204483032\n",
            "\tBatch 24 loss: 0.6985567808151245\n",
            "\tBatch 25 loss: 0.6959071755409241\n",
            "\tBatch 26 loss: 0.6764767169952393\n",
            "\tBatch 27 loss: 0.6899714469909668\n",
            "\tBatch 28 loss: 0.6897009611129761\n",
            "\tBatch 29 loss: 0.6812261343002319\n",
            "\tBatch 30 loss: 0.6893923282623291\n",
            "\tBatch 31 loss: 0.6950168013572693\n",
            "\tBatch 32 loss: 0.6775842308998108\n",
            "\tBatch 33 loss: 0.6816660761833191\n",
            "\tBatch 34 loss: 0.6746821403503418\n",
            "\tBatch 35 loss: 0.6817535161972046\n",
            "\tBatch 36 loss: 0.6750274300575256\n",
            "\tBatch 37 loss: 0.6989398002624512\n",
            "\tBatch 38 loss: 0.702153742313385\n",
            "\tBatch 39 loss: 0.6527538895606995\n",
            "\tBatch 40 loss: 0.6724711060523987\n",
            "\tBatch 41 loss: 0.6522064208984375\n",
            "\tBatch 42 loss: 0.61823570728302\n",
            "\tBatch 43 loss: 0.6910806894302368\n",
            "\tBatch 44 loss: 0.671146810054779\n",
            "\tBatch 45 loss: 0.6830792427062988\n",
            "\tBatch 46 loss: 0.6893858313560486\n",
            "\tBatch 47 loss: 0.6308501362800598\n",
            "\tBatch 48 loss: 0.6141384840011597\n",
            "\tBatch 49 loss: 0.7173205614089966\n",
            "\tBatch 50 loss: 0.6977110505104065\n",
            "\tBatch 51 loss: 0.6547942757606506\n",
            "\tBatch 52 loss: 0.711275041103363\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9103307284259955\n",
            "        AUPRC: 0.9097086191177368\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.750723125214483\n",
            "        AUPRC: 0.749646782875061\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.651974618434906\n",
            "\tBatch 1 loss: 0.7555592060089111\n",
            "\tBatch 2 loss: 0.6346726417541504\n",
            "\tBatch 3 loss: 0.7275502681732178\n",
            "\tBatch 4 loss: 0.7110711336135864\n",
            "\tBatch 5 loss: 0.6270920634269714\n",
            "\tBatch 6 loss: 0.6808192133903503\n",
            "\tBatch 7 loss: 0.7011623382568359\n",
            "\tBatch 8 loss: 0.645220160484314\n",
            "\tBatch 9 loss: 0.6858592629432678\n",
            "\tBatch 10 loss: 0.6884955763816833\n",
            "\tBatch 11 loss: 0.702998697757721\n",
            "\tBatch 12 loss: 0.6847273111343384\n",
            "\tBatch 13 loss: 0.6803184747695923\n",
            "\tBatch 14 loss: 0.6995786428451538\n",
            "\tBatch 15 loss: 0.6763253211975098\n",
            "\tBatch 16 loss: 0.6829422116279602\n",
            "\tBatch 17 loss: 0.6907637715339661\n",
            "\tBatch 18 loss: 0.6865948438644409\n",
            "\tBatch 19 loss: 0.6878162026405334\n",
            "\tBatch 20 loss: 0.688285231590271\n",
            "\tBatch 21 loss: 0.6818336844444275\n",
            "\tBatch 22 loss: 0.6839820742607117\n",
            "\tBatch 23 loss: 0.6787875294685364\n",
            "\tBatch 24 loss: 0.6965891718864441\n",
            "\tBatch 25 loss: 0.6943936944007874\n",
            "\tBatch 26 loss: 0.6690674424171448\n",
            "\tBatch 27 loss: 0.6867294311523438\n",
            "\tBatch 28 loss: 0.6868367195129395\n",
            "\tBatch 29 loss: 0.6750085353851318\n",
            "\tBatch 30 loss: 0.686702311038971\n",
            "\tBatch 31 loss: 0.6948636770248413\n",
            "\tBatch 32 loss: 0.6692565083503723\n",
            "\tBatch 33 loss: 0.6752498745918274\n",
            "\tBatch 34 loss: 0.6654322147369385\n",
            "\tBatch 35 loss: 0.6760685443878174\n",
            "\tBatch 36 loss: 0.666772723197937\n",
            "\tBatch 37 loss: 0.700637936592102\n",
            "\tBatch 38 loss: 0.7036666870117188\n",
            "\tBatch 39 loss: 0.6409902572631836\n",
            "\tBatch 40 loss: 0.6657319664955139\n",
            "\tBatch 41 loss: 0.6402671933174133\n",
            "\tBatch 42 loss: 0.6011084914207458\n",
            "\tBatch 43 loss: 0.6905678510665894\n",
            "\tBatch 44 loss: 0.6678563356399536\n",
            "\tBatch 45 loss: 0.6824793815612793\n",
            "\tBatch 46 loss: 0.6890086531639099\n",
            "\tBatch 47 loss: 0.6224030256271362\n",
            "\tBatch 48 loss: 0.6069348454475403\n",
            "\tBatch 49 loss: 0.7182371020317078\n",
            "\tBatch 50 loss: 0.6950228810310364\n",
            "\tBatch 51 loss: 0.6497179865837097\n",
            "\tBatch 52 loss: 0.70595782995224\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9106794832922068\n",
            "        AUPRC: 0.9092391133308411\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7655540599832066\n",
            "        AUPRC: 0.7642943859100342\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6458274126052856\n",
            "\tBatch 1 loss: 0.741895854473114\n",
            "\tBatch 2 loss: 0.6325997710227966\n",
            "\tBatch 3 loss: 0.7134206891059875\n",
            "\tBatch 4 loss: 0.6965093612670898\n",
            "\tBatch 5 loss: 0.6315898895263672\n",
            "\tBatch 6 loss: 0.6713839769363403\n",
            "\tBatch 7 loss: 0.6857495903968811\n",
            "\tBatch 8 loss: 0.6543511152267456\n",
            "\tBatch 9 loss: 0.6785715222358704\n",
            "\tBatch 10 loss: 0.6778558492660522\n",
            "\tBatch 11 loss: 0.6872193813323975\n",
            "\tBatch 12 loss: 0.678776204586029\n",
            "\tBatch 13 loss: 0.6768338680267334\n",
            "\tBatch 14 loss: 0.6820309162139893\n",
            "\tBatch 15 loss: 0.6761720776557922\n",
            "\tBatch 16 loss: 0.6779958009719849\n",
            "\tBatch 17 loss: 0.6860510110855103\n",
            "\tBatch 18 loss: 0.6870226263999939\n",
            "\tBatch 19 loss: 0.6842206716537476\n",
            "\tBatch 20 loss: 0.6860641837120056\n",
            "\tBatch 21 loss: 0.6715188026428223\n",
            "\tBatch 22 loss: 0.6740191578865051\n",
            "\tBatch 23 loss: 0.6582867503166199\n",
            "\tBatch 24 loss: 0.7120809555053711\n",
            "\tBatch 25 loss: 0.7070159912109375\n",
            "\tBatch 26 loss: 0.6469810605049133\n",
            "\tBatch 27 loss: 0.6893867254257202\n",
            "\tBatch 28 loss: 0.6851431131362915\n",
            "\tBatch 29 loss: 0.6623148322105408\n",
            "\tBatch 30 loss: 0.6823470592498779\n",
            "\tBatch 31 loss: 0.6907909512519836\n",
            "\tBatch 32 loss: 0.6589140295982361\n",
            "\tBatch 33 loss: 0.6665586829185486\n",
            "\tBatch 34 loss: 0.6594486236572266\n",
            "\tBatch 35 loss: 0.6681837439537048\n",
            "\tBatch 36 loss: 0.6569228172302246\n",
            "\tBatch 37 loss: 0.6892403960227966\n",
            "\tBatch 38 loss: 0.688493549823761\n",
            "\tBatch 39 loss: 0.6442277431488037\n",
            "\tBatch 40 loss: 0.6584864854812622\n",
            "\tBatch 41 loss: 0.6316320300102234\n",
            "\tBatch 42 loss: 0.5920802354812622\n",
            "\tBatch 43 loss: 0.6858966946601868\n",
            "\tBatch 44 loss: 0.6625090837478638\n",
            "\tBatch 45 loss: 0.682773232460022\n",
            "\tBatch 46 loss: 0.6892141699790955\n",
            "\tBatch 47 loss: 0.6100836396217346\n",
            "\tBatch 48 loss: 0.5953936576843262\n",
            "\tBatch 49 loss: 0.7200438380241394\n",
            "\tBatch 50 loss: 0.6889302730560303\n",
            "\tBatch 51 loss: 0.6382872462272644\n",
            "\tBatch 52 loss: 0.6906507015228271\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8999302892508952\n",
            "        AUPRC: 0.8956764936447144\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7825074386818174\n",
            "        AUPRC: 0.7697328925132751\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6339782476425171\n",
            "\tBatch 1 loss: 0.7095437049865723\n",
            "\tBatch 2 loss: 0.6347458362579346\n",
            "\tBatch 3 loss: 0.6860229969024658\n",
            "\tBatch 4 loss: 0.6717061996459961\n",
            "\tBatch 5 loss: 0.6497461199760437\n",
            "\tBatch 6 loss: 0.6595972180366516\n",
            "\tBatch 7 loss: 0.664810061454773\n",
            "\tBatch 8 loss: 0.6651385426521301\n",
            "\tBatch 9 loss: 0.6686651110649109\n",
            "\tBatch 10 loss: 0.6634731292724609\n",
            "\tBatch 11 loss: 0.6782210469245911\n",
            "\tBatch 12 loss: 0.6667075157165527\n",
            "\tBatch 13 loss: 0.661622941493988\n",
            "\tBatch 14 loss: 0.6848153471946716\n",
            "\tBatch 15 loss: 0.6468796730041504\n",
            "\tBatch 16 loss: 0.6585853099822998\n",
            "\tBatch 17 loss: 0.6844248175621033\n",
            "\tBatch 18 loss: 0.654670238494873\n",
            "\tBatch 19 loss: 0.649966299533844\n",
            "\tBatch 20 loss: 0.6922580003738403\n",
            "\tBatch 21 loss: 0.6562306880950928\n",
            "\tBatch 22 loss: 0.6610101461410522\n",
            "\tBatch 23 loss: 0.636227548122406\n",
            "\tBatch 24 loss: 0.7204004526138306\n",
            "\tBatch 25 loss: 0.7040602564811707\n",
            "\tBatch 26 loss: 0.6262978315353394\n",
            "\tBatch 27 loss: 0.6592481732368469\n",
            "\tBatch 28 loss: 0.6653274893760681\n",
            "\tBatch 29 loss: 0.6668294668197632\n",
            "\tBatch 30 loss: 0.661361575126648\n",
            "\tBatch 31 loss: 0.6429575681686401\n",
            "\tBatch 32 loss: 0.6693632006645203\n",
            "\tBatch 33 loss: 0.6634517908096313\n",
            "\tBatch 34 loss: 0.6623254418373108\n",
            "\tBatch 35 loss: 0.650553286075592\n",
            "\tBatch 36 loss: 0.6266876459121704\n",
            "\tBatch 37 loss: 0.6925079822540283\n",
            "\tBatch 38 loss: 0.7027437686920166\n",
            "\tBatch 39 loss: 0.595054030418396\n",
            "\tBatch 40 loss: 0.6323961615562439\n",
            "\tBatch 41 loss: 0.5844415426254272\n",
            "\tBatch 42 loss: 0.5254528522491455\n",
            "\tBatch 43 loss: 0.6987934708595276\n",
            "\tBatch 44 loss: 0.6613953709602356\n",
            "\tBatch 45 loss: 0.683380663394928\n",
            "\tBatch 46 loss: 0.6723392009735107\n",
            "\tBatch 47 loss: 0.5930591821670532\n",
            "\tBatch 48 loss: 0.5996935963630676\n",
            "\tBatch 49 loss: 0.6665700674057007\n",
            "\tBatch 50 loss: 0.636180579662323\n",
            "\tBatch 51 loss: 0.6179779767990112\n",
            "\tBatch 52 loss: 0.6338357329368591\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8439468943963448\n",
            "        AUPRC: 0.8161059021949768\n",
            "        Sensitivity: 0.4698789119720459\n",
            "        Specificity: 0.8796179294586182\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7878709477221522\n",
            "        AUPRC: 0.73651123046875\n",
            "        Sensitivity: 0.611232578754425\n",
            "        Specificity: 0.8158421516418457\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6217566728591919\n",
            "\tBatch 1 loss: 0.6431976556777954\n",
            "\tBatch 2 loss: 0.6481207013130188\n",
            "\tBatch 3 loss: 0.6560673117637634\n",
            "\tBatch 4 loss: 0.6453419327735901\n",
            "\tBatch 5 loss: 0.6060999035835266\n",
            "\tBatch 6 loss: 0.6191948652267456\n",
            "\tBatch 7 loss: 0.6469112634658813\n",
            "\tBatch 8 loss: 0.5846907496452332\n",
            "\tBatch 9 loss: 0.6444997787475586\n",
            "\tBatch 10 loss: 0.6433715224266052\n",
            "\tBatch 11 loss: 0.696162760257721\n",
            "\tBatch 12 loss: 0.6365247964859009\n",
            "\tBatch 13 loss: 0.6306830048561096\n",
            "\tBatch 14 loss: 0.6208415031433105\n",
            "\tBatch 15 loss: 0.6608564853668213\n",
            "\tBatch 16 loss: 0.6523990631103516\n",
            "\tBatch 17 loss: 0.6648167371749878\n",
            "\tBatch 18 loss: 0.674934983253479\n",
            "\tBatch 19 loss: 0.6322809457778931\n",
            "\tBatch 20 loss: 0.7180841565132141\n",
            "\tBatch 21 loss: 0.647148072719574\n",
            "\tBatch 22 loss: 0.6657337546348572\n",
            "\tBatch 23 loss: 0.6069623827934265\n",
            "\tBatch 24 loss: 0.764268159866333\n",
            "\tBatch 25 loss: 0.6977149844169617\n",
            "\tBatch 26 loss: 0.5925085544586182\n",
            "\tBatch 27 loss: 0.5869346261024475\n",
            "\tBatch 28 loss: 0.6535621881484985\n",
            "\tBatch 29 loss: 0.7451696395874023\n",
            "\tBatch 30 loss: 0.6653095483779907\n",
            "\tBatch 31 loss: 0.5875112414360046\n",
            "\tBatch 32 loss: 0.6591648459434509\n",
            "\tBatch 33 loss: 0.6248122453689575\n",
            "\tBatch 34 loss: 0.5937759876251221\n",
            "\tBatch 35 loss: 0.6321521997451782\n",
            "\tBatch 36 loss: 0.6038154363632202\n",
            "\tBatch 37 loss: 0.776695966720581\n",
            "\tBatch 38 loss: 0.767854630947113\n",
            "\tBatch 39 loss: 0.5534855127334595\n",
            "\tBatch 40 loss: 0.6008581519126892\n",
            "\tBatch 41 loss: 0.5597419142723083\n",
            "\tBatch 42 loss: 0.5568357110023499\n",
            "\tBatch 43 loss: 0.6465715765953064\n",
            "\tBatch 44 loss: 0.6268910765647888\n",
            "\tBatch 45 loss: 0.6368027925491333\n",
            "\tBatch 46 loss: 0.6190898418426514\n",
            "\tBatch 47 loss: 0.5782585144042969\n",
            "\tBatch 48 loss: 0.5944727063179016\n",
            "\tBatch 49 loss: 0.6425119042396545\n",
            "\tBatch 50 loss: 0.6106449365615845\n",
            "\tBatch 51 loss: 0.5804175734519958\n",
            "\tBatch 52 loss: 0.6122939586639404\n",
            "    Train data metrics:\n",
            "        AUROC: 0.857010609869611\n",
            "        AUPRC: 0.8360421657562256\n",
            "        Sensitivity: 0.3138628304004669\n",
            "        Specificity: 0.9384905099868774\n",
            "    Test data metrics:\n",
            "        AUROC: 0.789175757732272\n",
            "        AUPRC: 0.7433958053588867\n",
            "        Sensitivity: 0.4381897449493408\n",
            "        Specificity: 0.8810099959373474\n",
            "====================================\n",
            "     Epoch #9\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5717185735702515\n",
            "\tBatch 1 loss: 0.6253677606582642\n",
            "\tBatch 2 loss: 0.616388738155365\n",
            "\tBatch 3 loss: 0.6303175687789917\n",
            "\tBatch 4 loss: 0.6130779385566711\n",
            "\tBatch 5 loss: 0.588564395904541\n",
            "\tBatch 6 loss: 0.578559935092926\n",
            "\tBatch 7 loss: 0.6012781858444214\n",
            "\tBatch 8 loss: 0.5521320700645447\n",
            "\tBatch 9 loss: 0.6130609512329102\n",
            "\tBatch 10 loss: 0.6089396476745605\n",
            "\tBatch 11 loss: 0.690049946308136\n",
            "\tBatch 12 loss: 0.6065471768379211\n",
            "\tBatch 13 loss: 0.5986805558204651\n",
            "\tBatch 14 loss: 0.5783527493476868\n",
            "\tBatch 15 loss: 0.6319879293441772\n",
            "\tBatch 16 loss: 0.6229586601257324\n",
            "\tBatch 17 loss: 0.6488147974014282\n",
            "\tBatch 18 loss: 0.6416172385215759\n",
            "\tBatch 19 loss: 0.5949551463127136\n",
            "\tBatch 20 loss: 0.7423083186149597\n",
            "\tBatch 21 loss: 0.631158173084259\n",
            "\tBatch 22 loss: 0.654907763004303\n",
            "\tBatch 23 loss: 0.5901908874511719\n",
            "\tBatch 24 loss: 0.7213356494903564\n",
            "\tBatch 25 loss: 0.6368285417556763\n",
            "\tBatch 26 loss: 0.5878159403800964\n",
            "\tBatch 27 loss: 0.5302835702896118\n",
            "\tBatch 28 loss: 0.6590929627418518\n",
            "\tBatch 29 loss: 0.7742696404457092\n",
            "\tBatch 30 loss: 0.6348958015441895\n",
            "\tBatch 31 loss: 0.5403075218200684\n",
            "\tBatch 32 loss: 0.5771011710166931\n",
            "\tBatch 33 loss: 0.5738632678985596\n",
            "\tBatch 34 loss: 0.5427563190460205\n",
            "\tBatch 35 loss: 0.6407923698425293\n",
            "\tBatch 36 loss: 0.578185498714447\n",
            "\tBatch 37 loss: 0.7442474961280823\n",
            "\tBatch 38 loss: 0.6780627965927124\n",
            "\tBatch 39 loss: 0.5824206471443176\n",
            "\tBatch 40 loss: 0.5954875349998474\n",
            "\tBatch 41 loss: 0.5867769718170166\n",
            "\tBatch 42 loss: 0.6156956553459167\n",
            "\tBatch 43 loss: 0.6263418793678284\n",
            "\tBatch 44 loss: 0.6067554950714111\n",
            "\tBatch 45 loss: 0.6483187675476074\n",
            "\tBatch 46 loss: 0.6254410743713379\n",
            "\tBatch 47 loss: 0.5370693206787109\n",
            "\tBatch 48 loss: 0.5563077330589294\n",
            "\tBatch 49 loss: 0.6295482516288757\n",
            "\tBatch 50 loss: 0.5736472606658936\n",
            "\tBatch 51 loss: 0.545500636100769\n",
            "\tBatch 52 loss: 0.5464022755622864\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8286446810926956\n",
            "        AUPRC: 0.7973930835723877\n",
            "        Sensitivity: 0.8700031638145447\n",
            "        Specificity: 0.6542224287986755\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7829071662320027\n",
            "        AUPRC: 0.7311640977859497\n",
            "        Sensitivity: 0.841755211353302\n",
            "        Specificity: 0.5989201068878174\n",
            "====================================\n",
            "     Epoch #10\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5586695075035095\n",
            "\tBatch 1 loss: 0.5467044115066528\n",
            "\tBatch 2 loss: 0.6541720628738403\n",
            "\tBatch 3 loss: 0.6041581034660339\n",
            "\tBatch 4 loss: 0.5763700008392334\n",
            "\tBatch 5 loss: 0.5213273167610168\n",
            "\tBatch 6 loss: 0.5305771231651306\n",
            "\tBatch 7 loss: 0.5942221879959106\n",
            "\tBatch 8 loss: 0.4897252917289734\n",
            "\tBatch 9 loss: 0.593880295753479\n",
            "\tBatch 10 loss: 0.5708932280540466\n",
            "\tBatch 11 loss: 0.6595092415809631\n",
            "\tBatch 12 loss: 0.5630812048912048\n",
            "\tBatch 13 loss: 0.5861189365386963\n",
            "\tBatch 14 loss: 0.5156948566436768\n",
            "\tBatch 15 loss: 0.6411883234977722\n",
            "\tBatch 16 loss: 0.59995037317276\n",
            "\tBatch 17 loss: 0.6283513307571411\n",
            "\tBatch 18 loss: 0.5821697115898132\n",
            "\tBatch 19 loss: 0.5443709492683411\n",
            "\tBatch 20 loss: 0.8055649399757385\n",
            "\tBatch 21 loss: 0.6346893310546875\n",
            "\tBatch 22 loss: 0.643390417098999\n",
            "\tBatch 23 loss: 0.5731018781661987\n",
            "\tBatch 24 loss: 0.6546235084533691\n",
            "\tBatch 25 loss: 0.5780934691429138\n",
            "\tBatch 26 loss: 0.609158456325531\n",
            "\tBatch 27 loss: 0.4926716685295105\n",
            "\tBatch 28 loss: 0.663969874382019\n",
            "\tBatch 29 loss: 0.7690274715423584\n",
            "\tBatch 30 loss: 0.5950514078140259\n",
            "\tBatch 31 loss: 0.5135660767555237\n",
            "\tBatch 32 loss: 0.5296598672866821\n",
            "\tBatch 33 loss: 0.551120400428772\n",
            "\tBatch 34 loss: 0.5118327140808105\n",
            "\tBatch 35 loss: 0.6264094114303589\n",
            "\tBatch 36 loss: 0.5278254151344299\n",
            "\tBatch 37 loss: 0.6804513335227966\n",
            "\tBatch 38 loss: 0.5864162445068359\n",
            "\tBatch 39 loss: 0.6355048418045044\n",
            "\tBatch 40 loss: 0.6158101558685303\n",
            "\tBatch 41 loss: 0.5847311615943909\n",
            "\tBatch 42 loss: 0.5705044865608215\n",
            "\tBatch 43 loss: 0.6235332489013672\n",
            "\tBatch 44 loss: 0.607916533946991\n",
            "\tBatch 45 loss: 0.6841545701026917\n",
            "\tBatch 46 loss: 0.6437838077545166\n",
            "\tBatch 47 loss: 0.5215729475021362\n",
            "\tBatch 48 loss: 0.5444810390472412\n",
            "\tBatch 49 loss: 0.585352897644043\n",
            "\tBatch 50 loss: 0.52840656042099\n",
            "\tBatch 51 loss: 0.5319321155548096\n",
            "\tBatch 52 loss: 0.5008357763290405\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8230773727765696\n",
            "        AUPRC: 0.7913361191749573\n",
            "        Sensitivity: 0.9494815468788147\n",
            "        Specificity: 0.5369728207588196\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7818097808866439\n",
            "        AUPRC: 0.7299082279205322\n",
            "        Sensitivity: 0.9070625305175781\n",
            "        Specificity: 0.49454444646835327\n",
            "====================================\n",
            "     Epoch #11\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5556644797325134\n",
            "\tBatch 1 loss: 0.5069817900657654\n",
            "\tBatch 2 loss: 0.6343936324119568\n",
            "\tBatch 3 loss: 0.5863690376281738\n",
            "\tBatch 4 loss: 0.5640628337860107\n",
            "\tBatch 5 loss: 0.47128748893737793\n",
            "\tBatch 6 loss: 0.5029066801071167\n",
            "\tBatch 7 loss: 0.5750019550323486\n",
            "\tBatch 8 loss: 0.45535045862197876\n",
            "\tBatch 9 loss: 0.5617368221282959\n",
            "\tBatch 10 loss: 0.533302903175354\n",
            "\tBatch 11 loss: 0.624946117401123\n",
            "\tBatch 12 loss: 0.5378941893577576\n",
            "\tBatch 13 loss: 0.5675931572914124\n",
            "\tBatch 14 loss: 0.4879956543445587\n",
            "\tBatch 15 loss: 0.5929574370384216\n",
            "\tBatch 16 loss: 0.5641608238220215\n",
            "\tBatch 17 loss: 0.6097898483276367\n",
            "\tBatch 18 loss: 0.5535245537757874\n",
            "\tBatch 19 loss: 0.5247631072998047\n",
            "\tBatch 20 loss: 0.8167427182197571\n",
            "\tBatch 21 loss: 0.5993656516075134\n",
            "\tBatch 22 loss: 0.6236788630485535\n",
            "\tBatch 23 loss: 0.559015691280365\n",
            "\tBatch 24 loss: 0.6178783178329468\n",
            "\tBatch 25 loss: 0.5548609495162964\n",
            "\tBatch 26 loss: 0.5983222126960754\n",
            "\tBatch 27 loss: 0.4550430178642273\n",
            "\tBatch 28 loss: 0.6531882286071777\n",
            "\tBatch 29 loss: 0.753375232219696\n",
            "\tBatch 30 loss: 0.5774087905883789\n",
            "\tBatch 31 loss: 0.48729565739631653\n",
            "\tBatch 32 loss: 0.5070240497589111\n",
            "\tBatch 33 loss: 0.5365960597991943\n",
            "\tBatch 34 loss: 0.4879031777381897\n",
            "\tBatch 35 loss: 0.6095147132873535\n",
            "\tBatch 36 loss: 0.49186521768569946\n",
            "\tBatch 37 loss: 0.635737955570221\n",
            "\tBatch 38 loss: 0.541475772857666\n",
            "\tBatch 39 loss: 0.674280047416687\n",
            "\tBatch 40 loss: 0.6112414598464966\n",
            "\tBatch 41 loss: 0.5581719875335693\n",
            "\tBatch 42 loss: 0.5292276740074158\n",
            "\tBatch 43 loss: 0.6312660574913025\n",
            "\tBatch 44 loss: 0.6221514940261841\n",
            "\tBatch 45 loss: 0.7173788547515869\n",
            "\tBatch 46 loss: 0.642237663269043\n",
            "\tBatch 47 loss: 0.5047820806503296\n",
            "\tBatch 48 loss: 0.5420259237289429\n",
            "\tBatch 49 loss: 0.5381409525871277\n",
            "\tBatch 50 loss: 0.4969733953475952\n",
            "\tBatch 51 loss: 0.5295805931091309\n",
            "\tBatch 52 loss: 0.4584729075431824\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8295946814598244\n",
            "        AUPRC: 0.7988817691802979\n",
            "        Sensitivity: 0.9537522196769714\n",
            "        Specificity: 0.5451672077178955\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7814402244117407\n",
            "        AUPRC: 0.7324156165122986\n",
            "        Sensitivity: 0.9031809568405151\n",
            "        Specificity: 0.4956822097301483\n",
            "Training on data block 1!!!!\n",
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4606937766075134\n",
            "\tBatch 1 loss: 0.48762011528015137\n",
            "\tBatch 2 loss: 0.5194295644760132\n",
            "\tBatch 3 loss: 0.500896155834198\n",
            "\tBatch 4 loss: 0.7406968474388123\n",
            "\tBatch 5 loss: 0.5777851343154907\n",
            "\tBatch 6 loss: 0.4196826219558716\n",
            "\tBatch 7 loss: 0.49068471789360046\n",
            "\tBatch 8 loss: 0.44791027903556824\n",
            "\tBatch 9 loss: 0.6672903299331665\n",
            "\tBatch 10 loss: 0.5894421339035034\n",
            "\tBatch 11 loss: 0.673935055732727\n",
            "\tBatch 12 loss: 0.4361836612224579\n",
            "\tBatch 13 loss: 0.49362611770629883\n",
            "\tBatch 14 loss: 0.44982877373695374\n",
            "\tBatch 15 loss: 0.5396891236305237\n",
            "\tBatch 16 loss: 0.5445086359977722\n",
            "\tBatch 17 loss: 0.44701167941093445\n",
            "\tBatch 18 loss: 0.5320688486099243\n",
            "\tBatch 19 loss: 0.5943785309791565\n",
            "\tBatch 20 loss: 0.4724310636520386\n",
            "\tBatch 21 loss: 0.5406980514526367\n",
            "\tBatch 22 loss: 0.5505713820457458\n",
            "\tBatch 23 loss: 0.488319993019104\n",
            "\tBatch 24 loss: 0.3769371211528778\n",
            "\tBatch 25 loss: 0.4797566533088684\n",
            "\tBatch 26 loss: 0.5548876523971558\n",
            "\tBatch 27 loss: 0.594777524471283\n",
            "\tBatch 28 loss: 0.4311578571796417\n",
            "\tBatch 29 loss: 0.4702974855899811\n",
            "\tBatch 30 loss: 0.4855450987815857\n",
            "\tBatch 31 loss: 0.5418037176132202\n",
            "\tBatch 32 loss: 0.46596330404281616\n",
            "\tBatch 33 loss: 0.5222663283348083\n",
            "\tBatch 34 loss: 0.6852982640266418\n",
            "\tBatch 35 loss: 0.44361215829849243\n",
            "\tBatch 36 loss: 0.5111045241355896\n",
            "\tBatch 37 loss: 0.4071083068847656\n",
            "\tBatch 38 loss: 0.5659629106521606\n",
            "\tBatch 39 loss: 0.5946320295333862\n",
            "\tBatch 40 loss: 0.6678193211555481\n",
            "\tBatch 41 loss: 0.41785359382629395\n",
            "\tBatch 42 loss: 0.6328585147857666\n",
            "\tBatch 43 loss: 0.34427279233932495\n",
            "\tBatch 44 loss: 0.4609714150428772\n",
            "\tBatch 45 loss: 0.37346071004867554\n",
            "\tBatch 46 loss: 0.5390269756317139\n",
            "\tBatch 47 loss: 0.3704204261302948\n",
            "\tBatch 48 loss: 0.5212597846984863\n",
            "\tBatch 49 loss: 0.3640103042125702\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8725680162848285\n",
            "        AUPRC: 0.8706079721450806\n",
            "        Sensitivity: 0.5977674126625061\n",
            "        Specificity: 0.9179162383079529\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7827554038877753\n",
            "        AUPRC: 0.7375479340553284\n",
            "        Sensitivity: 0.46828335523605347\n",
            "        Specificity: 0.866024911403656\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5227168202400208\n",
            "\tBatch 1 loss: 0.4729730188846588\n",
            "\tBatch 2 loss: 0.48239558935165405\n",
            "\tBatch 3 loss: 0.5039246678352356\n",
            "\tBatch 4 loss: 0.669255793094635\n",
            "\tBatch 5 loss: 0.5649381279945374\n",
            "\tBatch 6 loss: 0.3943307399749756\n",
            "\tBatch 7 loss: 0.5121161341667175\n",
            "\tBatch 8 loss: 0.35519301891326904\n",
            "\tBatch 9 loss: 0.5417547821998596\n",
            "\tBatch 10 loss: 0.49788954854011536\n",
            "\tBatch 11 loss: 0.6958574652671814\n",
            "\tBatch 12 loss: 0.3856741189956665\n",
            "\tBatch 13 loss: 0.43794095516204834\n",
            "\tBatch 14 loss: 0.4780556559562683\n",
            "\tBatch 15 loss: 0.4134809672832489\n",
            "\tBatch 16 loss: 0.45151594281196594\n",
            "\tBatch 17 loss: 0.42952099442481995\n",
            "\tBatch 18 loss: 0.565040647983551\n",
            "\tBatch 19 loss: 0.6354573965072632\n",
            "\tBatch 20 loss: 0.44180846214294434\n",
            "\tBatch 21 loss: 0.5573745965957642\n",
            "\tBatch 22 loss: 0.5133004188537598\n",
            "\tBatch 23 loss: 0.47802916169166565\n",
            "\tBatch 24 loss: 0.32292377948760986\n",
            "\tBatch 25 loss: 0.48965296149253845\n",
            "\tBatch 26 loss: 0.5661797523498535\n",
            "\tBatch 27 loss: 0.5934698581695557\n",
            "\tBatch 28 loss: 0.37125325202941895\n",
            "\tBatch 29 loss: 0.45303118228912354\n",
            "\tBatch 30 loss: 0.46500325202941895\n",
            "\tBatch 31 loss: 0.5750680565834045\n",
            "\tBatch 32 loss: 0.4403160810470581\n",
            "\tBatch 33 loss: 0.5116991996765137\n",
            "\tBatch 34 loss: 0.7159493565559387\n",
            "\tBatch 35 loss: 0.4110034704208374\n",
            "\tBatch 36 loss: 0.5286430716514587\n",
            "\tBatch 37 loss: 0.36784231662750244\n",
            "\tBatch 38 loss: 0.6096753478050232\n",
            "\tBatch 39 loss: 0.6470972299575806\n",
            "\tBatch 40 loss: 0.6968787908554077\n",
            "\tBatch 41 loss: 0.3951604664325714\n",
            "\tBatch 42 loss: 0.6775603294372559\n",
            "\tBatch 43 loss: 0.3179739713668823\n",
            "\tBatch 44 loss: 0.4726864993572235\n",
            "\tBatch 45 loss: 0.33404892683029175\n",
            "\tBatch 46 loss: 0.5436127781867981\n",
            "\tBatch 47 loss: 0.33729255199432373\n",
            "\tBatch 48 loss: 0.5192558765411377\n",
            "\tBatch 49 loss: 0.3371308147907257\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8737391912455392\n",
            "        AUPRC: 0.8723308444023132\n",
            "        Sensitivity: 0.6595140695571899\n",
            "        Specificity: 0.8795790672302246\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7749587199592651\n",
            "        AUPRC: 0.7241314649581909\n",
            "        Sensitivity: 0.555846631526947\n",
            "        Specificity: 0.8204959630966187\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.48009827733039856\n",
            "\tBatch 1 loss: 0.44071704149246216\n",
            "\tBatch 2 loss: 0.4750744700431824\n",
            "\tBatch 3 loss: 0.4669000208377838\n",
            "\tBatch 4 loss: 0.7033369541168213\n",
            "\tBatch 5 loss: 0.5586809515953064\n",
            "\tBatch 6 loss: 0.3628291189670563\n",
            "\tBatch 7 loss: 0.49250373244285583\n",
            "\tBatch 8 loss: 0.3346795439720154\n",
            "\tBatch 9 loss: 0.5576744675636292\n",
            "\tBatch 10 loss: 0.4971463084220886\n",
            "\tBatch 11 loss: 0.696234941482544\n",
            "\tBatch 12 loss: 0.3607458472251892\n",
            "\tBatch 13 loss: 0.42322102189064026\n",
            "\tBatch 14 loss: 0.4629540741443634\n",
            "\tBatch 15 loss: 0.41798824071884155\n",
            "\tBatch 16 loss: 0.44591623544692993\n",
            "\tBatch 17 loss: 0.41093313694000244\n",
            "\tBatch 18 loss: 0.5353566408157349\n",
            "\tBatch 19 loss: 0.6335381269454956\n",
            "\tBatch 20 loss: 0.41948452591896057\n",
            "\tBatch 21 loss: 0.5495962500572205\n",
            "\tBatch 22 loss: 0.5224714279174805\n",
            "\tBatch 23 loss: 0.46536874771118164\n",
            "\tBatch 24 loss: 0.3109776973724365\n",
            "\tBatch 25 loss: 0.4802502691745758\n",
            "\tBatch 26 loss: 0.5629343390464783\n",
            "\tBatch 27 loss: 0.591681182384491\n",
            "\tBatch 28 loss: 0.3619421124458313\n",
            "\tBatch 29 loss: 0.4426969885826111\n",
            "\tBatch 30 loss: 0.4574142396450043\n",
            "\tBatch 31 loss: 0.5602270364761353\n",
            "\tBatch 32 loss: 0.4434346556663513\n",
            "\tBatch 33 loss: 0.5015872716903687\n",
            "\tBatch 34 loss: 0.7112722396850586\n",
            "\tBatch 35 loss: 0.3962278962135315\n",
            "\tBatch 36 loss: 0.52524334192276\n",
            "\tBatch 37 loss: 0.358865886926651\n",
            "\tBatch 38 loss: 0.5857635140419006\n",
            "\tBatch 39 loss: 0.6214855313301086\n",
            "\tBatch 40 loss: 0.6927127838134766\n",
            "\tBatch 41 loss: 0.3819807171821594\n",
            "\tBatch 42 loss: 0.6727914810180664\n",
            "\tBatch 43 loss: 0.3018765151500702\n",
            "\tBatch 44 loss: 0.44682058691978455\n",
            "\tBatch 45 loss: 0.31861403584480286\n",
            "\tBatch 46 loss: 0.5346430540084839\n",
            "\tBatch 47 loss: 0.32340389490127563\n",
            "\tBatch 48 loss: 0.5276262164115906\n",
            "\tBatch 49 loss: 0.320377379655838\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8762748408749451\n",
            "        AUPRC: 0.8740194439888\n",
            "        Sensitivity: 0.6591576337814331\n",
            "        Specificity: 0.8745834827423096\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7708834363961985\n",
            "        AUPRC: 0.7208592891693115\n",
            "        Sensitivity: 0.5492050647735596\n",
            "        Specificity: 0.8176840543746948\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4630747437477112\n",
            "\tBatch 1 loss: 0.42793163657188416\n",
            "\tBatch 2 loss: 0.4637201726436615\n",
            "\tBatch 3 loss: 0.47433480620384216\n",
            "\tBatch 4 loss: 0.6988345384597778\n",
            "\tBatch 5 loss: 0.5331807732582092\n",
            "\tBatch 6 loss: 0.35009562969207764\n",
            "\tBatch 7 loss: 0.4776521921157837\n",
            "\tBatch 8 loss: 0.32284510135650635\n",
            "\tBatch 9 loss: 0.5559878945350647\n",
            "\tBatch 10 loss: 0.4868830144405365\n",
            "\tBatch 11 loss: 0.6870051622390747\n",
            "\tBatch 12 loss: 0.34681397676467896\n",
            "\tBatch 13 loss: 0.40831857919692993\n",
            "\tBatch 14 loss: 0.4502233862876892\n",
            "\tBatch 15 loss: 0.41917961835861206\n",
            "\tBatch 16 loss: 0.4446397125720978\n",
            "\tBatch 17 loss: 0.39070549607276917\n",
            "\tBatch 18 loss: 0.5314465165138245\n",
            "\tBatch 19 loss: 0.6301389336585999\n",
            "\tBatch 20 loss: 0.42009207606315613\n",
            "\tBatch 21 loss: 0.5450969338417053\n",
            "\tBatch 22 loss: 0.517095148563385\n",
            "\tBatch 23 loss: 0.4684417247772217\n",
            "\tBatch 24 loss: 0.30196940898895264\n",
            "\tBatch 25 loss: 0.4736926853656769\n",
            "\tBatch 26 loss: 0.5642802715301514\n",
            "\tBatch 27 loss: 0.5979520678520203\n",
            "\tBatch 28 loss: 0.36382898688316345\n",
            "\tBatch 29 loss: 0.4402799904346466\n",
            "\tBatch 30 loss: 0.44920048117637634\n",
            "\tBatch 31 loss: 0.55488520860672\n",
            "\tBatch 32 loss: 0.4507923722267151\n",
            "\tBatch 33 loss: 0.4930385649204254\n",
            "\tBatch 34 loss: 0.7174261212348938\n",
            "\tBatch 35 loss: 0.3882395625114441\n",
            "\tBatch 36 loss: 0.5342956781387329\n",
            "\tBatch 37 loss: 0.3518265187740326\n",
            "\tBatch 38 loss: 0.5671390891075134\n",
            "\tBatch 39 loss: 0.6018818020820618\n",
            "\tBatch 40 loss: 0.684719443321228\n",
            "\tBatch 41 loss: 0.38583147525787354\n",
            "\tBatch 42 loss: 0.6937004923820496\n",
            "\tBatch 43 loss: 0.28552794456481934\n",
            "\tBatch 44 loss: 0.4327142536640167\n",
            "\tBatch 45 loss: 0.31287699937820435\n",
            "\tBatch 46 loss: 0.5357382297515869\n",
            "\tBatch 47 loss: 0.3159712851047516\n",
            "\tBatch 48 loss: 0.5268380045890808\n",
            "\tBatch 49 loss: 0.3087365925312042\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8785691866565465\n",
            "        AUPRC: 0.8767849206924438\n",
            "        Sensitivity: 0.674996554851532\n",
            "        Specificity: 0.8770347833633423\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7674040537533104\n",
            "        AUPRC: 0.7142716646194458\n",
            "        Sensitivity: 0.5375308394432068\n",
            "        Specificity: 0.8110733032226562\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4583370089530945\n",
            "\tBatch 1 loss: 0.4301314353942871\n",
            "\tBatch 2 loss: 0.45139074325561523\n",
            "\tBatch 3 loss: 0.46742063760757446\n",
            "\tBatch 4 loss: 0.6995117664337158\n",
            "\tBatch 5 loss: 0.5298129916191101\n",
            "\tBatch 6 loss: 0.3445702791213989\n",
            "\tBatch 7 loss: 0.4774323105812073\n",
            "\tBatch 8 loss: 0.3194395899772644\n",
            "\tBatch 9 loss: 0.5556529760360718\n",
            "\tBatch 10 loss: 0.4704766571521759\n",
            "\tBatch 11 loss: 0.6833775043487549\n",
            "\tBatch 12 loss: 0.3367280960083008\n",
            "\tBatch 13 loss: 0.4157717525959015\n",
            "\tBatch 14 loss: 0.4394526779651642\n",
            "\tBatch 15 loss: 0.41261863708496094\n",
            "\tBatch 16 loss: 0.4275822341442108\n",
            "\tBatch 17 loss: 0.38396644592285156\n",
            "\tBatch 18 loss: 0.526050865650177\n",
            "\tBatch 19 loss: 0.6348328590393066\n",
            "\tBatch 20 loss: 0.4149796664714813\n",
            "\tBatch 21 loss: 0.5539820790290833\n",
            "\tBatch 22 loss: 0.5176488757133484\n",
            "\tBatch 23 loss: 0.4571637213230133\n",
            "\tBatch 24 loss: 0.29234445095062256\n",
            "\tBatch 25 loss: 0.47197818756103516\n",
            "\tBatch 26 loss: 0.5699557662010193\n",
            "\tBatch 27 loss: 0.5963972806930542\n",
            "\tBatch 28 loss: 0.3600636124610901\n",
            "\tBatch 29 loss: 0.42957717180252075\n",
            "\tBatch 30 loss: 0.43668296933174133\n",
            "\tBatch 31 loss: 0.5568720102310181\n",
            "\tBatch 32 loss: 0.44468769431114197\n",
            "\tBatch 33 loss: 0.49602872133255005\n",
            "\tBatch 34 loss: 0.714893102645874\n",
            "\tBatch 35 loss: 0.3782529830932617\n",
            "\tBatch 36 loss: 0.5319850444793701\n",
            "\tBatch 37 loss: 0.34710854291915894\n",
            "\tBatch 38 loss: 0.5541133284568787\n",
            "\tBatch 39 loss: 0.6030071377754211\n",
            "\tBatch 40 loss: 0.7003504633903503\n",
            "\tBatch 41 loss: 0.3807735741138458\n",
            "\tBatch 42 loss: 0.6704646944999695\n",
            "\tBatch 43 loss: 0.28256264328956604\n",
            "\tBatch 44 loss: 0.42709794640541077\n",
            "\tBatch 45 loss: 0.30496785044670105\n",
            "\tBatch 46 loss: 0.5391772985458374\n",
            "\tBatch 47 loss: 0.3110487759113312\n",
            "\tBatch 48 loss: 0.5358285903930664\n",
            "\tBatch 49 loss: 0.2896732985973358\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8821075266460936\n",
            "        AUPRC: 0.8804792761802673\n",
            "        Sensitivity: 0.6863834857940674\n",
            "        Specificity: 0.8762609958648682\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7636088443603678\n",
            "        AUPRC: 0.7093013525009155\n",
            "        Sensitivity: 0.5213524103164673\n",
            "        Specificity: 0.8160401582717896\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4523637592792511\n",
            "\tBatch 1 loss: 0.4333527386188507\n",
            "\tBatch 2 loss: 0.44943997263908386\n",
            "\tBatch 3 loss: 0.46819162368774414\n",
            "\tBatch 4 loss: 0.7086274027824402\n",
            "\tBatch 5 loss: 0.5133059024810791\n",
            "\tBatch 6 loss: 0.3367209732532501\n",
            "\tBatch 7 loss: 0.4767443835735321\n",
            "\tBatch 8 loss: 0.31507179141044617\n",
            "\tBatch 9 loss: 0.5467411875724792\n",
            "\tBatch 10 loss: 0.4684920310974121\n",
            "\tBatch 11 loss: 0.6764745712280273\n",
            "\tBatch 12 loss: 0.3293994665145874\n",
            "\tBatch 13 loss: 0.39741066098213196\n",
            "\tBatch 14 loss: 0.44367459416389465\n",
            "\tBatch 15 loss: 0.40924984216690063\n",
            "\tBatch 16 loss: 0.42239055037498474\n",
            "\tBatch 17 loss: 0.380409836769104\n",
            "\tBatch 18 loss: 0.516681432723999\n",
            "\tBatch 19 loss: 0.622033417224884\n",
            "\tBatch 20 loss: 0.419535756111145\n",
            "\tBatch 21 loss: 0.5465368032455444\n",
            "\tBatch 22 loss: 0.5224794149398804\n",
            "\tBatch 23 loss: 0.4572603404521942\n",
            "\tBatch 24 loss: 0.2822811007499695\n",
            "\tBatch 25 loss: 0.4754834473133087\n",
            "\tBatch 26 loss: 0.5712281465530396\n",
            "\tBatch 27 loss: 0.5931095480918884\n",
            "\tBatch 28 loss: 0.3524463176727295\n",
            "\tBatch 29 loss: 0.4272947907447815\n",
            "\tBatch 30 loss: 0.43651023507118225\n",
            "\tBatch 31 loss: 0.5503770112991333\n",
            "\tBatch 32 loss: 0.4442141056060791\n",
            "\tBatch 33 loss: 0.4784451723098755\n",
            "\tBatch 34 loss: 0.7074257135391235\n",
            "\tBatch 35 loss: 0.3773370683193207\n",
            "\tBatch 36 loss: 0.5335355997085571\n",
            "\tBatch 37 loss: 0.3479144871234894\n",
            "\tBatch 38 loss: 0.5418336391448975\n",
            "\tBatch 39 loss: 0.6001715660095215\n",
            "\tBatch 40 loss: 0.6784473657608032\n",
            "\tBatch 41 loss: 0.37453576922416687\n",
            "\tBatch 42 loss: 0.6917873620986938\n",
            "\tBatch 43 loss: 0.27818676829338074\n",
            "\tBatch 44 loss: 0.41300997138023376\n",
            "\tBatch 45 loss: 0.30386272072792053\n",
            "\tBatch 46 loss: 0.5306645631790161\n",
            "\tBatch 47 loss: 0.29765456914901733\n",
            "\tBatch 48 loss: 0.5364585518836975\n",
            "\tBatch 49 loss: 0.2802140712738037\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8837999832923111\n",
            "        AUPRC: 0.8812523484230042\n",
            "        Sensitivity: 0.7004462480545044\n",
            "        Specificity: 0.8750495910644531\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7592212495632331\n",
            "        AUPRC: 0.7070997953414917\n",
            "        Sensitivity: 0.5310344696044922\n",
            "        Specificity: 0.8115453720092773\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4464353322982788\n",
            "\tBatch 1 loss: 0.4166578948497772\n",
            "\tBatch 2 loss: 0.4531918466091156\n",
            "\tBatch 3 loss: 0.4572761058807373\n",
            "\tBatch 4 loss: 0.6869959831237793\n",
            "\tBatch 5 loss: 0.5090962648391724\n",
            "\tBatch 6 loss: 0.3344566226005554\n",
            "\tBatch 7 loss: 0.4741133153438568\n",
            "\tBatch 8 loss: 0.3063738942146301\n",
            "\tBatch 9 loss: 0.5418938994407654\n",
            "\tBatch 10 loss: 0.4552801847457886\n",
            "\tBatch 11 loss: 0.6571412682533264\n",
            "\tBatch 12 loss: 0.3251243531703949\n",
            "\tBatch 13 loss: 0.39637458324432373\n",
            "\tBatch 14 loss: 0.42726629972457886\n",
            "\tBatch 15 loss: 0.40936338901519775\n",
            "\tBatch 16 loss: 0.4155026972293854\n",
            "\tBatch 17 loss: 0.371503621339798\n",
            "\tBatch 18 loss: 0.5179721117019653\n",
            "\tBatch 19 loss: 0.6207920908927917\n",
            "\tBatch 20 loss: 0.4140346050262451\n",
            "\tBatch 21 loss: 0.5438568592071533\n",
            "\tBatch 22 loss: 0.5186042785644531\n",
            "\tBatch 23 loss: 0.4742126762866974\n",
            "\tBatch 24 loss: 0.2837739586830139\n",
            "\tBatch 25 loss: 0.47843772172927856\n",
            "\tBatch 26 loss: 0.5655378103256226\n",
            "\tBatch 27 loss: 0.5909302234649658\n",
            "\tBatch 28 loss: 0.35464224219322205\n",
            "\tBatch 29 loss: 0.4282176196575165\n",
            "\tBatch 30 loss: 0.4367784857749939\n",
            "\tBatch 31 loss: 0.5531038641929626\n",
            "\tBatch 32 loss: 0.44174298644065857\n",
            "\tBatch 33 loss: 0.467596173286438\n",
            "\tBatch 34 loss: 0.7067295908927917\n",
            "\tBatch 35 loss: 0.3712468147277832\n",
            "\tBatch 36 loss: 0.5345507860183716\n",
            "\tBatch 37 loss: 0.34245502948760986\n",
            "\tBatch 38 loss: 0.5521122217178345\n",
            "\tBatch 39 loss: 0.5908380150794983\n",
            "\tBatch 40 loss: 0.6852043271064758\n",
            "\tBatch 41 loss: 0.3735387921333313\n",
            "\tBatch 42 loss: 0.6764934062957764\n",
            "\tBatch 43 loss: 0.27595046162605286\n",
            "\tBatch 44 loss: 0.40649208426475525\n",
            "\tBatch 45 loss: 0.3015941381454468\n",
            "\tBatch 46 loss: 0.5365078449249268\n",
            "\tBatch 47 loss: 0.3007097542285919\n",
            "\tBatch 48 loss: 0.534206748008728\n",
            "\tBatch 49 loss: 0.28006795048713684\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8859755088447129\n",
            "        AUPRC: 0.8835234045982361\n",
            "        Sensitivity: 0.7003862857818604\n",
            "        Specificity: 0.8764105439186096\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7555025864192908\n",
            "        AUPRC: 0.7024128437042236\n",
            "        Sensitivity: 0.5183413624763489\n",
            "        Specificity: 0.8119075298309326\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4451856315135956\n",
            "\tBatch 1 loss: 0.4126822352409363\n",
            "\tBatch 2 loss: 0.4492543339729309\n",
            "\tBatch 3 loss: 0.4568461775779724\n",
            "\tBatch 4 loss: 0.6810956001281738\n",
            "\tBatch 5 loss: 0.49777087569236755\n",
            "\tBatch 6 loss: 0.3282364308834076\n",
            "\tBatch 7 loss: 0.46939006447792053\n",
            "\tBatch 8 loss: 0.30523213744163513\n",
            "\tBatch 9 loss: 0.5420014262199402\n",
            "\tBatch 10 loss: 0.4656403958797455\n",
            "\tBatch 11 loss: 0.6533121466636658\n",
            "\tBatch 12 loss: 0.32177621126174927\n",
            "\tBatch 13 loss: 0.3973674476146698\n",
            "\tBatch 14 loss: 0.42110973596572876\n",
            "\tBatch 15 loss: 0.40040427446365356\n",
            "\tBatch 16 loss: 0.41605344414711\n",
            "\tBatch 17 loss: 0.36984771490097046\n",
            "\tBatch 18 loss: 0.4964817762374878\n",
            "\tBatch 19 loss: 0.6174079179763794\n",
            "\tBatch 20 loss: 0.4065830111503601\n",
            "\tBatch 21 loss: 0.5448180437088013\n",
            "\tBatch 22 loss: 0.5182603597640991\n",
            "\tBatch 23 loss: 0.4689761698246002\n",
            "\tBatch 24 loss: 0.271779328584671\n",
            "\tBatch 25 loss: 0.4715869426727295\n",
            "\tBatch 26 loss: 0.5715324878692627\n",
            "\tBatch 27 loss: 0.5924760699272156\n",
            "\tBatch 28 loss: 0.35586655139923096\n",
            "\tBatch 29 loss: 0.42463016510009766\n",
            "\tBatch 30 loss: 0.4355784058570862\n",
            "\tBatch 31 loss: 0.5422351956367493\n",
            "\tBatch 32 loss: 0.4541172981262207\n",
            "\tBatch 33 loss: 0.4719100594520569\n",
            "\tBatch 34 loss: 0.6908196210861206\n",
            "\tBatch 35 loss: 0.3622952699661255\n",
            "\tBatch 36 loss: 0.5270735025405884\n",
            "\tBatch 37 loss: 0.3429880440235138\n",
            "\tBatch 38 loss: 0.5388913154602051\n",
            "\tBatch 39 loss: 0.5877752900123596\n",
            "\tBatch 40 loss: 0.6827492713928223\n",
            "\tBatch 41 loss: 0.3788257837295532\n",
            "\tBatch 42 loss: 0.6817574501037598\n",
            "\tBatch 43 loss: 0.2686821520328522\n",
            "\tBatch 44 loss: 0.4011380970478058\n",
            "\tBatch 45 loss: 0.2984285056591034\n",
            "\tBatch 46 loss: 0.5317748785018921\n",
            "\tBatch 47 loss: 0.29946205019950867\n",
            "\tBatch 48 loss: 0.5314971208572388\n",
            "\tBatch 49 loss: 0.2722488343715668\n",
            "    Train data metrics:\n",
            "        AUROC: 0.888321314287221\n",
            "        AUPRC: 0.8847548961639404\n",
            "        Sensitivity: 0.7187466621398926\n",
            "        Specificity: 0.8753600120544434\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7498575722967054\n",
            "        AUPRC: 0.6944736242294312\n",
            "        Sensitivity: 0.5055191516876221\n",
            "        Specificity: 0.8143922090530396\n",
            "====================================\n",
            "     Epoch #9\n",
            "====================================\n",
            "\tBatch 0 loss: 0.43672534823417664\n",
            "\tBatch 1 loss: 0.41299009323120117\n",
            "\tBatch 2 loss: 0.44146180152893066\n",
            "\tBatch 3 loss: 0.48375841975212097\n",
            "\tBatch 4 loss: 0.6729059815406799\n",
            "\tBatch 5 loss: 0.4926576614379883\n",
            "\tBatch 6 loss: 0.3334521949291229\n",
            "\tBatch 7 loss: 0.47075071930885315\n",
            "\tBatch 8 loss: 0.2992554306983948\n",
            "\tBatch 9 loss: 0.5365750193595886\n",
            "\tBatch 10 loss: 0.45161211490631104\n",
            "\tBatch 11 loss: 0.6443812847137451\n",
            "\tBatch 12 loss: 0.32048314809799194\n",
            "\tBatch 13 loss: 0.3911881744861603\n",
            "\tBatch 14 loss: 0.42316508293151855\n",
            "\tBatch 15 loss: 0.40034908056259155\n",
            "\tBatch 16 loss: 0.40214911103248596\n",
            "\tBatch 17 loss: 0.3723931908607483\n",
            "\tBatch 18 loss: 0.502221941947937\n",
            "\tBatch 19 loss: 0.6166821718215942\n",
            "\tBatch 20 loss: 0.40987274050712585\n",
            "\tBatch 21 loss: 0.5504313111305237\n",
            "\tBatch 22 loss: 0.5071646571159363\n",
            "\tBatch 23 loss: 0.47117769718170166\n",
            "\tBatch 24 loss: 0.26964181661605835\n",
            "\tBatch 25 loss: 0.461725115776062\n",
            "\tBatch 26 loss: 0.57792067527771\n",
            "\tBatch 27 loss: 0.5946633815765381\n",
            "\tBatch 28 loss: 0.35425251722335815\n",
            "\tBatch 29 loss: 0.4159991145133972\n",
            "\tBatch 30 loss: 0.4289941191673279\n",
            "\tBatch 31 loss: 0.5389615297317505\n",
            "\tBatch 32 loss: 0.4502889811992645\n",
            "\tBatch 33 loss: 0.46274879574775696\n",
            "\tBatch 34 loss: 0.6817740797996521\n",
            "\tBatch 35 loss: 0.35377129912376404\n",
            "\tBatch 36 loss: 0.5296040177345276\n",
            "\tBatch 37 loss: 0.33030033111572266\n",
            "\tBatch 38 loss: 0.5377088785171509\n",
            "\tBatch 39 loss: 0.5712923407554626\n",
            "\tBatch 40 loss: 0.6759888529777527\n",
            "\tBatch 41 loss: 0.3772609829902649\n",
            "\tBatch 42 loss: 0.6811410188674927\n",
            "\tBatch 43 loss: 0.264356791973114\n",
            "\tBatch 44 loss: 0.38956981897354126\n",
            "\tBatch 45 loss: 0.2963753640651703\n",
            "\tBatch 46 loss: 0.5277910828590393\n",
            "\tBatch 47 loss: 0.29828959703445435\n",
            "\tBatch 48 loss: 0.533425509929657\n",
            "\tBatch 49 loss: 0.2634815275669098\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8890397008077838\n",
            "        AUPRC: 0.8850361704826355\n",
            "        Sensitivity: 0.719219982624054\n",
            "        Specificity: 0.868664562702179\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7459426516163917\n",
            "        AUPRC: 0.6918853521347046\n",
            "        Sensitivity: 0.5134655237197876\n",
            "        Specificity: 0.8155556917190552\n",
            "====================================\n",
            "     Epoch #10\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4279021620750427\n",
            "\tBatch 1 loss: 0.3950527310371399\n",
            "\tBatch 2 loss: 0.44719642400741577\n",
            "\tBatch 3 loss: 0.47059333324432373\n",
            "\tBatch 4 loss: 0.6777421832084656\n",
            "\tBatch 5 loss: 0.4781269431114197\n",
            "\tBatch 6 loss: 0.3236237168312073\n",
            "\tBatch 7 loss: 0.46093857288360596\n",
            "\tBatch 8 loss: 0.29874518513679504\n",
            "\tBatch 9 loss: 0.538550078868866\n",
            "\tBatch 10 loss: 0.44112706184387207\n",
            "\tBatch 11 loss: 0.6359906792640686\n",
            "\tBatch 12 loss: 0.31831011176109314\n",
            "\tBatch 13 loss: 0.3867875933647156\n",
            "\tBatch 14 loss: 0.41230320930480957\n",
            "\tBatch 15 loss: 0.40019094944000244\n",
            "\tBatch 16 loss: 0.4035572409629822\n",
            "\tBatch 17 loss: 0.36890894174575806\n",
            "\tBatch 18 loss: 0.4851420819759369\n",
            "\tBatch 19 loss: 0.6050864458084106\n",
            "\tBatch 20 loss: 0.42150092124938965\n",
            "\tBatch 21 loss: 0.5421776175498962\n",
            "\tBatch 22 loss: 0.5138018131256104\n",
            "\tBatch 23 loss: 0.46934133768081665\n",
            "\tBatch 24 loss: 0.2653152644634247\n",
            "\tBatch 25 loss: 0.45961660146713257\n",
            "\tBatch 26 loss: 0.5650631189346313\n",
            "\tBatch 27 loss: 0.5870946645736694\n",
            "\tBatch 28 loss: 0.358696311712265\n",
            "\tBatch 29 loss: 0.4201393723487854\n",
            "\tBatch 30 loss: 0.4242439270019531\n",
            "\tBatch 31 loss: 0.5398987531661987\n",
            "\tBatch 32 loss: 0.4489736557006836\n",
            "\tBatch 33 loss: 0.4533240795135498\n",
            "\tBatch 34 loss: 0.6805652976036072\n",
            "\tBatch 35 loss: 0.34908825159072876\n",
            "\tBatch 36 loss: 0.5362448692321777\n",
            "\tBatch 37 loss: 0.3339634835720062\n",
            "\tBatch 38 loss: 0.5290253162384033\n",
            "\tBatch 39 loss: 0.56610107421875\n",
            "\tBatch 40 loss: 0.669816255569458\n",
            "\tBatch 41 loss: 0.3735864460468292\n",
            "\tBatch 42 loss: 0.6981967687606812\n",
            "\tBatch 43 loss: 0.2626371383666992\n",
            "\tBatch 44 loss: 0.38990968465805054\n",
            "\tBatch 45 loss: 0.29195088148117065\n",
            "\tBatch 46 loss: 0.52653568983078\n",
            "\tBatch 47 loss: 0.29381707310676575\n",
            "\tBatch 48 loss: 0.5302605032920837\n",
            "\tBatch 49 loss: 0.26063868403434753\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8910985844757705\n",
            "        AUPRC: 0.8871780633926392\n",
            "        Sensitivity: 0.7260212898254395\n",
            "        Specificity: 0.8700825572013855\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7435590983068543\n",
            "        AUPRC: 0.6910659074783325\n",
            "        Sensitivity: 0.5056201219558716\n",
            "        Specificity: 0.8157346248626709\n",
            "====================================\n",
            "     Epoch #11\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4299966096878052\n",
            "\tBatch 1 loss: 0.39527833461761475\n",
            "\tBatch 2 loss: 0.42945432662963867\n",
            "\tBatch 3 loss: 0.45803311467170715\n",
            "\tBatch 4 loss: 0.6858395934104919\n",
            "\tBatch 5 loss: 0.46540576219558716\n",
            "\tBatch 6 loss: 0.32143115997314453\n",
            "\tBatch 7 loss: 0.46146976947784424\n",
            "\tBatch 8 loss: 0.2989216148853302\n",
            "\tBatch 9 loss: 0.5287690162658691\n",
            "\tBatch 10 loss: 0.4342818260192871\n",
            "\tBatch 11 loss: 0.6315435767173767\n",
            "\tBatch 12 loss: 0.30795276165008545\n",
            "\tBatch 13 loss: 0.378643661737442\n",
            "\tBatch 14 loss: 0.3952029347419739\n",
            "\tBatch 15 loss: 0.40356945991516113\n",
            "\tBatch 16 loss: 0.3944905996322632\n",
            "\tBatch 17 loss: 0.36365604400634766\n",
            "\tBatch 18 loss: 0.49257051944732666\n",
            "\tBatch 19 loss: 0.6029341220855713\n",
            "\tBatch 20 loss: 0.40856578946113586\n",
            "\tBatch 21 loss: 0.5436018705368042\n",
            "\tBatch 22 loss: 0.5095431804656982\n",
            "\tBatch 23 loss: 0.4756753742694855\n",
            "\tBatch 24 loss: 0.2597297132015228\n",
            "\tBatch 25 loss: 0.4685131013393402\n",
            "\tBatch 26 loss: 0.5802518129348755\n",
            "\tBatch 27 loss: 0.584711492061615\n",
            "\tBatch 28 loss: 0.3639591336250305\n",
            "\tBatch 29 loss: 0.41263648867607117\n",
            "\tBatch 30 loss: 0.42347386479377747\n",
            "\tBatch 31 loss: 0.5368356704711914\n",
            "\tBatch 32 loss: 0.4452405571937561\n",
            "\tBatch 33 loss: 0.4614523649215698\n",
            "\tBatch 34 loss: 0.6921716928482056\n",
            "\tBatch 35 loss: 0.34947752952575684\n",
            "\tBatch 36 loss: 0.5432559251785278\n",
            "\tBatch 37 loss: 0.3279138207435608\n",
            "\tBatch 38 loss: 0.5380879044532776\n",
            "\tBatch 39 loss: 0.5740880370140076\n",
            "\tBatch 40 loss: 0.6820359230041504\n",
            "\tBatch 41 loss: 0.3796032667160034\n",
            "\tBatch 42 loss: 0.68687903881073\n",
            "\tBatch 43 loss: 0.26857611536979675\n",
            "\tBatch 44 loss: 0.3862977623939514\n",
            "\tBatch 45 loss: 0.2885701358318329\n",
            "\tBatch 46 loss: 0.5252017974853516\n",
            "\tBatch 47 loss: 0.29179900884628296\n",
            "\tBatch 48 loss: 0.5274319648742676\n",
            "\tBatch 49 loss: 0.2602737247943878\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8920137271006388\n",
            "        AUPRC: 0.8883015513420105\n",
            "        Sensitivity: 0.7378314137458801\n",
            "        Specificity: 0.8587216138839722\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7403150232048605\n",
            "        AUPRC: 0.6861253380775452\n",
            "        Sensitivity: 0.506407618522644\n",
            "        Specificity: 0.8078140020370483\n",
            "Training on data block 2!!!!\n",
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 0.7544858455657959\n",
            "\tBatch 1 loss: 0.5028032064437866\n",
            "\tBatch 2 loss: 0.4497133195400238\n",
            "\tBatch 3 loss: 0.5749361515045166\n",
            "\tBatch 4 loss: 0.6749623417854309\n",
            "\tBatch 5 loss: 0.48787009716033936\n",
            "\tBatch 6 loss: 0.583167552947998\n",
            "\tBatch 7 loss: 0.4241446852684021\n",
            "\tBatch 8 loss: 0.5980247855186462\n",
            "\tBatch 9 loss: 0.506138026714325\n",
            "\tBatch 10 loss: 0.4369238615036011\n",
            "\tBatch 11 loss: 0.5550209283828735\n",
            "\tBatch 12 loss: 0.5676222443580627\n",
            "\tBatch 13 loss: 0.6717582941055298\n",
            "\tBatch 14 loss: 0.39794713258743286\n",
            "\tBatch 15 loss: 0.6293500661849976\n",
            "\tBatch 16 loss: 0.7954663038253784\n",
            "\tBatch 17 loss: 0.6242567300796509\n",
            "\tBatch 18 loss: 0.5052082538604736\n",
            "\tBatch 19 loss: 0.5665520429611206\n",
            "\tBatch 20 loss: 0.5640947818756104\n",
            "\tBatch 21 loss: 0.7717939019203186\n",
            "\tBatch 22 loss: 0.5029221773147583\n",
            "\tBatch 23 loss: 0.6841412782669067\n",
            "\tBatch 24 loss: 0.699516236782074\n",
            "\tBatch 25 loss: 0.496623694896698\n",
            "\tBatch 26 loss: 0.4680241346359253\n",
            "\tBatch 27 loss: 0.4170995354652405\n",
            "\tBatch 28 loss: 0.6019222140312195\n",
            "\tBatch 29 loss: 0.454629123210907\n",
            "\tBatch 30 loss: 0.5292655825614929\n",
            "\tBatch 31 loss: 0.5479956865310669\n",
            "\tBatch 32 loss: 0.5774733424186707\n",
            "\tBatch 33 loss: 0.5115295052528381\n",
            "\tBatch 34 loss: 0.6386522054672241\n",
            "\tBatch 35 loss: 0.6505266427993774\n",
            "\tBatch 36 loss: 0.5311711430549622\n",
            "\tBatch 37 loss: 0.5833853483200073\n",
            "\tBatch 38 loss: 0.8085330724716187\n",
            "\tBatch 39 loss: 0.671745777130127\n",
            "\tBatch 40 loss: 0.7922298908233643\n",
            "\tBatch 41 loss: 0.8599472045898438\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8221651524960449\n",
            "        AUPRC: 0.7916585206985474\n",
            "        Sensitivity: 0.6931306719779968\n",
            "        Specificity: 0.8146020770072937\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7299909515020186\n",
            "        AUPRC: 0.6726230978965759\n",
            "        Sensitivity: 0.7765406966209412\n",
            "        Specificity: 0.6088333129882812\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.555210530757904\n",
            "\tBatch 1 loss: 0.5102844834327698\n",
            "\tBatch 2 loss: 0.4652666449546814\n",
            "\tBatch 3 loss: 0.5461623072624207\n",
            "\tBatch 4 loss: 0.6946595907211304\n",
            "\tBatch 5 loss: 0.5170513391494751\n",
            "\tBatch 6 loss: 0.5860000848770142\n",
            "\tBatch 7 loss: 0.48688560724258423\n",
            "\tBatch 8 loss: 0.5739325881004333\n",
            "\tBatch 9 loss: 0.5004669427871704\n",
            "\tBatch 10 loss: 0.44584691524505615\n",
            "\tBatch 11 loss: 0.5512068271636963\n",
            "\tBatch 12 loss: 0.5522040724754333\n",
            "\tBatch 13 loss: 0.5625157356262207\n",
            "\tBatch 14 loss: 0.43263962864875793\n",
            "\tBatch 15 loss: 0.629257082939148\n",
            "\tBatch 16 loss: 0.7425183653831482\n",
            "\tBatch 17 loss: 0.6288339495658875\n",
            "\tBatch 18 loss: 0.4938228130340576\n",
            "\tBatch 19 loss: 0.5710890889167786\n",
            "\tBatch 20 loss: 0.5381254553794861\n",
            "\tBatch 21 loss: 0.6970185041427612\n",
            "\tBatch 22 loss: 0.4858884811401367\n",
            "\tBatch 23 loss: 0.6645011901855469\n",
            "\tBatch 24 loss: 0.6764287352561951\n",
            "\tBatch 25 loss: 0.5323246121406555\n",
            "\tBatch 26 loss: 0.47788503766059875\n",
            "\tBatch 27 loss: 0.4329937696456909\n",
            "\tBatch 28 loss: 0.5719259977340698\n",
            "\tBatch 29 loss: 0.46014469861984253\n",
            "\tBatch 30 loss: 0.5289885997772217\n",
            "\tBatch 31 loss: 0.5637511610984802\n",
            "\tBatch 32 loss: 0.6344015598297119\n",
            "\tBatch 33 loss: 0.49639177322387695\n",
            "\tBatch 34 loss: 0.5046505928039551\n",
            "\tBatch 35 loss: 0.5659005641937256\n",
            "\tBatch 36 loss: 0.5171232223510742\n",
            "\tBatch 37 loss: 0.5589216947555542\n",
            "\tBatch 38 loss: 0.7340899705886841\n",
            "\tBatch 39 loss: 0.6211913824081421\n",
            "\tBatch 40 loss: 0.7441102266311646\n",
            "\tBatch 41 loss: 0.7141492962837219\n",
            "    Train data metrics:\n",
            "        AUROC: 0.821345061237427\n",
            "        AUPRC: 0.7892197966575623\n",
            "        Sensitivity: 0.7533957362174988\n",
            "        Specificity: 0.7216466069221497\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7247556124316441\n",
            "        AUPRC: 0.6651229858398438\n",
            "        Sensitivity: 0.8356004357337952\n",
            "        Specificity: 0.5267995595932007\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5339133143424988\n",
            "\tBatch 1 loss: 0.5480998158454895\n",
            "\tBatch 2 loss: 0.47128158807754517\n",
            "\tBatch 3 loss: 0.5368139147758484\n",
            "\tBatch 4 loss: 0.6724475622177124\n",
            "\tBatch 5 loss: 0.5039947628974915\n",
            "\tBatch 6 loss: 0.5710912346839905\n",
            "\tBatch 7 loss: 0.4699374735355377\n",
            "\tBatch 8 loss: 0.563349187374115\n",
            "\tBatch 9 loss: 0.5027568936347961\n",
            "\tBatch 10 loss: 0.44942259788513184\n",
            "\tBatch 11 loss: 0.5448187589645386\n",
            "\tBatch 12 loss: 0.5467902421951294\n",
            "\tBatch 13 loss: 0.5800623893737793\n",
            "\tBatch 14 loss: 0.4262963831424713\n",
            "\tBatch 15 loss: 0.5934498906135559\n",
            "\tBatch 16 loss: 0.6998389959335327\n",
            "\tBatch 17 loss: 0.6026546955108643\n",
            "\tBatch 18 loss: 0.494052916765213\n",
            "\tBatch 19 loss: 0.5587874054908752\n",
            "\tBatch 20 loss: 0.5237187743186951\n",
            "\tBatch 21 loss: 0.6754204630851746\n",
            "\tBatch 22 loss: 0.47944581508636475\n",
            "\tBatch 23 loss: 0.6444287300109863\n",
            "\tBatch 24 loss: 0.6608859300613403\n",
            "\tBatch 25 loss: 0.5265253186225891\n",
            "\tBatch 26 loss: 0.4823826551437378\n",
            "\tBatch 27 loss: 0.41778478026390076\n",
            "\tBatch 28 loss: 0.5674474835395813\n",
            "\tBatch 29 loss: 0.46642133593559265\n",
            "\tBatch 30 loss: 0.5255399346351624\n",
            "\tBatch 31 loss: 0.5451145172119141\n",
            "\tBatch 32 loss: 0.6181042194366455\n",
            "\tBatch 33 loss: 0.4993772804737091\n",
            "\tBatch 34 loss: 0.49719181656837463\n",
            "\tBatch 35 loss: 0.5671440362930298\n",
            "\tBatch 36 loss: 0.5057942271232605\n",
            "\tBatch 37 loss: 0.5583363175392151\n",
            "\tBatch 38 loss: 0.7375018000602722\n",
            "\tBatch 39 loss: 0.6210905313491821\n",
            "\tBatch 40 loss: 0.7367240190505981\n",
            "\tBatch 41 loss: 0.718925952911377\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8250817077405562\n",
            "        AUPRC: 0.7931473851203918\n",
            "        Sensitivity: 0.7592583894729614\n",
            "        Specificity: 0.7314875721931458\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7202910099939559\n",
            "        AUPRC: 0.6624398231506348\n",
            "        Sensitivity: 0.8362736105918884\n",
            "        Specificity: 0.5196006298065186\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5269950032234192\n",
            "\tBatch 1 loss: 0.5626075267791748\n",
            "\tBatch 2 loss: 0.48136085271835327\n",
            "\tBatch 3 loss: 0.5224964618682861\n",
            "\tBatch 4 loss: 0.6558601260185242\n",
            "\tBatch 5 loss: 0.500440239906311\n",
            "\tBatch 6 loss: 0.5630906224250793\n",
            "\tBatch 7 loss: 0.48125508427619934\n",
            "\tBatch 8 loss: 0.568522036075592\n",
            "\tBatch 9 loss: 0.5014791488647461\n",
            "\tBatch 10 loss: 0.45540574193000793\n",
            "\tBatch 11 loss: 0.5365278124809265\n",
            "\tBatch 12 loss: 0.5411528944969177\n",
            "\tBatch 13 loss: 0.5626038908958435\n",
            "\tBatch 14 loss: 0.43277204036712646\n",
            "\tBatch 15 loss: 0.5954632759094238\n",
            "\tBatch 16 loss: 0.6957259178161621\n",
            "\tBatch 17 loss: 0.5952296257019043\n",
            "\tBatch 18 loss: 0.4980277121067047\n",
            "\tBatch 19 loss: 0.5503409504890442\n",
            "\tBatch 20 loss: 0.5307568907737732\n",
            "\tBatch 21 loss: 0.6842597126960754\n",
            "\tBatch 22 loss: 0.47632893919944763\n",
            "\tBatch 23 loss: 0.6385900378227234\n",
            "\tBatch 24 loss: 0.66658616065979\n",
            "\tBatch 25 loss: 0.5127981901168823\n",
            "\tBatch 26 loss: 0.4810311198234558\n",
            "\tBatch 27 loss: 0.41270768642425537\n",
            "\tBatch 28 loss: 0.576279878616333\n",
            "\tBatch 29 loss: 0.45995721220970154\n",
            "\tBatch 30 loss: 0.517998993396759\n",
            "\tBatch 31 loss: 0.5482183694839478\n",
            "\tBatch 32 loss: 0.6124548316001892\n",
            "\tBatch 33 loss: 0.5004415512084961\n",
            "\tBatch 34 loss: 0.5158159732818604\n",
            "\tBatch 35 loss: 0.5734918713569641\n",
            "\tBatch 36 loss: 0.5064113140106201\n",
            "\tBatch 37 loss: 0.5515830516815186\n",
            "\tBatch 38 loss: 0.7389894127845764\n",
            "\tBatch 39 loss: 0.6197022199630737\n",
            "\tBatch 40 loss: 0.7336504459381104\n",
            "\tBatch 41 loss: 0.7378267645835876\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8269918675221817\n",
            "        AUPRC: 0.797003984451294\n",
            "        Sensitivity: 0.736485481262207\n",
            "        Specificity: 0.7364906072616577\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7165830275587344\n",
            "        AUPRC: 0.6578831076622009\n",
            "        Sensitivity: 0.8208064436912537\n",
            "        Specificity: 0.5231109261512756\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5178991556167603\n",
            "\tBatch 1 loss: 0.5600073337554932\n",
            "\tBatch 2 loss: 0.47810065746307373\n",
            "\tBatch 3 loss: 0.5219449400901794\n",
            "\tBatch 4 loss: 0.677489161491394\n",
            "\tBatch 5 loss: 0.49493131041526794\n",
            "\tBatch 6 loss: 0.5592111349105835\n",
            "\tBatch 7 loss: 0.46724221110343933\n",
            "\tBatch 8 loss: 0.5553891658782959\n",
            "\tBatch 9 loss: 0.49559634923934937\n",
            "\tBatch 10 loss: 0.4564574360847473\n",
            "\tBatch 11 loss: 0.5443083047866821\n",
            "\tBatch 12 loss: 0.5457566380500793\n",
            "\tBatch 13 loss: 0.5580363273620605\n",
            "\tBatch 14 loss: 0.42892876267433167\n",
            "\tBatch 15 loss: 0.5830038785934448\n",
            "\tBatch 16 loss: 0.6797140836715698\n",
            "\tBatch 17 loss: 0.5952773094177246\n",
            "\tBatch 18 loss: 0.4951985478401184\n",
            "\tBatch 19 loss: 0.549518883228302\n",
            "\tBatch 20 loss: 0.5298366546630859\n",
            "\tBatch 21 loss: 0.6839665770530701\n",
            "\tBatch 22 loss: 0.4750347137451172\n",
            "\tBatch 23 loss: 0.6273602247238159\n",
            "\tBatch 24 loss: 0.6537357568740845\n",
            "\tBatch 25 loss: 0.5136932134628296\n",
            "\tBatch 26 loss: 0.4715142846107483\n",
            "\tBatch 27 loss: 0.42250511050224304\n",
            "\tBatch 28 loss: 0.5703373551368713\n",
            "\tBatch 29 loss: 0.4585717022418976\n",
            "\tBatch 30 loss: 0.5182895064353943\n",
            "\tBatch 31 loss: 0.5522933006286621\n",
            "\tBatch 32 loss: 0.6042463779449463\n",
            "\tBatch 33 loss: 0.500923216342926\n",
            "\tBatch 34 loss: 0.528130054473877\n",
            "\tBatch 35 loss: 0.5660578012466431\n",
            "\tBatch 36 loss: 0.5077559351921082\n",
            "\tBatch 37 loss: 0.5514113903045654\n",
            "\tBatch 38 loss: 0.742529034614563\n",
            "\tBatch 39 loss: 0.6241587400436401\n",
            "\tBatch 40 loss: 0.7320413589477539\n",
            "\tBatch 41 loss: 0.7414993643760681\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8293625678498965\n",
            "        AUPRC: 0.8006913065910339\n",
            "        Sensitivity: 0.7482092976570129\n",
            "        Specificity: 0.7473887205123901\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7137617217146008\n",
            "        AUPRC: 0.6562052965164185\n",
            "        Sensitivity: 0.8116507530212402\n",
            "        Specificity: 0.5358983874320984\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5278968214988708\n",
            "\tBatch 1 loss: 0.5603615641593933\n",
            "\tBatch 2 loss: 0.47211864590644836\n",
            "\tBatch 3 loss: 0.5043134689331055\n",
            "\tBatch 4 loss: 0.6676516532897949\n",
            "\tBatch 5 loss: 0.49702486395835876\n",
            "\tBatch 6 loss: 0.5584340691566467\n",
            "\tBatch 7 loss: 0.4648677408695221\n",
            "\tBatch 8 loss: 0.5635645389556885\n",
            "\tBatch 9 loss: 0.4956302344799042\n",
            "\tBatch 10 loss: 0.4526473879814148\n",
            "\tBatch 11 loss: 0.5333930850028992\n",
            "\tBatch 12 loss: 0.5365414023399353\n",
            "\tBatch 13 loss: 0.5534197092056274\n",
            "\tBatch 14 loss: 0.433217853307724\n",
            "\tBatch 15 loss: 0.588381826877594\n",
            "\tBatch 16 loss: 0.684857964515686\n",
            "\tBatch 17 loss: 0.598240077495575\n",
            "\tBatch 18 loss: 0.49383261799812317\n",
            "\tBatch 19 loss: 0.5452562570571899\n",
            "\tBatch 20 loss: 0.531208336353302\n",
            "\tBatch 21 loss: 0.6816047430038452\n",
            "\tBatch 22 loss: 0.47505658864974976\n",
            "\tBatch 23 loss: 0.6248376369476318\n",
            "\tBatch 24 loss: 0.666654109954834\n",
            "\tBatch 25 loss: 0.5125036239624023\n",
            "\tBatch 26 loss: 0.4661104381084442\n",
            "\tBatch 27 loss: 0.40729185938835144\n",
            "\tBatch 28 loss: 0.5738376975059509\n",
            "\tBatch 29 loss: 0.46119529008865356\n",
            "\tBatch 30 loss: 0.5214381217956543\n",
            "\tBatch 31 loss: 0.5479426383972168\n",
            "\tBatch 32 loss: 0.6007805466651917\n",
            "\tBatch 33 loss: 0.49665647745132446\n",
            "\tBatch 34 loss: 0.5339360237121582\n",
            "\tBatch 35 loss: 0.5780066847801208\n",
            "\tBatch 36 loss: 0.4951694905757904\n",
            "\tBatch 37 loss: 0.5363683104515076\n",
            "\tBatch 38 loss: 0.7414427399635315\n",
            "\tBatch 39 loss: 0.6356726288795471\n",
            "\tBatch 40 loss: 0.728381872177124\n",
            "\tBatch 41 loss: 0.7750281095504761\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8333043620609737\n",
            "        AUPRC: 0.805955171585083\n",
            "        Sensitivity: 0.7419852018356323\n",
            "        Specificity: 0.7388026118278503\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7099701333058157\n",
            "        AUPRC: 0.6521055698394775\n",
            "        Sensitivity: 0.8143402338027954\n",
            "        Specificity: 0.5305124521255493\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5276982188224792\n",
            "\tBatch 1 loss: 0.5705385208129883\n",
            "\tBatch 2 loss: 0.4734836220741272\n",
            "\tBatch 3 loss: 0.5125119686126709\n",
            "\tBatch 4 loss: 0.6773732900619507\n",
            "\tBatch 5 loss: 0.4930805563926697\n",
            "\tBatch 6 loss: 0.5582921504974365\n",
            "\tBatch 7 loss: 0.46937423944473267\n",
            "\tBatch 8 loss: 0.5503656268119812\n",
            "\tBatch 9 loss: 0.49728822708129883\n",
            "\tBatch 10 loss: 0.45280933380126953\n",
            "\tBatch 11 loss: 0.5326900482177734\n",
            "\tBatch 12 loss: 0.5413714647293091\n",
            "\tBatch 13 loss: 0.5407285094261169\n",
            "\tBatch 14 loss: 0.43931493163108826\n",
            "\tBatch 15 loss: 0.5873039364814758\n",
            "\tBatch 16 loss: 0.6656713485717773\n",
            "\tBatch 17 loss: 0.5897006392478943\n",
            "\tBatch 18 loss: 0.49377867579460144\n",
            "\tBatch 19 loss: 0.5408676862716675\n",
            "\tBatch 20 loss: 0.5161571502685547\n",
            "\tBatch 21 loss: 0.6670176386833191\n",
            "\tBatch 22 loss: 0.47316843271255493\n",
            "\tBatch 23 loss: 0.6168407201766968\n",
            "\tBatch 24 loss: 0.6483670473098755\n",
            "\tBatch 25 loss: 0.5076919198036194\n",
            "\tBatch 26 loss: 0.4694352149963379\n",
            "\tBatch 27 loss: 0.412638783454895\n",
            "\tBatch 28 loss: 0.5721628665924072\n",
            "\tBatch 29 loss: 0.45244306325912476\n",
            "\tBatch 30 loss: 0.5198004841804504\n",
            "\tBatch 31 loss: 0.5422502756118774\n",
            "\tBatch 32 loss: 0.5961709022521973\n",
            "\tBatch 33 loss: 0.505634069442749\n",
            "\tBatch 34 loss: 0.5257729291915894\n",
            "\tBatch 35 loss: 0.5730363130569458\n",
            "\tBatch 36 loss: 0.49987325072288513\n",
            "\tBatch 37 loss: 0.5343756675720215\n",
            "\tBatch 38 loss: 0.7419270277023315\n",
            "\tBatch 39 loss: 0.6401380896568298\n",
            "\tBatch 40 loss: 0.7253293395042419\n",
            "\tBatch 41 loss: 0.7680399417877197\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8359287403297009\n",
            "        AUPRC: 0.8093091249465942\n",
            "        Sensitivity: 0.7605885863304138\n",
            "        Specificity: 0.7314521074295044\n",
            "    Test data metrics:\n",
            "        AUROC: 0.70709968836999\n",
            "        AUPRC: 0.6505488753318787\n",
            "        Sensitivity: 0.8156439065933228\n",
            "        Specificity: 0.5232778787612915\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5261837840080261\n",
            "\tBatch 1 loss: 0.5737193822860718\n",
            "\tBatch 2 loss: 0.47301340103149414\n",
            "\tBatch 3 loss: 0.5016689896583557\n",
            "\tBatch 4 loss: 0.672121524810791\n",
            "\tBatch 5 loss: 0.4939804971218109\n",
            "\tBatch 6 loss: 0.5465023517608643\n",
            "\tBatch 7 loss: 0.4676453173160553\n",
            "\tBatch 8 loss: 0.560675859451294\n",
            "\tBatch 9 loss: 0.4912535548210144\n",
            "\tBatch 10 loss: 0.44413816928863525\n",
            "\tBatch 11 loss: 0.5320819616317749\n",
            "\tBatch 12 loss: 0.5396015644073486\n",
            "\tBatch 13 loss: 0.5486366152763367\n",
            "\tBatch 14 loss: 0.42650970816612244\n",
            "\tBatch 15 loss: 0.57267165184021\n",
            "\tBatch 16 loss: 0.6690548658370972\n",
            "\tBatch 17 loss: 0.5834454298019409\n",
            "\tBatch 18 loss: 0.4936201572418213\n",
            "\tBatch 19 loss: 0.5355900526046753\n",
            "\tBatch 20 loss: 0.5211679935455322\n",
            "\tBatch 21 loss: 0.6680871248245239\n",
            "\tBatch 22 loss: 0.47316259145736694\n",
            "\tBatch 23 loss: 0.6106235384941101\n",
            "\tBatch 24 loss: 0.6556298732757568\n",
            "\tBatch 25 loss: 0.4979626536369324\n",
            "\tBatch 26 loss: 0.46337613463401794\n",
            "\tBatch 27 loss: 0.40741634368896484\n",
            "\tBatch 28 loss: 0.5735636353492737\n",
            "\tBatch 29 loss: 0.4559791684150696\n",
            "\tBatch 30 loss: 0.5121516585350037\n",
            "\tBatch 31 loss: 0.52824467420578\n",
            "\tBatch 32 loss: 0.5894938707351685\n",
            "\tBatch 33 loss: 0.5024664402008057\n",
            "\tBatch 34 loss: 0.5359663963317871\n",
            "\tBatch 35 loss: 0.575054407119751\n",
            "\tBatch 36 loss: 0.4879286289215088\n",
            "\tBatch 37 loss: 0.5331059694290161\n",
            "\tBatch 38 loss: 0.7557581067085266\n",
            "\tBatch 39 loss: 0.631210207939148\n",
            "\tBatch 40 loss: 0.7165523767471313\n",
            "\tBatch 41 loss: 0.7834336757659912\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8361727147357179\n",
            "        AUPRC: 0.8115811944007874\n",
            "        Sensitivity: 0.7714077830314636\n",
            "        Specificity: 0.7303661704063416\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7039674694758967\n",
            "        AUPRC: 0.6490973234176636\n",
            "        Sensitivity: 0.8162548542022705\n",
            "        Specificity: 0.5243508219718933\n",
            "====================================\n",
            "     Epoch #9\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5276858806610107\n",
            "\tBatch 1 loss: 0.5726364850997925\n",
            "\tBatch 2 loss: 0.46882423758506775\n",
            "\tBatch 3 loss: 0.4951145648956299\n",
            "\tBatch 4 loss: 0.6767352819442749\n",
            "\tBatch 5 loss: 0.491435706615448\n",
            "\tBatch 6 loss: 0.5493720173835754\n",
            "\tBatch 7 loss: 0.4581502079963684\n",
            "\tBatch 8 loss: 0.5496708154678345\n",
            "\tBatch 9 loss: 0.49139147996902466\n",
            "\tBatch 10 loss: 0.4494927227497101\n",
            "\tBatch 11 loss: 0.5338228940963745\n",
            "\tBatch 12 loss: 0.5416915416717529\n",
            "\tBatch 13 loss: 0.5386636853218079\n",
            "\tBatch 14 loss: 0.42764177918434143\n",
            "\tBatch 15 loss: 0.5607120990753174\n",
            "\tBatch 16 loss: 0.6447109580039978\n",
            "\tBatch 17 loss: 0.5734517574310303\n",
            "\tBatch 18 loss: 0.49794670939445496\n",
            "\tBatch 19 loss: 0.525582492351532\n",
            "\tBatch 20 loss: 0.5125769972801208\n",
            "\tBatch 21 loss: 0.6630838513374329\n",
            "\tBatch 22 loss: 0.4628705382347107\n",
            "\tBatch 23 loss: 0.5984200239181519\n",
            "\tBatch 24 loss: 0.645339846611023\n",
            "\tBatch 25 loss: 0.5028632283210754\n",
            "\tBatch 26 loss: 0.4577673077583313\n",
            "\tBatch 27 loss: 0.4011073708534241\n",
            "\tBatch 28 loss: 0.5703855752944946\n",
            "\tBatch 29 loss: 0.4553513526916504\n",
            "\tBatch 30 loss: 0.5076830983161926\n",
            "\tBatch 31 loss: 0.531000554561615\n",
            "\tBatch 32 loss: 0.5740618109703064\n",
            "\tBatch 33 loss: 0.5029470920562744\n",
            "\tBatch 34 loss: 0.5389211177825928\n",
            "\tBatch 35 loss: 0.5742157101631165\n",
            "\tBatch 36 loss: 0.4803057312965393\n",
            "\tBatch 37 loss: 0.5282559990882874\n",
            "\tBatch 38 loss: 0.7645577192306519\n",
            "\tBatch 39 loss: 0.632341742515564\n",
            "\tBatch 40 loss: 0.7160040736198425\n",
            "\tBatch 41 loss: 0.7869778275489807\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8407119203876017\n",
            "        AUPRC: 0.8149521350860596\n",
            "        Sensitivity: 0.7749773859977722\n",
            "        Specificity: 0.7277029752731323\n",
            "    Test data metrics:\n",
            "        AUROC: 0.6996413292951338\n",
            "        AUPRC: 0.6438893675804138\n",
            "        Sensitivity: 0.8161023855209351\n",
            "        Specificity: 0.5218935012817383\n",
            "====================================\n",
            "     Epoch #10\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5268129706382751\n",
            "\tBatch 1 loss: 0.5752938985824585\n",
            "\tBatch 2 loss: 0.4723317325115204\n",
            "\tBatch 3 loss: 0.49063840508461\n",
            "\tBatch 4 loss: 0.6777042150497437\n",
            "\tBatch 5 loss: 0.49054211378097534\n",
            "\tBatch 6 loss: 0.5488525629043579\n",
            "\tBatch 7 loss: 0.457546204328537\n",
            "\tBatch 8 loss: 0.5463520288467407\n",
            "\tBatch 9 loss: 0.4869157075881958\n",
            "\tBatch 10 loss: 0.45162227749824524\n",
            "\tBatch 11 loss: 0.531464695930481\n",
            "\tBatch 12 loss: 0.5281431674957275\n",
            "\tBatch 13 loss: 0.5321825742721558\n",
            "\tBatch 14 loss: 0.4259505271911621\n",
            "\tBatch 15 loss: 0.5604692101478577\n",
            "\tBatch 16 loss: 0.6513846516609192\n",
            "\tBatch 17 loss: 0.5669744610786438\n",
            "\tBatch 18 loss: 0.4924495220184326\n",
            "\tBatch 19 loss: 0.5127122402191162\n",
            "\tBatch 20 loss: 0.5133963823318481\n",
            "\tBatch 21 loss: 0.6625133156776428\n",
            "\tBatch 22 loss: 0.45842573046684265\n",
            "\tBatch 23 loss: 0.5974669456481934\n",
            "\tBatch 24 loss: 0.6433258056640625\n",
            "\tBatch 25 loss: 0.4937785863876343\n",
            "\tBatch 26 loss: 0.46096888184547424\n",
            "\tBatch 27 loss: 0.40166178345680237\n",
            "\tBatch 28 loss: 0.5651838779449463\n",
            "\tBatch 29 loss: 0.45909544825553894\n",
            "\tBatch 30 loss: 0.5046364068984985\n",
            "\tBatch 31 loss: 0.5292972326278687\n",
            "\tBatch 32 loss: 0.5665101408958435\n",
            "\tBatch 33 loss: 0.5067659020423889\n",
            "\tBatch 34 loss: 0.5365957617759705\n",
            "\tBatch 35 loss: 0.5812336206436157\n",
            "\tBatch 36 loss: 0.479441374540329\n",
            "\tBatch 37 loss: 0.5285788178443909\n",
            "\tBatch 38 loss: 0.7587901949882507\n",
            "\tBatch 39 loss: 0.6498731374740601\n",
            "\tBatch 40 loss: 0.7187220454216003\n",
            "\tBatch 41 loss: 0.7922487854957581\n",
            "    Train data metrics:\n",
            "        AUROC: 0.841607475062227\n",
            "        AUPRC: 0.8157222270965576\n",
            "        Sensitivity: 0.772854208946228\n",
            "        Specificity: 0.7416684031486511\n",
            "    Test data metrics:\n",
            "        AUROC: 0.6982284787298074\n",
            "        AUPRC: 0.642792284488678\n",
            "        Sensitivity: 0.806344211101532\n",
            "        Specificity: 0.5314134955406189\n",
            "====================================\n",
            "     Epoch #11\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5194199681282043\n",
            "\tBatch 1 loss: 0.583473801612854\n",
            "\tBatch 2 loss: 0.46728459000587463\n",
            "\tBatch 3 loss: 0.4805752635002136\n",
            "\tBatch 4 loss: 0.6835234761238098\n",
            "\tBatch 5 loss: 0.48823001980781555\n",
            "\tBatch 6 loss: 0.5453053712844849\n",
            "\tBatch 7 loss: 0.45792248845100403\n",
            "\tBatch 8 loss: 0.5481154918670654\n",
            "\tBatch 9 loss: 0.4845329225063324\n",
            "\tBatch 10 loss: 0.4425376057624817\n",
            "\tBatch 11 loss: 0.5279672145843506\n",
            "\tBatch 12 loss: 0.5385090112686157\n",
            "\tBatch 13 loss: 0.5328265428543091\n",
            "\tBatch 14 loss: 0.41894617676734924\n",
            "\tBatch 15 loss: 0.5585516095161438\n",
            "\tBatch 16 loss: 0.6496223211288452\n",
            "\tBatch 17 loss: 0.5679563283920288\n",
            "\tBatch 18 loss: 0.5006105899810791\n",
            "\tBatch 19 loss: 0.5142603516578674\n",
            "\tBatch 20 loss: 0.5150899887084961\n",
            "\tBatch 21 loss: 0.6651154160499573\n",
            "\tBatch 22 loss: 0.46858546137809753\n",
            "\tBatch 23 loss: 0.5925525426864624\n",
            "\tBatch 24 loss: 0.63885098695755\n",
            "\tBatch 25 loss: 0.49404042959213257\n",
            "\tBatch 26 loss: 0.46285712718963623\n",
            "\tBatch 27 loss: 0.3999367952346802\n",
            "\tBatch 28 loss: 0.5725384950637817\n",
            "\tBatch 29 loss: 0.4547346830368042\n",
            "\tBatch 30 loss: 0.5081837177276611\n",
            "\tBatch 31 loss: 0.5318118929862976\n",
            "\tBatch 32 loss: 0.5645745992660522\n",
            "\tBatch 33 loss: 0.5030216574668884\n",
            "\tBatch 34 loss: 0.5559377670288086\n",
            "\tBatch 35 loss: 0.5879735946655273\n",
            "\tBatch 36 loss: 0.4860619306564331\n",
            "\tBatch 37 loss: 0.5231095552444458\n",
            "\tBatch 38 loss: 0.7716081142425537\n",
            "\tBatch 39 loss: 0.6532082557678223\n",
            "\tBatch 40 loss: 0.7153809666633606\n",
            "\tBatch 41 loss: 0.7944194674491882\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8437161726581325\n",
            "        AUPRC: 0.8190061450004578\n",
            "        Sensitivity: 0.7729460597038269\n",
            "        Specificity: 0.7380591034889221\n",
            "    Test data metrics:\n",
            "        AUROC: 0.6969816453760542\n",
            "        AUPRC: 0.6440718173980713\n",
            "        Sensitivity: 0.7979555130004883\n",
            "        Specificity: 0.5299029350280762\n",
            "Training on data block 3!!!!\n",
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6686688661575317\n",
            "\tBatch 1 loss: 0.468168705701828\n",
            "\tBatch 2 loss: 0.6960843801498413\n",
            "\tBatch 3 loss: 0.46480029821395874\n",
            "\tBatch 4 loss: 0.6520388126373291\n",
            "\tBatch 5 loss: 0.5297161340713501\n",
            "\tBatch 6 loss: 0.5618566274642944\n",
            "\tBatch 7 loss: 0.6117386221885681\n",
            "\tBatch 8 loss: 0.4607592523097992\n",
            "\tBatch 9 loss: 0.5308893322944641\n",
            "\tBatch 10 loss: 0.6674782037734985\n",
            "\tBatch 11 loss: 0.5283124446868896\n",
            "\tBatch 12 loss: 0.5192212462425232\n",
            "\tBatch 13 loss: 0.4696342647075653\n",
            "\tBatch 14 loss: 0.600246787071228\n",
            "\tBatch 15 loss: 0.49520349502563477\n",
            "\tBatch 16 loss: 0.5866826772689819\n",
            "\tBatch 17 loss: 0.6007992029190063\n",
            "\tBatch 18 loss: 0.6707656383514404\n",
            "\tBatch 19 loss: 0.6710541248321533\n",
            "\tBatch 20 loss: 0.4160148799419403\n",
            "\tBatch 21 loss: 0.5293774604797363\n",
            "\tBatch 22 loss: 0.565685510635376\n",
            "\tBatch 23 loss: 0.5026397109031677\n",
            "\tBatch 24 loss: 0.5237089991569519\n",
            "\tBatch 25 loss: 0.7338609099388123\n",
            "\tBatch 26 loss: 0.7042703032493591\n",
            "\tBatch 27 loss: 0.7404806613922119\n",
            "\tBatch 28 loss: 0.7405233383178711\n",
            "\tBatch 29 loss: 0.6250133514404297\n",
            "\tBatch 30 loss: 0.6369119882583618\n",
            "\tBatch 31 loss: 0.3999733030796051\n",
            "\tBatch 32 loss: 0.61625075340271\n",
            "\tBatch 33 loss: 0.6966339945793152\n",
            "\tBatch 34 loss: 0.4667745530605316\n",
            "\tBatch 35 loss: 0.5313446521759033\n",
            "\tBatch 36 loss: 0.5932672619819641\n",
            "\tBatch 37 loss: 0.6177104115486145\n",
            "\tBatch 38 loss: 0.5696157217025757\n",
            "\tBatch 39 loss: 0.5840930938720703\n",
            "\tBatch 40 loss: 0.5885746479034424\n",
            "\tBatch 41 loss: 0.6897099614143372\n",
            "\tBatch 42 loss: 0.5057640075683594\n",
            "\tBatch 43 loss: 0.5643004775047302\n",
            "\tBatch 44 loss: 0.49531254172325134\n",
            "\tBatch 45 loss: 0.6168161630630493\n",
            "\tBatch 46 loss: 0.658247172832489\n",
            "\tBatch 47 loss: 0.614509642124176\n",
            "\tBatch 48 loss: 0.5255768895149231\n",
            "\tBatch 49 loss: 0.5614047050476074\n",
            "\tBatch 50 loss: 0.7508045434951782\n",
            "\tBatch 51 loss: 0.579568088054657\n",
            "\tBatch 52 loss: 0.5448139309883118\n",
            "\tBatch 53 loss: 0.733392596244812\n",
            "\tBatch 54 loss: 0.6018843650817871\n",
            "\tBatch 55 loss: 0.5290892720222473\n",
            "\tBatch 56 loss: 0.536209225654602\n",
            "\tBatch 57 loss: 0.6113995909690857\n",
            "\tBatch 58 loss: 0.6510894894599915\n",
            "\tBatch 59 loss: 0.6871038675308228\n",
            "\tBatch 60 loss: 0.6320956349372864\n",
            "\tBatch 61 loss: 0.49892130494117737\n",
            "\tBatch 62 loss: 0.6023623943328857\n",
            "\tBatch 63 loss: 0.5363379716873169\n",
            "\tBatch 64 loss: 0.694340169429779\n",
            "\tBatch 65 loss: 0.6670984625816345\n",
            "\tBatch 66 loss: 0.6086214780807495\n",
            "\tBatch 67 loss: 0.6362783908843994\n",
            "\tBatch 68 loss: 0.5151830315589905\n",
            "\tBatch 69 loss: 0.7470406889915466\n",
            "\tBatch 70 loss: 0.5059964656829834\n",
            "\tBatch 71 loss: 0.48684805631637573\n",
            "\tBatch 72 loss: 0.5560349225997925\n",
            "\tBatch 73 loss: 0.6924506425857544\n",
            "\tBatch 74 loss: 0.5076566934585571\n",
            "\tBatch 75 loss: 0.4984794855117798\n",
            "\tBatch 76 loss: 0.5433198809623718\n",
            "\tBatch 77 loss: 0.734793484210968\n",
            "\tBatch 78 loss: 0.525224506855011\n",
            "\tBatch 79 loss: 0.534315824508667\n",
            "\tBatch 80 loss: 0.4790620803833008\n",
            "\tBatch 81 loss: 0.6264943480491638\n",
            "\tBatch 82 loss: 0.5935150384902954\n",
            "\tBatch 83 loss: 0.4974361062049866\n",
            "\tBatch 84 loss: 0.5910605192184448\n",
            "\tBatch 85 loss: 0.4859207272529602\n",
            "\tBatch 86 loss: 0.548818051815033\n",
            "\tBatch 87 loss: 0.501904308795929\n",
            "\tBatch 88 loss: 0.5338779091835022\n",
            "\tBatch 89 loss: 0.6088626980781555\n",
            "\tBatch 90 loss: 0.5829797387123108\n",
            "    Train data metrics:\n",
            "        AUROC: 0.7865076309150647\n",
            "        AUPRC: 0.7438601851463318\n",
            "        Sensitivity: 0.792803168296814\n",
            "        Specificity: 0.6422015428543091\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7004254373682643\n",
            "        AUPRC: 0.6518078446388245\n",
            "        Sensitivity: 0.664556622505188\n",
            "        Specificity: 0.6645975112915039\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6176983118057251\n",
            "\tBatch 1 loss: 0.46823781728744507\n",
            "\tBatch 2 loss: 0.6087685227394104\n",
            "\tBatch 3 loss: 0.48683974146842957\n",
            "\tBatch 4 loss: 0.6677658557891846\n",
            "\tBatch 5 loss: 0.48020699620246887\n",
            "\tBatch 6 loss: 0.4425833523273468\n",
            "\tBatch 7 loss: 0.6087989807128906\n",
            "\tBatch 8 loss: 0.49248558282852173\n",
            "\tBatch 9 loss: 0.5309767127037048\n",
            "\tBatch 10 loss: 0.6135028600692749\n",
            "\tBatch 11 loss: 0.517784833908081\n",
            "\tBatch 12 loss: 0.4874737858772278\n",
            "\tBatch 13 loss: 0.48932820558547974\n",
            "\tBatch 14 loss: 0.588095486164093\n",
            "\tBatch 15 loss: 0.4974464774131775\n",
            "\tBatch 16 loss: 0.5824616551399231\n",
            "\tBatch 17 loss: 0.5845983624458313\n",
            "\tBatch 18 loss: 0.6230618357658386\n",
            "\tBatch 19 loss: 0.6508389711380005\n",
            "\tBatch 20 loss: 0.4248485565185547\n",
            "\tBatch 21 loss: 0.5269950032234192\n",
            "\tBatch 22 loss: 0.5619833469390869\n",
            "\tBatch 23 loss: 0.5174365043640137\n",
            "\tBatch 24 loss: 0.5357273817062378\n",
            "\tBatch 25 loss: 0.6925395131111145\n",
            "\tBatch 26 loss: 0.6688321232795715\n",
            "\tBatch 27 loss: 0.7040070295333862\n",
            "\tBatch 28 loss: 0.7247145771980286\n",
            "\tBatch 29 loss: 0.625882625579834\n",
            "\tBatch 30 loss: 0.6282674074172974\n",
            "\tBatch 31 loss: 0.4226573407649994\n",
            "\tBatch 32 loss: 0.6020085215568542\n",
            "\tBatch 33 loss: 0.6729462146759033\n",
            "\tBatch 34 loss: 0.4777625799179077\n",
            "\tBatch 35 loss: 0.5266506671905518\n",
            "\tBatch 36 loss: 0.5884472131729126\n",
            "\tBatch 37 loss: 0.6078591346740723\n",
            "\tBatch 38 loss: 0.5877484679222107\n",
            "\tBatch 39 loss: 0.5975661277770996\n",
            "\tBatch 40 loss: 0.5628343820571899\n",
            "\tBatch 41 loss: 0.6713374853134155\n",
            "\tBatch 42 loss: 0.5086190700531006\n",
            "\tBatch 43 loss: 0.5537347197532654\n",
            "\tBatch 44 loss: 0.5039174556732178\n",
            "\tBatch 45 loss: 0.6113148331642151\n",
            "\tBatch 46 loss: 0.6483184695243835\n",
            "\tBatch 47 loss: 0.6185376644134521\n",
            "\tBatch 48 loss: 0.5321414470672607\n",
            "\tBatch 49 loss: 0.548570990562439\n",
            "\tBatch 50 loss: 0.7199057340621948\n",
            "\tBatch 51 loss: 0.56699138879776\n",
            "\tBatch 52 loss: 0.5404731035232544\n",
            "\tBatch 53 loss: 0.7354715466499329\n",
            "\tBatch 54 loss: 0.6132440567016602\n",
            "\tBatch 55 loss: 0.5301890969276428\n",
            "\tBatch 56 loss: 0.5304384231567383\n",
            "\tBatch 57 loss: 0.6055063605308533\n",
            "\tBatch 58 loss: 0.657244086265564\n",
            "\tBatch 59 loss: 0.6794523596763611\n",
            "\tBatch 60 loss: 0.627251148223877\n",
            "\tBatch 61 loss: 0.4985959231853485\n",
            "\tBatch 62 loss: 0.598023533821106\n",
            "\tBatch 63 loss: 0.5350711345672607\n",
            "\tBatch 64 loss: 0.6822534799575806\n",
            "\tBatch 65 loss: 0.669621467590332\n",
            "\tBatch 66 loss: 0.6093079447746277\n",
            "\tBatch 67 loss: 0.6362433433532715\n",
            "\tBatch 68 loss: 0.5093842148780823\n",
            "\tBatch 69 loss: 0.7319234609603882\n",
            "\tBatch 70 loss: 0.5072446465492249\n",
            "\tBatch 71 loss: 0.49315640330314636\n",
            "\tBatch 72 loss: 0.5548966526985168\n",
            "\tBatch 73 loss: 0.7059553861618042\n",
            "\tBatch 74 loss: 0.5110161900520325\n",
            "\tBatch 75 loss: 0.5012072324752808\n",
            "\tBatch 76 loss: 0.5398488640785217\n",
            "\tBatch 77 loss: 0.7417476773262024\n",
            "\tBatch 78 loss: 0.5214473009109497\n",
            "\tBatch 79 loss: 0.5327979922294617\n",
            "\tBatch 80 loss: 0.4765683114528656\n",
            "\tBatch 81 loss: 0.6136821508407593\n",
            "\tBatch 82 loss: 0.5848881006240845\n",
            "\tBatch 83 loss: 0.4959756135940552\n",
            "\tBatch 84 loss: 0.5918882489204407\n",
            "\tBatch 85 loss: 0.4894977807998657\n",
            "\tBatch 86 loss: 0.5493851900100708\n",
            "\tBatch 87 loss: 0.5139126777648926\n",
            "\tBatch 88 loss: 0.5298513770103455\n",
            "\tBatch 89 loss: 0.6017087697982788\n",
            "\tBatch 90 loss: 0.5809162259101868\n",
            "    Train data metrics:\n",
            "        AUROC: 0.7928676770468369\n",
            "        AUPRC: 0.7533697485923767\n",
            "        Sensitivity: 0.7677550911903381\n",
            "        Specificity: 0.6695787906646729\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7024275729850793\n",
            "        AUPRC: 0.6548011302947998\n",
            "        Sensitivity: 0.6407361030578613\n",
            "        Specificity: 0.6795908212661743\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6032907962799072\n",
            "\tBatch 1 loss: 0.46701639890670776\n",
            "\tBatch 2 loss: 0.6121315956115723\n",
            "\tBatch 3 loss: 0.48882701992988586\n",
            "\tBatch 4 loss: 0.6708022356033325\n",
            "\tBatch 5 loss: 0.476275235414505\n",
            "\tBatch 6 loss: 0.4423657953739166\n",
            "\tBatch 7 loss: 0.5991045832633972\n",
            "\tBatch 8 loss: 0.4845723509788513\n",
            "\tBatch 9 loss: 0.5223575830459595\n",
            "\tBatch 10 loss: 0.6188422441482544\n",
            "\tBatch 11 loss: 0.5165504813194275\n",
            "\tBatch 12 loss: 0.4786447286605835\n",
            "\tBatch 13 loss: 0.4814128279685974\n",
            "\tBatch 14 loss: 0.5842142105102539\n",
            "\tBatch 15 loss: 0.4951925277709961\n",
            "\tBatch 16 loss: 0.5808809995651245\n",
            "\tBatch 17 loss: 0.5837572813034058\n",
            "\tBatch 18 loss: 0.6086312532424927\n",
            "\tBatch 19 loss: 0.6428984999656677\n",
            "\tBatch 20 loss: 0.42490077018737793\n",
            "\tBatch 21 loss: 0.5300588607788086\n",
            "\tBatch 22 loss: 0.5680976510047913\n",
            "\tBatch 23 loss: 0.5161505937576294\n",
            "\tBatch 24 loss: 0.5348114967346191\n",
            "\tBatch 25 loss: 0.6938159465789795\n",
            "\tBatch 26 loss: 0.6700767874717712\n",
            "\tBatch 27 loss: 0.6945083737373352\n",
            "\tBatch 28 loss: 0.7258284687995911\n",
            "\tBatch 29 loss: 0.6267778277397156\n",
            "\tBatch 30 loss: 0.6208896636962891\n",
            "\tBatch 31 loss: 0.4200187921524048\n",
            "\tBatch 32 loss: 0.6024532318115234\n",
            "\tBatch 33 loss: 0.6794205904006958\n",
            "\tBatch 34 loss: 0.4759982228279114\n",
            "\tBatch 35 loss: 0.532964289188385\n",
            "\tBatch 36 loss: 0.5881420373916626\n",
            "\tBatch 37 loss: 0.6088309288024902\n",
            "\tBatch 38 loss: 0.5694922804832458\n",
            "\tBatch 39 loss: 0.5785614848136902\n",
            "\tBatch 40 loss: 0.568436861038208\n",
            "\tBatch 41 loss: 0.6822362542152405\n",
            "\tBatch 42 loss: 0.5087382197380066\n",
            "\tBatch 43 loss: 0.549889862537384\n",
            "\tBatch 44 loss: 0.4999588429927826\n",
            "\tBatch 45 loss: 0.612919807434082\n",
            "\tBatch 46 loss: 0.6484286785125732\n",
            "\tBatch 47 loss: 0.6128232479095459\n",
            "\tBatch 48 loss: 0.5293079614639282\n",
            "\tBatch 49 loss: 0.5465415716171265\n",
            "\tBatch 50 loss: 0.7383928894996643\n",
            "\tBatch 51 loss: 0.575707197189331\n",
            "\tBatch 52 loss: 0.5361791849136353\n",
            "\tBatch 53 loss: 0.7271773219108582\n",
            "\tBatch 54 loss: 0.5968421697616577\n",
            "\tBatch 55 loss: 0.5245866775512695\n",
            "\tBatch 56 loss: 0.5270717144012451\n",
            "\tBatch 57 loss: 0.61622554063797\n",
            "\tBatch 58 loss: 0.6718900203704834\n",
            "\tBatch 59 loss: 0.6910907030105591\n",
            "\tBatch 60 loss: 0.6293361783027649\n",
            "\tBatch 61 loss: 0.4903702139854431\n",
            "\tBatch 62 loss: 0.5914180278778076\n",
            "\tBatch 63 loss: 0.5327250361442566\n",
            "\tBatch 64 loss: 0.6796239614486694\n",
            "\tBatch 65 loss: 0.6706546545028687\n",
            "\tBatch 66 loss: 0.6123305559158325\n",
            "\tBatch 67 loss: 0.6397314667701721\n",
            "\tBatch 68 loss: 0.5131463408470154\n",
            "\tBatch 69 loss: 0.725503146648407\n",
            "\tBatch 70 loss: 0.5039396286010742\n",
            "\tBatch 71 loss: 0.5016062259674072\n",
            "\tBatch 72 loss: 0.5517176985740662\n",
            "\tBatch 73 loss: 0.7054370045661926\n",
            "\tBatch 74 loss: 0.5105310678482056\n",
            "\tBatch 75 loss: 0.5012291669845581\n",
            "\tBatch 76 loss: 0.5312011241912842\n",
            "\tBatch 77 loss: 0.7448863387107849\n",
            "\tBatch 78 loss: 0.5175766944885254\n",
            "\tBatch 79 loss: 0.533104419708252\n",
            "\tBatch 80 loss: 0.47121185064315796\n",
            "\tBatch 81 loss: 0.5993145108222961\n",
            "\tBatch 82 loss: 0.5720723867416382\n",
            "\tBatch 83 loss: 0.4950157105922699\n",
            "\tBatch 84 loss: 0.6032515168190002\n",
            "\tBatch 85 loss: 0.4919983744621277\n",
            "\tBatch 86 loss: 0.5550695657730103\n",
            "\tBatch 87 loss: 0.5101348757743835\n",
            "\tBatch 88 loss: 0.542657196521759\n",
            "\tBatch 89 loss: 0.5961055159568787\n",
            "\tBatch 90 loss: 0.5730926990509033\n",
            "    Train data metrics:\n",
            "        AUROC: 0.7939489210842288\n",
            "        AUPRC: 0.7548425793647766\n",
            "        Sensitivity: 0.7393726110458374\n",
            "        Specificity: 0.6972171068191528\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7025736804728164\n",
            "        AUPRC: 0.6565638780593872\n",
            "        Sensitivity: 0.6072021126747131\n",
            "        Specificity: 0.7069479823112488\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6025385856628418\n",
            "\tBatch 1 loss: 0.4682480990886688\n",
            "\tBatch 2 loss: 0.6104381084442139\n",
            "\tBatch 3 loss: 0.48742952942848206\n",
            "\tBatch 4 loss: 0.661649227142334\n",
            "\tBatch 5 loss: 0.47955843806266785\n",
            "\tBatch 6 loss: 0.4515836536884308\n",
            "\tBatch 7 loss: 0.594640851020813\n",
            "\tBatch 8 loss: 0.4871510863304138\n",
            "\tBatch 9 loss: 0.5346693992614746\n",
            "\tBatch 10 loss: 0.6022570133209229\n",
            "\tBatch 11 loss: 0.5086349248886108\n",
            "\tBatch 12 loss: 0.478640615940094\n",
            "\tBatch 13 loss: 0.4815741777420044\n",
            "\tBatch 14 loss: 0.5957087278366089\n",
            "\tBatch 15 loss: 0.4963981509208679\n",
            "\tBatch 16 loss: 0.5792499780654907\n",
            "\tBatch 17 loss: 0.5718424916267395\n",
            "\tBatch 18 loss: 0.5950284004211426\n",
            "\tBatch 19 loss: 0.6572741866111755\n",
            "\tBatch 20 loss: 0.4280923902988434\n",
            "\tBatch 21 loss: 0.5436490774154663\n",
            "\tBatch 22 loss: 0.5598635077476501\n",
            "\tBatch 23 loss: 0.508527398109436\n",
            "\tBatch 24 loss: 0.5395185947418213\n",
            "\tBatch 25 loss: 0.6895254254341125\n",
            "\tBatch 26 loss: 0.6571474075317383\n",
            "\tBatch 27 loss: 0.6962577700614929\n",
            "\tBatch 28 loss: 0.726614773273468\n",
            "\tBatch 29 loss: 0.6160072684288025\n",
            "\tBatch 30 loss: 0.6246370673179626\n",
            "\tBatch 31 loss: 0.4119112193584442\n",
            "\tBatch 32 loss: 0.6056811809539795\n",
            "\tBatch 33 loss: 0.6842114329338074\n",
            "\tBatch 34 loss: 0.47864800691604614\n",
            "\tBatch 35 loss: 0.5214966535568237\n",
            "\tBatch 36 loss: 0.5772174596786499\n",
            "\tBatch 37 loss: 0.5975886583328247\n",
            "\tBatch 38 loss: 0.5777450203895569\n",
            "\tBatch 39 loss: 0.5777853727340698\n",
            "\tBatch 40 loss: 0.5718874335289001\n",
            "\tBatch 41 loss: 0.6799497604370117\n",
            "\tBatch 42 loss: 0.5017479062080383\n",
            "\tBatch 43 loss: 0.5546699166297913\n",
            "\tBatch 44 loss: 0.4938955008983612\n",
            "\tBatch 45 loss: 0.6073589324951172\n",
            "\tBatch 46 loss: 0.6452077627182007\n",
            "\tBatch 47 loss: 0.6150450706481934\n",
            "\tBatch 48 loss: 0.5174010396003723\n",
            "\tBatch 49 loss: 0.529297947883606\n",
            "\tBatch 50 loss: 0.7494650483131409\n",
            "\tBatch 51 loss: 0.5772070288658142\n",
            "\tBatch 52 loss: 0.533995509147644\n",
            "\tBatch 53 loss: 0.7245346307754517\n",
            "\tBatch 54 loss: 0.5953407287597656\n",
            "\tBatch 55 loss: 0.5300539135932922\n",
            "\tBatch 56 loss: 0.5327333211898804\n",
            "\tBatch 57 loss: 0.6163316965103149\n",
            "\tBatch 58 loss: 0.6751949787139893\n",
            "\tBatch 59 loss: 0.6937252283096313\n",
            "\tBatch 60 loss: 0.6146774888038635\n",
            "\tBatch 61 loss: 0.49034398794174194\n",
            "\tBatch 62 loss: 0.584480881690979\n",
            "\tBatch 63 loss: 0.5289209485054016\n",
            "\tBatch 64 loss: 0.678322434425354\n",
            "\tBatch 65 loss: 0.6815158128738403\n",
            "\tBatch 66 loss: 0.6169679164886475\n",
            "\tBatch 67 loss: 0.646510899066925\n",
            "\tBatch 68 loss: 0.5075235366821289\n",
            "\tBatch 69 loss: 0.7171025276184082\n",
            "\tBatch 70 loss: 0.49696338176727295\n",
            "\tBatch 71 loss: 0.49558568000793457\n",
            "\tBatch 72 loss: 0.5460542440414429\n",
            "\tBatch 73 loss: 0.701345682144165\n",
            "\tBatch 74 loss: 0.5134860873222351\n",
            "\tBatch 75 loss: 0.49999719858169556\n",
            "\tBatch 76 loss: 0.5313605070114136\n",
            "\tBatch 77 loss: 0.7514326572418213\n",
            "\tBatch 78 loss: 0.5255575180053711\n",
            "\tBatch 79 loss: 0.5352392792701721\n",
            "\tBatch 80 loss: 0.4724389612674713\n",
            "\tBatch 81 loss: 0.5965867042541504\n",
            "\tBatch 82 loss: 0.568531334400177\n",
            "\tBatch 83 loss: 0.5006613731384277\n",
            "\tBatch 84 loss: 0.5914182662963867\n",
            "\tBatch 85 loss: 0.4910079836845398\n",
            "\tBatch 86 loss: 0.5465291738510132\n",
            "\tBatch 87 loss: 0.5012373924255371\n",
            "\tBatch 88 loss: 0.5401355028152466\n",
            "\tBatch 89 loss: 0.5971020460128784\n",
            "\tBatch 90 loss: 0.5637145638465881\n",
            "    Train data metrics:\n",
            "        AUROC: 0.7955515259786661\n",
            "        AUPRC: 0.7554976344108582\n",
            "        Sensitivity: 0.737851083278656\n",
            "        Specificity: 0.6998883485794067\n",
            "    Test data metrics:\n",
            "        AUROC: 0.703457037001025\n",
            "        AUPRC: 0.6607033014297485\n",
            "        Sensitivity: 0.6057935953140259\n",
            "        Specificity: 0.6980288028717041\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5983741283416748\n",
            "\tBatch 1 loss: 0.4669918715953827\n",
            "\tBatch 2 loss: 0.6134943962097168\n",
            "\tBatch 3 loss: 0.4786239564418793\n",
            "\tBatch 4 loss: 0.6603964567184448\n",
            "\tBatch 5 loss: 0.4810757040977478\n",
            "\tBatch 6 loss: 0.4481315016746521\n",
            "\tBatch 7 loss: 0.5965571999549866\n",
            "\tBatch 8 loss: 0.47433948516845703\n",
            "\tBatch 9 loss: 0.5261369347572327\n",
            "\tBatch 10 loss: 0.5924423933029175\n",
            "\tBatch 11 loss: 0.5060814023017883\n",
            "\tBatch 12 loss: 0.47688785195350647\n",
            "\tBatch 13 loss: 0.48436862230300903\n",
            "\tBatch 14 loss: 0.5961047410964966\n",
            "\tBatch 15 loss: 0.49170246720314026\n",
            "\tBatch 16 loss: 0.5834347009658813\n",
            "\tBatch 17 loss: 0.5708090662956238\n",
            "\tBatch 18 loss: 0.5951499342918396\n",
            "\tBatch 19 loss: 0.6440749764442444\n",
            "\tBatch 20 loss: 0.4236792325973511\n",
            "\tBatch 21 loss: 0.5364034175872803\n",
            "\tBatch 22 loss: 0.5635191202163696\n",
            "\tBatch 23 loss: 0.5017809867858887\n",
            "\tBatch 24 loss: 0.530002236366272\n",
            "\tBatch 25 loss: 0.6949471831321716\n",
            "\tBatch 26 loss: 0.6627675294876099\n",
            "\tBatch 27 loss: 0.6894423961639404\n",
            "\tBatch 28 loss: 0.7167826294898987\n",
            "\tBatch 29 loss: 0.6189497113227844\n",
            "\tBatch 30 loss: 0.6162354946136475\n",
            "\tBatch 31 loss: 0.40348953008651733\n",
            "\tBatch 32 loss: 0.6013795137405396\n",
            "\tBatch 33 loss: 0.696505606174469\n",
            "\tBatch 34 loss: 0.47843122482299805\n",
            "\tBatch 35 loss: 0.5260116457939148\n",
            "\tBatch 36 loss: 0.5724753737449646\n",
            "\tBatch 37 loss: 0.5993603467941284\n",
            "\tBatch 38 loss: 0.5674123167991638\n",
            "\tBatch 39 loss: 0.5690827965736389\n",
            "\tBatch 40 loss: 0.5860137343406677\n",
            "\tBatch 41 loss: 0.6794452667236328\n",
            "\tBatch 42 loss: 0.5071167945861816\n",
            "\tBatch 43 loss: 0.5512553453445435\n",
            "\tBatch 44 loss: 0.4938575327396393\n",
            "\tBatch 45 loss: 0.6062998175621033\n",
            "\tBatch 46 loss: 0.6290420889854431\n",
            "\tBatch 47 loss: 0.6059114336967468\n",
            "\tBatch 48 loss: 0.5124508142471313\n",
            "\tBatch 49 loss: 0.5410382151603699\n",
            "\tBatch 50 loss: 0.7462329864501953\n",
            "\tBatch 51 loss: 0.5659390687942505\n",
            "\tBatch 52 loss: 0.534089207649231\n",
            "\tBatch 53 loss: 0.7174328565597534\n",
            "\tBatch 54 loss: 0.5860614776611328\n",
            "\tBatch 55 loss: 0.5236271023750305\n",
            "\tBatch 56 loss: 0.5305675268173218\n",
            "\tBatch 57 loss: 0.6201769709587097\n",
            "\tBatch 58 loss: 0.6791117191314697\n",
            "\tBatch 59 loss: 0.6959770321846008\n",
            "\tBatch 60 loss: 0.616163969039917\n",
            "\tBatch 61 loss: 0.48092561960220337\n",
            "\tBatch 62 loss: 0.5749111175537109\n",
            "\tBatch 63 loss: 0.5347989797592163\n",
            "\tBatch 64 loss: 0.678667426109314\n",
            "\tBatch 65 loss: 0.6859219670295715\n",
            "\tBatch 66 loss: 0.6123907566070557\n",
            "\tBatch 67 loss: 0.6414275765419006\n",
            "\tBatch 68 loss: 0.502268373966217\n",
            "\tBatch 69 loss: 0.6961983442306519\n",
            "\tBatch 70 loss: 0.5044609308242798\n",
            "\tBatch 71 loss: 0.501602053642273\n",
            "\tBatch 72 loss: 0.5456259250640869\n",
            "\tBatch 73 loss: 0.7108205556869507\n",
            "\tBatch 74 loss: 0.5112826228141785\n",
            "\tBatch 75 loss: 0.49345141649246216\n",
            "\tBatch 76 loss: 0.5316663980484009\n",
            "\tBatch 77 loss: 0.7395601272583008\n",
            "\tBatch 78 loss: 0.5198688507080078\n",
            "\tBatch 79 loss: 0.5312405824661255\n",
            "\tBatch 80 loss: 0.47345995903015137\n",
            "\tBatch 81 loss: 0.5956742167472839\n",
            "\tBatch 82 loss: 0.5644443035125732\n",
            "\tBatch 83 loss: 0.4952830672264099\n",
            "\tBatch 84 loss: 0.6031599044799805\n",
            "\tBatch 85 loss: 0.49272239208221436\n",
            "\tBatch 86 loss: 0.5519780516624451\n",
            "\tBatch 87 loss: 0.5042928457260132\n",
            "\tBatch 88 loss: 0.539720892906189\n",
            "\tBatch 89 loss: 0.590127170085907\n",
            "\tBatch 90 loss: 0.5641936659812927\n",
            "    Train data metrics:\n",
            "        AUROC: 0.7982489874772116\n",
            "        AUPRC: 0.75894695520401\n",
            "        Sensitivity: 0.7191694378852844\n",
            "        Specificity: 0.717800498008728\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7016300738505937\n",
            "        AUPRC: 0.6582688093185425\n",
            "        Sensitivity: 0.5877234935760498\n",
            "        Specificity: 0.7257006168365479\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5944126844406128\n",
            "\tBatch 1 loss: 0.4659331440925598\n",
            "\tBatch 2 loss: 0.6171801090240479\n",
            "\tBatch 3 loss: 0.4843781888484955\n",
            "\tBatch 4 loss: 0.6686212420463562\n",
            "\tBatch 5 loss: 0.47248920798301697\n",
            "\tBatch 6 loss: 0.45423364639282227\n",
            "\tBatch 7 loss: 0.5965506434440613\n",
            "\tBatch 8 loss: 0.47097858786582947\n",
            "\tBatch 9 loss: 0.5291897654533386\n",
            "\tBatch 10 loss: 0.6037625074386597\n",
            "\tBatch 11 loss: 0.5005730986595154\n",
            "\tBatch 12 loss: 0.47548869252204895\n",
            "\tBatch 13 loss: 0.47985076904296875\n",
            "\tBatch 14 loss: 0.596289873123169\n",
            "\tBatch 15 loss: 0.48464033007621765\n",
            "\tBatch 16 loss: 0.589431881904602\n",
            "\tBatch 17 loss: 0.5695726275444031\n",
            "\tBatch 18 loss: 0.5883230566978455\n",
            "\tBatch 19 loss: 0.630418598651886\n",
            "\tBatch 20 loss: 0.419173926115036\n",
            "\tBatch 21 loss: 0.5391258001327515\n",
            "\tBatch 22 loss: 0.5638812184333801\n",
            "\tBatch 23 loss: 0.5145249962806702\n",
            "\tBatch 24 loss: 0.5364709496498108\n",
            "\tBatch 25 loss: 0.6932045221328735\n",
            "\tBatch 26 loss: 0.653640866279602\n",
            "\tBatch 27 loss: 0.6984063386917114\n",
            "\tBatch 28 loss: 0.7116085290908813\n",
            "\tBatch 29 loss: 0.6142049431800842\n",
            "\tBatch 30 loss: 0.6119452118873596\n",
            "\tBatch 31 loss: 0.40900781750679016\n",
            "\tBatch 32 loss: 0.602561891078949\n",
            "\tBatch 33 loss: 0.6792999505996704\n",
            "\tBatch 34 loss: 0.49476397037506104\n",
            "\tBatch 35 loss: 0.5196976065635681\n",
            "\tBatch 36 loss: 0.5726611614227295\n",
            "\tBatch 37 loss: 0.5931471586227417\n",
            "\tBatch 38 loss: 0.5742340087890625\n",
            "\tBatch 39 loss: 0.5676844716072083\n",
            "\tBatch 40 loss: 0.5671311020851135\n",
            "\tBatch 41 loss: 0.6700985431671143\n",
            "\tBatch 42 loss: 0.4976077079772949\n",
            "\tBatch 43 loss: 0.5450162291526794\n",
            "\tBatch 44 loss: 0.4900606870651245\n",
            "\tBatch 45 loss: 0.6036067008972168\n",
            "\tBatch 46 loss: 0.6564027070999146\n",
            "\tBatch 47 loss: 0.5920745134353638\n",
            "\tBatch 48 loss: 0.5120717287063599\n",
            "\tBatch 49 loss: 0.5362827777862549\n",
            "\tBatch 50 loss: 0.7502661943435669\n",
            "\tBatch 51 loss: 0.5703083872795105\n",
            "\tBatch 52 loss: 0.5285257697105408\n",
            "\tBatch 53 loss: 0.723337709903717\n",
            "\tBatch 54 loss: 0.5857386589050293\n",
            "\tBatch 55 loss: 0.5208621621131897\n",
            "\tBatch 56 loss: 0.5298138856887817\n",
            "\tBatch 57 loss: 0.6193057894706726\n",
            "\tBatch 58 loss: 0.680736780166626\n",
            "\tBatch 59 loss: 0.691344141960144\n",
            "\tBatch 60 loss: 0.6169987320899963\n",
            "\tBatch 61 loss: 0.4760768413543701\n",
            "\tBatch 62 loss: 0.5717690587043762\n",
            "\tBatch 63 loss: 0.5312227606773376\n",
            "\tBatch 64 loss: 0.6852232217788696\n",
            "\tBatch 65 loss: 0.6822992563247681\n",
            "\tBatch 66 loss: 0.6084060668945312\n",
            "\tBatch 67 loss: 0.6419926881790161\n",
            "\tBatch 68 loss: 0.5010398626327515\n",
            "\tBatch 69 loss: 0.6861522197723389\n",
            "\tBatch 70 loss: 0.5043163895606995\n",
            "\tBatch 71 loss: 0.5003507733345032\n",
            "\tBatch 72 loss: 0.5486847162246704\n",
            "\tBatch 73 loss: 0.7117928862571716\n",
            "\tBatch 74 loss: 0.5057362914085388\n",
            "\tBatch 75 loss: 0.4912561774253845\n",
            "\tBatch 76 loss: 0.5371126532554626\n",
            "\tBatch 77 loss: 0.7404031157493591\n",
            "\tBatch 78 loss: 0.5187121033668518\n",
            "\tBatch 79 loss: 0.5323697924613953\n",
            "\tBatch 80 loss: 0.47212398052215576\n",
            "\tBatch 81 loss: 0.5908375382423401\n",
            "\tBatch 82 loss: 0.5664557814598083\n",
            "\tBatch 83 loss: 0.49849528074264526\n",
            "\tBatch 84 loss: 0.6012526750564575\n",
            "\tBatch 85 loss: 0.4860694408416748\n",
            "\tBatch 86 loss: 0.5492970943450928\n",
            "\tBatch 87 loss: 0.5012518167495728\n",
            "\tBatch 88 loss: 0.5394213795661926\n",
            "\tBatch 89 loss: 0.5823569893836975\n",
            "\tBatch 90 loss: 0.5563730597496033\n",
            "    Train data metrics:\n",
            "        AUROC: 0.800476488384392\n",
            "        AUPRC: 0.7627581357955933\n",
            "        Sensitivity: 0.7134491801261902\n",
            "        Specificity: 0.7243279218673706\n",
            "    Test data metrics:\n",
            "        AUROC: 0.6998125500080601\n",
            "        AUPRC: 0.6572668552398682\n",
            "        Sensitivity: 0.5590313673019409\n",
            "        Specificity: 0.7287737131118774\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6014474630355835\n",
            "\tBatch 1 loss: 0.4656161069869995\n",
            "\tBatch 2 loss: 0.6132534742355347\n",
            "\tBatch 3 loss: 0.4828100800514221\n",
            "\tBatch 4 loss: 0.6621752381324768\n",
            "\tBatch 5 loss: 0.4745202660560608\n",
            "\tBatch 6 loss: 0.4569990336894989\n",
            "\tBatch 7 loss: 0.5964757800102234\n",
            "\tBatch 8 loss: 0.4644353985786438\n",
            "\tBatch 9 loss: 0.5340273976325989\n",
            "\tBatch 10 loss: 0.5947153568267822\n",
            "\tBatch 11 loss: 0.4947088956832886\n",
            "\tBatch 12 loss: 0.47641521692276\n",
            "\tBatch 13 loss: 0.47917884588241577\n",
            "\tBatch 14 loss: 0.6046006083488464\n",
            "\tBatch 15 loss: 0.47823095321655273\n",
            "\tBatch 16 loss: 0.5819309949874878\n",
            "\tBatch 17 loss: 0.5676306486129761\n",
            "\tBatch 18 loss: 0.588164746761322\n",
            "\tBatch 19 loss: 0.6335784196853638\n",
            "\tBatch 20 loss: 0.41721177101135254\n",
            "\tBatch 21 loss: 0.5421246290206909\n",
            "\tBatch 22 loss: 0.5576112270355225\n",
            "\tBatch 23 loss: 0.5007681846618652\n",
            "\tBatch 24 loss: 0.5353596210479736\n",
            "\tBatch 25 loss: 0.6964832544326782\n",
            "\tBatch 26 loss: 0.651458740234375\n",
            "\tBatch 27 loss: 0.6896657943725586\n",
            "\tBatch 28 loss: 0.7132224440574646\n",
            "\tBatch 29 loss: 0.6182821989059448\n",
            "\tBatch 30 loss: 0.6131216883659363\n",
            "\tBatch 31 loss: 0.403490275144577\n",
            "\tBatch 32 loss: 0.5935559868812561\n",
            "\tBatch 33 loss: 0.6763548254966736\n",
            "\tBatch 34 loss: 0.49473488330841064\n",
            "\tBatch 35 loss: 0.513552188873291\n",
            "\tBatch 36 loss: 0.5651074647903442\n",
            "\tBatch 37 loss: 0.5871590971946716\n",
            "\tBatch 38 loss: 0.5664452910423279\n",
            "\tBatch 39 loss: 0.5700785517692566\n",
            "\tBatch 40 loss: 0.5654782056808472\n",
            "\tBatch 41 loss: 0.6661684513092041\n",
            "\tBatch 42 loss: 0.500034511089325\n",
            "\tBatch 43 loss: 0.5454186201095581\n",
            "\tBatch 44 loss: 0.4927099943161011\n",
            "\tBatch 45 loss: 0.5963736772537231\n",
            "\tBatch 46 loss: 0.6529375314712524\n",
            "\tBatch 47 loss: 0.5941980481147766\n",
            "\tBatch 48 loss: 0.5187848806381226\n",
            "\tBatch 49 loss: 0.5304381251335144\n",
            "\tBatch 50 loss: 0.7496680617332458\n",
            "\tBatch 51 loss: 0.5590727925300598\n",
            "\tBatch 52 loss: 0.523019552230835\n",
            "\tBatch 53 loss: 0.7145513296127319\n",
            "\tBatch 54 loss: 0.5728875398635864\n",
            "\tBatch 55 loss: 0.5252522230148315\n",
            "\tBatch 56 loss: 0.5228813886642456\n",
            "\tBatch 57 loss: 0.6198573708534241\n",
            "\tBatch 58 loss: 0.6792559027671814\n",
            "\tBatch 59 loss: 0.6928160190582275\n",
            "\tBatch 60 loss: 0.6103734374046326\n",
            "\tBatch 61 loss: 0.4763927757740021\n",
            "\tBatch 62 loss: 0.5691236853599548\n",
            "\tBatch 63 loss: 0.5197315812110901\n",
            "\tBatch 64 loss: 0.6841714382171631\n",
            "\tBatch 65 loss: 0.6990426778793335\n",
            "\tBatch 66 loss: 0.6123582720756531\n",
            "\tBatch 67 loss: 0.6444752812385559\n",
            "\tBatch 68 loss: 0.49255385994911194\n",
            "\tBatch 69 loss: 0.6714299321174622\n",
            "\tBatch 70 loss: 0.49556052684783936\n",
            "\tBatch 71 loss: 0.5000705718994141\n",
            "\tBatch 72 loss: 0.5524675846099854\n",
            "\tBatch 73 loss: 0.7226086854934692\n",
            "\tBatch 74 loss: 0.5085452795028687\n",
            "\tBatch 75 loss: 0.4860827326774597\n",
            "\tBatch 76 loss: 0.5388026237487793\n",
            "\tBatch 77 loss: 0.7490798234939575\n",
            "\tBatch 78 loss: 0.5186454057693481\n",
            "\tBatch 79 loss: 0.5306304097175598\n",
            "\tBatch 80 loss: 0.4684237837791443\n",
            "\tBatch 81 loss: 0.587077260017395\n",
            "\tBatch 82 loss: 0.5635733604431152\n",
            "\tBatch 83 loss: 0.5024911165237427\n",
            "\tBatch 84 loss: 0.5976950526237488\n",
            "\tBatch 85 loss: 0.4942074418067932\n",
            "\tBatch 86 loss: 0.5462480187416077\n",
            "\tBatch 87 loss: 0.49899381399154663\n",
            "\tBatch 88 loss: 0.5400469899177551\n",
            "\tBatch 89 loss: 0.5815925598144531\n",
            "\tBatch 90 loss: 0.552676796913147\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8023613950543127\n",
            "        AUPRC: 0.7656415104866028\n",
            "        Sensitivity: 0.7161401510238647\n",
            "        Specificity: 0.726055920124054\n",
            "    Test data metrics:\n",
            "        AUROC: 0.6997621277540225\n",
            "        AUPRC: 0.6584631204605103\n",
            "        Sensitivity: 0.5731477737426758\n",
            "        Specificity: 0.7206413149833679\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5916191339492798\n",
            "\tBatch 1 loss: 0.4586300253868103\n",
            "\tBatch 2 loss: 0.6200346946716309\n",
            "\tBatch 3 loss: 0.4829593300819397\n",
            "\tBatch 4 loss: 0.6568004488945007\n",
            "\tBatch 5 loss: 0.4776361584663391\n",
            "\tBatch 6 loss: 0.4558715224266052\n",
            "\tBatch 7 loss: 0.5962787866592407\n",
            "\tBatch 8 loss: 0.464738667011261\n",
            "\tBatch 9 loss: 0.5327099561691284\n",
            "\tBatch 10 loss: 0.5849257111549377\n",
            "\tBatch 11 loss: 0.4917004406452179\n",
            "\tBatch 12 loss: 0.4772587716579437\n",
            "\tBatch 13 loss: 0.46782514452934265\n",
            "\tBatch 14 loss: 0.6050312519073486\n",
            "\tBatch 15 loss: 0.4818078279495239\n",
            "\tBatch 16 loss: 0.5780976414680481\n",
            "\tBatch 17 loss: 0.5687060356140137\n",
            "\tBatch 18 loss: 0.5904746055603027\n",
            "\tBatch 19 loss: 0.6299431324005127\n",
            "\tBatch 20 loss: 0.42064011096954346\n",
            "\tBatch 21 loss: 0.5364242792129517\n",
            "\tBatch 22 loss: 0.5565634369850159\n",
            "\tBatch 23 loss: 0.5005132555961609\n",
            "\tBatch 24 loss: 0.5260799527168274\n",
            "\tBatch 25 loss: 0.6847462058067322\n",
            "\tBatch 26 loss: 0.6452051997184753\n",
            "\tBatch 27 loss: 0.6913972496986389\n",
            "\tBatch 28 loss: 0.7036659121513367\n",
            "\tBatch 29 loss: 0.6116854548454285\n",
            "\tBatch 30 loss: 0.6141165494918823\n",
            "\tBatch 31 loss: 0.3974948525428772\n",
            "\tBatch 32 loss: 0.595349133014679\n",
            "\tBatch 33 loss: 0.6840158104896545\n",
            "\tBatch 34 loss: 0.49019837379455566\n",
            "\tBatch 35 loss: 0.5103711485862732\n",
            "\tBatch 36 loss: 0.5610749125480652\n",
            "\tBatch 37 loss: 0.5859121680259705\n",
            "\tBatch 38 loss: 0.5776922106742859\n",
            "\tBatch 39 loss: 0.569756031036377\n",
            "\tBatch 40 loss: 0.564967155456543\n",
            "\tBatch 41 loss: 0.6779842376708984\n",
            "\tBatch 42 loss: 0.4995717406272888\n",
            "\tBatch 43 loss: 0.5434285402297974\n",
            "\tBatch 44 loss: 0.4931207001209259\n",
            "\tBatch 45 loss: 0.6020717620849609\n",
            "\tBatch 46 loss: 0.6412015557289124\n",
            "\tBatch 47 loss: 0.5998818874359131\n",
            "\tBatch 48 loss: 0.5042182207107544\n",
            "\tBatch 49 loss: 0.5285030007362366\n",
            "\tBatch 50 loss: 0.7579783201217651\n",
            "\tBatch 51 loss: 0.5610218048095703\n",
            "\tBatch 52 loss: 0.5256320238113403\n",
            "\tBatch 53 loss: 0.7120175957679749\n",
            "\tBatch 54 loss: 0.5748691558837891\n",
            "\tBatch 55 loss: 0.524925947189331\n",
            "\tBatch 56 loss: 0.5257330536842346\n",
            "\tBatch 57 loss: 0.6313288807868958\n",
            "\tBatch 58 loss: 0.6807023882865906\n",
            "\tBatch 59 loss: 0.6943020820617676\n",
            "\tBatch 60 loss: 0.615664005279541\n",
            "\tBatch 61 loss: 0.4679619371891022\n",
            "\tBatch 62 loss: 0.5651456117630005\n",
            "\tBatch 63 loss: 0.5238868594169617\n",
            "\tBatch 64 loss: 0.6878420114517212\n",
            "\tBatch 65 loss: 0.6946293711662292\n",
            "\tBatch 66 loss: 0.6083647012710571\n",
            "\tBatch 67 loss: 0.6415181756019592\n",
            "\tBatch 68 loss: 0.5009565353393555\n",
            "\tBatch 69 loss: 0.6760218143463135\n",
            "\tBatch 70 loss: 0.490443617105484\n",
            "\tBatch 71 loss: 0.5022855997085571\n",
            "\tBatch 72 loss: 0.5440483093261719\n",
            "\tBatch 73 loss: 0.7172669172286987\n",
            "\tBatch 74 loss: 0.5109961628913879\n",
            "\tBatch 75 loss: 0.4897102117538452\n",
            "\tBatch 76 loss: 0.5400396585464478\n",
            "\tBatch 77 loss: 0.7480476498603821\n",
            "\tBatch 78 loss: 0.5244050621986389\n",
            "\tBatch 79 loss: 0.5284371376037598\n",
            "\tBatch 80 loss: 0.4675804674625397\n",
            "\tBatch 81 loss: 0.5793554782867432\n",
            "\tBatch 82 loss: 0.5553066730499268\n",
            "\tBatch 83 loss: 0.4990167021751404\n",
            "\tBatch 84 loss: 0.6005722880363464\n",
            "\tBatch 85 loss: 0.49092742800712585\n",
            "\tBatch 86 loss: 0.551712691783905\n",
            "\tBatch 87 loss: 0.4890258014202118\n",
            "\tBatch 88 loss: 0.5448490977287292\n",
            "\tBatch 89 loss: 0.5746350884437561\n",
            "\tBatch 90 loss: 0.5473176836967468\n",
            "    Train data metrics:\n",
            "        AUROC: 0.802573580684016\n",
            "        AUPRC: 0.7660369277000427\n",
            "        Sensitivity: 0.708617091178894\n",
            "        Specificity: 0.7335336804389954\n",
            "    Test data metrics:\n",
            "        AUROC: 0.701888902304237\n",
            "        AUPRC: 0.6615966558456421\n",
            "        Sensitivity: 0.5532580018043518\n",
            "        Specificity: 0.7354757785797119\n",
            "====================================\n",
            "     Epoch #9\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5954239964485168\n",
            "\tBatch 1 loss: 0.4628324508666992\n",
            "\tBatch 2 loss: 0.6163270473480225\n",
            "\tBatch 3 loss: 0.4746623933315277\n",
            "\tBatch 4 loss: 0.658225953578949\n",
            "\tBatch 5 loss: 0.47449326515197754\n",
            "\tBatch 6 loss: 0.45001330971717834\n",
            "\tBatch 7 loss: 0.5865836143493652\n",
            "\tBatch 8 loss: 0.46584653854370117\n",
            "\tBatch 9 loss: 0.5317832231521606\n",
            "\tBatch 10 loss: 0.583498477935791\n",
            "\tBatch 11 loss: 0.5025156736373901\n",
            "\tBatch 12 loss: 0.47459813952445984\n",
            "\tBatch 13 loss: 0.4755348265171051\n",
            "\tBatch 14 loss: 0.6098945140838623\n",
            "\tBatch 15 loss: 0.47629424929618835\n",
            "\tBatch 16 loss: 0.5873138904571533\n",
            "\tBatch 17 loss: 0.5700660347938538\n",
            "\tBatch 18 loss: 0.5791814923286438\n",
            "\tBatch 19 loss: 0.6315935850143433\n",
            "\tBatch 20 loss: 0.413253515958786\n",
            "\tBatch 21 loss: 0.5365294218063354\n",
            "\tBatch 22 loss: 0.5655328035354614\n",
            "\tBatch 23 loss: 0.5053783655166626\n",
            "\tBatch 24 loss: 0.5306903123855591\n",
            "\tBatch 25 loss: 0.691802978515625\n",
            "\tBatch 26 loss: 0.6476447582244873\n",
            "\tBatch 27 loss: 0.6781971454620361\n",
            "\tBatch 28 loss: 0.7097957134246826\n",
            "\tBatch 29 loss: 0.6177423000335693\n",
            "\tBatch 30 loss: 0.6100155115127563\n",
            "\tBatch 31 loss: 0.3979690372943878\n",
            "\tBatch 32 loss: 0.5949786901473999\n",
            "\tBatch 33 loss: 0.6823581457138062\n",
            "\tBatch 34 loss: 0.49171292781829834\n",
            "\tBatch 35 loss: 0.5101678371429443\n",
            "\tBatch 36 loss: 0.5638302564620972\n",
            "\tBatch 37 loss: 0.591681182384491\n",
            "\tBatch 38 loss: 0.570988655090332\n",
            "\tBatch 39 loss: 0.5625606775283813\n",
            "\tBatch 40 loss: 0.5689732432365417\n",
            "\tBatch 41 loss: 0.6697410941123962\n",
            "\tBatch 42 loss: 0.4934617280960083\n",
            "\tBatch 43 loss: 0.5438669919967651\n",
            "\tBatch 44 loss: 0.4874153435230255\n",
            "\tBatch 45 loss: 0.5953250527381897\n",
            "\tBatch 46 loss: 0.6444624662399292\n",
            "\tBatch 47 loss: 0.5979520678520203\n",
            "\tBatch 48 loss: 0.4998817443847656\n",
            "\tBatch 49 loss: 0.5263898968696594\n",
            "\tBatch 50 loss: 0.7557832598686218\n",
            "\tBatch 51 loss: 0.5659467577934265\n",
            "\tBatch 52 loss: 0.5275620818138123\n",
            "\tBatch 53 loss: 0.7127980589866638\n",
            "\tBatch 54 loss: 0.5815223455429077\n",
            "\tBatch 55 loss: 0.5208243131637573\n",
            "\tBatch 56 loss: 0.5233680009841919\n",
            "\tBatch 57 loss: 0.6203518509864807\n",
            "\tBatch 58 loss: 0.6750900149345398\n",
            "\tBatch 59 loss: 0.689260721206665\n",
            "\tBatch 60 loss: 0.6118468046188354\n",
            "\tBatch 61 loss: 0.469388872385025\n",
            "\tBatch 62 loss: 0.5734970569610596\n",
            "\tBatch 63 loss: 0.5334607362747192\n",
            "\tBatch 64 loss: 0.6751036643981934\n",
            "\tBatch 65 loss: 0.6903334856033325\n",
            "\tBatch 66 loss: 0.611896276473999\n",
            "\tBatch 67 loss: 0.6341961026191711\n",
            "\tBatch 68 loss: 0.48868951201438904\n",
            "\tBatch 69 loss: 0.6750485301017761\n",
            "\tBatch 70 loss: 0.4892924427986145\n",
            "\tBatch 71 loss: 0.49693021178245544\n",
            "\tBatch 72 loss: 0.5497562885284424\n",
            "\tBatch 73 loss: 0.7132969498634338\n",
            "\tBatch 74 loss: 0.5028697848320007\n",
            "\tBatch 75 loss: 0.48862478137016296\n",
            "\tBatch 76 loss: 0.540460467338562\n",
            "\tBatch 77 loss: 0.7519923448562622\n",
            "\tBatch 78 loss: 0.5162196159362793\n",
            "\tBatch 79 loss: 0.5299445986747742\n",
            "\tBatch 80 loss: 0.47092747688293457\n",
            "\tBatch 81 loss: 0.5733301043510437\n",
            "\tBatch 82 loss: 0.5561602115631104\n",
            "\tBatch 83 loss: 0.49917715787887573\n",
            "\tBatch 84 loss: 0.5965274572372437\n",
            "\tBatch 85 loss: 0.49058327078819275\n",
            "\tBatch 86 loss: 0.5534158945083618\n",
            "\tBatch 87 loss: 0.4977266192436218\n",
            "\tBatch 88 loss: 0.5412642359733582\n",
            "\tBatch 89 loss: 0.5844774842262268\n",
            "\tBatch 90 loss: 0.5480725765228271\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8047496466819529\n",
            "        AUPRC: 0.7688531279563904\n",
            "        Sensitivity: 0.7058539986610413\n",
            "        Specificity: 0.7306023836135864\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7012332837901541\n",
            "        AUPRC: 0.6603511571884155\n",
            "        Sensitivity: 0.5647917985916138\n",
            "        Specificity: 0.7306609153747559\n",
            "====================================\n",
            "     Epoch #10\n",
            "====================================\n",
            "\tBatch 0 loss: 0.588289201259613\n",
            "\tBatch 1 loss: 0.46236568689346313\n",
            "\tBatch 2 loss: 0.6151257157325745\n",
            "\tBatch 3 loss: 0.47407814860343933\n",
            "\tBatch 4 loss: 0.6528307199478149\n",
            "\tBatch 5 loss: 0.47311514616012573\n",
            "\tBatch 6 loss: 0.4571424126625061\n",
            "\tBatch 7 loss: 0.592764675617218\n",
            "\tBatch 8 loss: 0.46492648124694824\n",
            "\tBatch 9 loss: 0.5147796869277954\n",
            "\tBatch 10 loss: 0.5896930694580078\n",
            "\tBatch 11 loss: 0.49806153774261475\n",
            "\tBatch 12 loss: 0.4746871590614319\n",
            "\tBatch 13 loss: 0.47459691762924194\n",
            "\tBatch 14 loss: 0.6012100577354431\n",
            "\tBatch 15 loss: 0.4807087779045105\n",
            "\tBatch 16 loss: 0.5810187458992004\n",
            "\tBatch 17 loss: 0.568264365196228\n",
            "\tBatch 18 loss: 0.5688871145248413\n",
            "\tBatch 19 loss: 0.6339234113693237\n",
            "\tBatch 20 loss: 0.4161168336868286\n",
            "\tBatch 21 loss: 0.536324679851532\n",
            "\tBatch 22 loss: 0.5613043308258057\n",
            "\tBatch 23 loss: 0.493945837020874\n",
            "\tBatch 24 loss: 0.5376574993133545\n",
            "\tBatch 25 loss: 0.693859875202179\n",
            "\tBatch 26 loss: 0.6427735686302185\n",
            "\tBatch 27 loss: 0.6806395053863525\n",
            "\tBatch 28 loss: 0.7089812755584717\n",
            "\tBatch 29 loss: 0.617251455783844\n",
            "\tBatch 30 loss: 0.6128045320510864\n",
            "\tBatch 31 loss: 0.3882468342781067\n",
            "\tBatch 32 loss: 0.5905585289001465\n",
            "\tBatch 33 loss: 0.6818069815635681\n",
            "\tBatch 34 loss: 0.4929506182670593\n",
            "\tBatch 35 loss: 0.5054937601089478\n",
            "\tBatch 36 loss: 0.5597395300865173\n",
            "\tBatch 37 loss: 0.5864516496658325\n",
            "\tBatch 38 loss: 0.5696933269500732\n",
            "\tBatch 39 loss: 0.5607615113258362\n",
            "\tBatch 40 loss: 0.5678849220275879\n",
            "\tBatch 41 loss: 0.6631582379341125\n",
            "\tBatch 42 loss: 0.4957124590873718\n",
            "\tBatch 43 loss: 0.5467768311500549\n",
            "\tBatch 44 loss: 0.48929715156555176\n",
            "\tBatch 45 loss: 0.5960240364074707\n",
            "\tBatch 46 loss: 0.6482672691345215\n",
            "\tBatch 47 loss: 0.596432089805603\n",
            "\tBatch 48 loss: 0.49911126494407654\n",
            "\tBatch 49 loss: 0.5242285132408142\n",
            "\tBatch 50 loss: 0.7461811900138855\n",
            "\tBatch 51 loss: 0.575666606426239\n",
            "\tBatch 52 loss: 0.523430585861206\n",
            "\tBatch 53 loss: 0.7126359343528748\n",
            "\tBatch 54 loss: 0.5796567797660828\n",
            "\tBatch 55 loss: 0.5132226943969727\n",
            "\tBatch 56 loss: 0.5191294550895691\n",
            "\tBatch 57 loss: 0.6297048926353455\n",
            "\tBatch 58 loss: 0.6814097762107849\n",
            "\tBatch 59 loss: 0.6819872856140137\n",
            "\tBatch 60 loss: 0.6052405834197998\n",
            "\tBatch 61 loss: 0.4641934037208557\n",
            "\tBatch 62 loss: 0.5676912069320679\n",
            "\tBatch 63 loss: 0.5293636918067932\n",
            "\tBatch 64 loss: 0.6830493211746216\n",
            "\tBatch 65 loss: 0.6867687702178955\n",
            "\tBatch 66 loss: 0.604395866394043\n",
            "\tBatch 67 loss: 0.6377361416816711\n",
            "\tBatch 68 loss: 0.48714014887809753\n",
            "\tBatch 69 loss: 0.6728813648223877\n",
            "\tBatch 70 loss: 0.48658519983291626\n",
            "\tBatch 71 loss: 0.504852294921875\n",
            "\tBatch 72 loss: 0.5444371700286865\n",
            "\tBatch 73 loss: 0.7111178636550903\n",
            "\tBatch 74 loss: 0.5044741034507751\n",
            "\tBatch 75 loss: 0.4718073010444641\n",
            "\tBatch 76 loss: 0.5445021390914917\n",
            "\tBatch 77 loss: 0.7566335797309875\n",
            "\tBatch 78 loss: 0.5232361555099487\n",
            "\tBatch 79 loss: 0.533417284488678\n",
            "\tBatch 80 loss: 0.46511515974998474\n",
            "\tBatch 81 loss: 0.5732350945472717\n",
            "\tBatch 82 loss: 0.5575578212738037\n",
            "\tBatch 83 loss: 0.5009384155273438\n",
            "\tBatch 84 loss: 0.601076602935791\n",
            "\tBatch 85 loss: 0.4877915680408478\n",
            "\tBatch 86 loss: 0.5478195548057556\n",
            "\tBatch 87 loss: 0.4893430769443512\n",
            "\tBatch 88 loss: 0.5455339550971985\n",
            "\tBatch 89 loss: 0.5775801539421082\n",
            "\tBatch 90 loss: 0.5452680587768555\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8044507234972407\n",
            "        AUPRC: 0.7689042687416077\n",
            "        Sensitivity: 0.7143765091896057\n",
            "        Specificity: 0.7251548171043396\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7000052514221222\n",
            "        AUPRC: 0.6594582796096802\n",
            "        Sensitivity: 0.5642356276512146\n",
            "        Specificity: 0.7294277548789978\n",
            "====================================\n",
            "     Epoch #11\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5900738835334778\n",
            "\tBatch 1 loss: 0.4589577317237854\n",
            "\tBatch 2 loss: 0.6142169833183289\n",
            "\tBatch 3 loss: 0.47428029775619507\n",
            "\tBatch 4 loss: 0.657876193523407\n",
            "\tBatch 5 loss: 0.47769802808761597\n",
            "\tBatch 6 loss: 0.4593518078327179\n",
            "\tBatch 7 loss: 0.5861949920654297\n",
            "\tBatch 8 loss: 0.46038612723350525\n",
            "\tBatch 9 loss: 0.5322521924972534\n",
            "\tBatch 10 loss: 0.5899933576583862\n",
            "\tBatch 11 loss: 0.49339577555656433\n",
            "\tBatch 12 loss: 0.4676065146923065\n",
            "\tBatch 13 loss: 0.46674293279647827\n",
            "\tBatch 14 loss: 0.6022630333900452\n",
            "\tBatch 15 loss: 0.4784795939922333\n",
            "\tBatch 16 loss: 0.5718825459480286\n",
            "\tBatch 17 loss: 0.5618204474449158\n",
            "\tBatch 18 loss: 0.5709007382392883\n",
            "\tBatch 19 loss: 0.627692699432373\n",
            "\tBatch 20 loss: 0.4138011932373047\n",
            "\tBatch 21 loss: 0.5402765274047852\n",
            "\tBatch 22 loss: 0.5600239634513855\n",
            "\tBatch 23 loss: 0.4981890320777893\n",
            "\tBatch 24 loss: 0.5250813364982605\n",
            "\tBatch 25 loss: 0.6800423860549927\n",
            "\tBatch 26 loss: 0.6415568590164185\n",
            "\tBatch 27 loss: 0.6714455485343933\n",
            "\tBatch 28 loss: 0.7148110866546631\n",
            "\tBatch 29 loss: 0.6216841340065002\n",
            "\tBatch 30 loss: 0.6075355410575867\n",
            "\tBatch 31 loss: 0.39372387528419495\n",
            "\tBatch 32 loss: 0.5861039757728577\n",
            "\tBatch 33 loss: 0.6801691055297852\n",
            "\tBatch 34 loss: 0.5031000375747681\n",
            "\tBatch 35 loss: 0.5057397484779358\n",
            "\tBatch 36 loss: 0.5549677610397339\n",
            "\tBatch 37 loss: 0.5899649858474731\n",
            "\tBatch 38 loss: 0.5700674057006836\n",
            "\tBatch 39 loss: 0.5571725368499756\n",
            "\tBatch 40 loss: 0.5606212615966797\n",
            "\tBatch 41 loss: 0.6596076488494873\n",
            "\tBatch 42 loss: 0.4906291961669922\n",
            "\tBatch 43 loss: 0.5448635220527649\n",
            "\tBatch 44 loss: 0.491798460483551\n",
            "\tBatch 45 loss: 0.5955815315246582\n",
            "\tBatch 46 loss: 0.6435540318489075\n",
            "\tBatch 47 loss: 0.5943084359169006\n",
            "\tBatch 48 loss: 0.4972259998321533\n",
            "\tBatch 49 loss: 0.5169678330421448\n",
            "\tBatch 50 loss: 0.7518613338470459\n",
            "\tBatch 51 loss: 0.5673317909240723\n",
            "\tBatch 52 loss: 0.5236411094665527\n",
            "\tBatch 53 loss: 0.7086421251296997\n",
            "\tBatch 54 loss: 0.5764052271842957\n",
            "\tBatch 55 loss: 0.5206444263458252\n",
            "\tBatch 56 loss: 0.515990138053894\n",
            "\tBatch 57 loss: 0.6221393942832947\n",
            "\tBatch 58 loss: 0.674197256565094\n",
            "\tBatch 59 loss: 0.6868033409118652\n",
            "\tBatch 60 loss: 0.6056641340255737\n",
            "\tBatch 61 loss: 0.4642094671726227\n",
            "\tBatch 62 loss: 0.5557786226272583\n",
            "\tBatch 63 loss: 0.5258752107620239\n",
            "\tBatch 64 loss: 0.6826492547988892\n",
            "\tBatch 65 loss: 0.6852652430534363\n",
            "\tBatch 66 loss: 0.6160920858383179\n",
            "\tBatch 67 loss: 0.6208993792533875\n",
            "\tBatch 68 loss: 0.49188947677612305\n",
            "\tBatch 69 loss: 0.6544862985610962\n",
            "\tBatch 70 loss: 0.48853451013565063\n",
            "\tBatch 71 loss: 0.49486708641052246\n",
            "\tBatch 72 loss: 0.5408859252929688\n",
            "\tBatch 73 loss: 0.7216370105743408\n",
            "\tBatch 74 loss: 0.5033615827560425\n",
            "\tBatch 75 loss: 0.47515368461608887\n",
            "\tBatch 76 loss: 0.5426648259162903\n",
            "\tBatch 77 loss: 0.7559412717819214\n",
            "\tBatch 78 loss: 0.5182178616523743\n",
            "\tBatch 79 loss: 0.5397194623947144\n",
            "\tBatch 80 loss: 0.45816344022750854\n",
            "\tBatch 81 loss: 0.5772755146026611\n",
            "\tBatch 82 loss: 0.5465729832649231\n",
            "\tBatch 83 loss: 0.49559879302978516\n",
            "\tBatch 84 loss: 0.5968266725540161\n",
            "\tBatch 85 loss: 0.48264169692993164\n",
            "\tBatch 86 loss: 0.5430262088775635\n",
            "\tBatch 87 loss: 0.4885840117931366\n",
            "\tBatch 88 loss: 0.5377106666564941\n",
            "\tBatch 89 loss: 0.5879930257797241\n",
            "\tBatch 90 loss: 0.5562782883644104\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8056741745914044\n",
            "        AUPRC: 0.7714658379554749\n",
            "        Sensitivity: 0.7245090007781982\n",
            "        Specificity: 0.7238566875457764\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7007913552214224\n",
            "        AUPRC: 0.660677433013916\n",
            "        Sensitivity: 0.5612422227859497\n",
            "        Specificity: 0.7291734218597412\n",
            "Training on data block 4!!!!\n",
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 0.40367230772972107\n",
            "\tBatch 1 loss: 0.45322081446647644\n",
            "\tBatch 2 loss: 0.3946413993835449\n",
            "\tBatch 3 loss: 0.4245184361934662\n",
            "\tBatch 4 loss: 0.4992142915725708\n",
            "\tBatch 5 loss: 0.4917042851448059\n",
            "\tBatch 6 loss: 0.41522419452667236\n",
            "\tBatch 7 loss: 0.3014538288116455\n",
            "\tBatch 8 loss: 0.4847525656223297\n",
            "\tBatch 9 loss: 0.3804823160171509\n",
            "\tBatch 10 loss: 0.41094526648521423\n",
            "\tBatch 11 loss: 0.710610032081604\n",
            "\tBatch 12 loss: 0.5785026550292969\n",
            "\tBatch 13 loss: 0.5762820243835449\n",
            "\tBatch 14 loss: 0.3551442325115204\n",
            "\tBatch 15 loss: 0.4369794428348541\n",
            "\tBatch 16 loss: 0.3452335298061371\n",
            "\tBatch 17 loss: 0.588202178478241\n",
            "\tBatch 18 loss: 0.603597104549408\n",
            "\tBatch 19 loss: 0.5901122093200684\n",
            "\tBatch 20 loss: 0.4487978219985962\n",
            "\tBatch 21 loss: 0.44443845748901367\n",
            "\tBatch 22 loss: 0.4639873504638672\n",
            "\tBatch 23 loss: 0.4443519115447998\n",
            "\tBatch 24 loss: 0.3812026381492615\n",
            "\tBatch 25 loss: 0.5345062613487244\n",
            "\tBatch 26 loss: 0.42291247844696045\n",
            "\tBatch 27 loss: 0.42101597785949707\n",
            "\tBatch 28 loss: 0.5485720038414001\n",
            "\tBatch 29 loss: 0.33180713653564453\n",
            "\tBatch 30 loss: 0.36056095361709595\n",
            "\tBatch 31 loss: 0.34549134969711304\n",
            "\tBatch 32 loss: 0.39130860567092896\n",
            "\tBatch 33 loss: 0.566913366317749\n",
            "\tBatch 34 loss: 0.3815435469150543\n",
            "\tBatch 35 loss: 0.2912375330924988\n",
            "\tBatch 36 loss: 0.45261770486831665\n",
            "\tBatch 37 loss: 0.6373948454856873\n",
            "\tBatch 38 loss: 0.3757140338420868\n",
            "\tBatch 39 loss: 0.45938602089881897\n",
            "\tBatch 40 loss: 0.32188814878463745\n",
            "\tBatch 41 loss: 0.39512449502944946\n",
            "\tBatch 42 loss: 0.3602465093135834\n",
            "\tBatch 43 loss: 0.4627654254436493\n",
            "\tBatch 44 loss: 0.45054078102111816\n",
            "\tBatch 45 loss: 0.3655211925506592\n",
            "\tBatch 46 loss: 0.4962416887283325\n",
            "\tBatch 47 loss: 0.47396692633628845\n",
            "\tBatch 48 loss: 0.5643590092658997\n",
            "\tBatch 49 loss: 0.4388566017150879\n",
            "\tBatch 50 loss: 0.4319413900375366\n",
            "\tBatch 51 loss: 0.2596965730190277\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9346264667305777\n",
            "        AUPRC: 0.9246489405632019\n",
            "        Sensitivity: 0.6713522672653198\n",
            "        Specificity: 0.9577362537384033\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7263229557179682\n",
            "        AUPRC: 0.691177248954773\n",
            "        Sensitivity: 0.18754196166992188\n",
            "        Specificity: 0.9296025037765503\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4116910696029663\n",
            "\tBatch 1 loss: 0.4352411925792694\n",
            "\tBatch 2 loss: 0.2990185022354126\n",
            "\tBatch 3 loss: 0.38580799102783203\n",
            "\tBatch 4 loss: 0.4470599889755249\n",
            "\tBatch 5 loss: 0.4556911885738373\n",
            "\tBatch 6 loss: 0.39838069677352905\n",
            "\tBatch 7 loss: 0.2562350630760193\n",
            "\tBatch 8 loss: 0.3904693126678467\n",
            "\tBatch 9 loss: 0.33346930146217346\n",
            "\tBatch 10 loss: 0.3870518207550049\n",
            "\tBatch 11 loss: 0.7468785047531128\n",
            "\tBatch 12 loss: 0.6414825320243835\n",
            "\tBatch 13 loss: 0.6716235876083374\n",
            "\tBatch 14 loss: 0.3240909278392792\n",
            "\tBatch 15 loss: 0.43226194381713867\n",
            "\tBatch 16 loss: 0.3114898204803467\n",
            "\tBatch 17 loss: 0.5070468783378601\n",
            "\tBatch 18 loss: 0.5299367308616638\n",
            "\tBatch 19 loss: 0.5653600096702576\n",
            "\tBatch 20 loss: 0.4359467625617981\n",
            "\tBatch 21 loss: 0.441078782081604\n",
            "\tBatch 22 loss: 0.4912106692790985\n",
            "\tBatch 23 loss: 0.4209168255329132\n",
            "\tBatch 24 loss: 0.3049950897693634\n",
            "\tBatch 25 loss: 0.49262264370918274\n",
            "\tBatch 26 loss: 0.37366968393325806\n",
            "\tBatch 27 loss: 0.3633621335029602\n",
            "\tBatch 28 loss: 0.5309927463531494\n",
            "\tBatch 29 loss: 0.2965020537376404\n",
            "\tBatch 30 loss: 0.3273095190525055\n",
            "\tBatch 31 loss: 0.3102421164512634\n",
            "\tBatch 32 loss: 0.3731774389743805\n",
            "\tBatch 33 loss: 0.5298348665237427\n",
            "\tBatch 34 loss: 0.32528433203697205\n",
            "\tBatch 35 loss: 0.2509108781814575\n",
            "\tBatch 36 loss: 0.44245970249176025\n",
            "\tBatch 37 loss: 0.6132567524909973\n",
            "\tBatch 38 loss: 0.3224635720252991\n",
            "\tBatch 39 loss: 0.430915892124176\n",
            "\tBatch 40 loss: 0.2870916426181793\n",
            "\tBatch 41 loss: 0.35747233033180237\n",
            "\tBatch 42 loss: 0.3390556573867798\n",
            "\tBatch 43 loss: 0.4334808588027954\n",
            "\tBatch 44 loss: 0.42050641775131226\n",
            "\tBatch 45 loss: 0.3354433476924896\n",
            "\tBatch 46 loss: 0.4671592712402344\n",
            "\tBatch 47 loss: 0.4527173638343811\n",
            "\tBatch 48 loss: 0.5577033758163452\n",
            "\tBatch 49 loss: 0.42119207978248596\n",
            "\tBatch 50 loss: 0.4342660903930664\n",
            "\tBatch 51 loss: 0.21913331747055054\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9390856347610447\n",
            "        AUPRC: 0.9324548244476318\n",
            "        Sensitivity: 0.6971229910850525\n",
            "        Specificity: 0.9611769318580627\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7376893851189157\n",
            "        AUPRC: 0.6998866200447083\n",
            "        Sensitivity: 0.22214797139167786\n",
            "        Specificity: 0.9238417744636536\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.39336004853248596\n",
            "\tBatch 1 loss: 0.40239855647087097\n",
            "\tBatch 2 loss: 0.27198871970176697\n",
            "\tBatch 3 loss: 0.36144566535949707\n",
            "\tBatch 4 loss: 0.4056708812713623\n",
            "\tBatch 5 loss: 0.40498170256614685\n",
            "\tBatch 6 loss: 0.3608262538909912\n",
            "\tBatch 7 loss: 0.21609723567962646\n",
            "\tBatch 8 loss: 0.3878442943096161\n",
            "\tBatch 9 loss: 0.2966969907283783\n",
            "\tBatch 10 loss: 0.3731309175491333\n",
            "\tBatch 11 loss: 0.7350324988365173\n",
            "\tBatch 12 loss: 0.6248810291290283\n",
            "\tBatch 13 loss: 0.6068280935287476\n",
            "\tBatch 14 loss: 0.2908910810947418\n",
            "\tBatch 15 loss: 0.42452165484428406\n",
            "\tBatch 16 loss: 0.27819597721099854\n",
            "\tBatch 17 loss: 0.5472596883773804\n",
            "\tBatch 18 loss: 0.5433035492897034\n",
            "\tBatch 19 loss: 0.560897171497345\n",
            "\tBatch 20 loss: 0.41699492931365967\n",
            "\tBatch 21 loss: 0.39407825469970703\n",
            "\tBatch 22 loss: 0.44948720932006836\n",
            "\tBatch 23 loss: 0.4116029143333435\n",
            "\tBatch 24 loss: 0.29906731843948364\n",
            "\tBatch 25 loss: 0.5158042907714844\n",
            "\tBatch 26 loss: 0.367084264755249\n",
            "\tBatch 27 loss: 0.3542332351207733\n",
            "\tBatch 28 loss: 0.5090153813362122\n",
            "\tBatch 29 loss: 0.25123903155326843\n",
            "\tBatch 30 loss: 0.3194010555744171\n",
            "\tBatch 31 loss: 0.2578580677509308\n",
            "\tBatch 32 loss: 0.3263457715511322\n",
            "\tBatch 33 loss: 0.5481070876121521\n",
            "\tBatch 34 loss: 0.31429433822631836\n",
            "\tBatch 35 loss: 0.21957501769065857\n",
            "\tBatch 36 loss: 0.4458664655685425\n",
            "\tBatch 37 loss: 0.5899733901023865\n",
            "\tBatch 38 loss: 0.2766156792640686\n",
            "\tBatch 39 loss: 0.40271222591400146\n",
            "\tBatch 40 loss: 0.2721874415874481\n",
            "\tBatch 41 loss: 0.3530098497867584\n",
            "\tBatch 42 loss: 0.32094335556030273\n",
            "\tBatch 43 loss: 0.4366067945957184\n",
            "\tBatch 44 loss: 0.39924517273902893\n",
            "\tBatch 45 loss: 0.3122915029525757\n",
            "\tBatch 46 loss: 0.43158042430877686\n",
            "\tBatch 47 loss: 0.42674124240875244\n",
            "\tBatch 48 loss: 0.5414078831672668\n",
            "\tBatch 49 loss: 0.3989872634410858\n",
            "\tBatch 50 loss: 0.4370178282260895\n",
            "\tBatch 51 loss: 0.20104444026947021\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9423106668986652\n",
            "        AUPRC: 0.9383112192153931\n",
            "        Sensitivity: 0.6842562556266785\n",
            "        Specificity: 0.9612858891487122\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7483750195324879\n",
            "        AUPRC: 0.7130550146102905\n",
            "        Sensitivity: 0.23686723411083221\n",
            "        Specificity: 0.9262489676475525\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.38259783387184143\n",
            "\tBatch 1 loss: 0.384120911359787\n",
            "\tBatch 2 loss: 0.2515064477920532\n",
            "\tBatch 3 loss: 0.3470189571380615\n",
            "\tBatch 4 loss: 0.36250218749046326\n",
            "\tBatch 5 loss: 0.36963582038879395\n",
            "\tBatch 6 loss: 0.3505096435546875\n",
            "\tBatch 7 loss: 0.19300879538059235\n",
            "\tBatch 8 loss: 0.3916202783584595\n",
            "\tBatch 9 loss: 0.2865602970123291\n",
            "\tBatch 10 loss: 0.3768623471260071\n",
            "\tBatch 11 loss: 0.7484635710716248\n",
            "\tBatch 12 loss: 0.6110543012619019\n",
            "\tBatch 13 loss: 0.5602797269821167\n",
            "\tBatch 14 loss: 0.26350417733192444\n",
            "\tBatch 15 loss: 0.42853689193725586\n",
            "\tBatch 16 loss: 0.2507706582546234\n",
            "\tBatch 17 loss: 0.5393133163452148\n",
            "\tBatch 18 loss: 0.5468733310699463\n",
            "\tBatch 19 loss: 0.5404372215270996\n",
            "\tBatch 20 loss: 0.40929168462753296\n",
            "\tBatch 21 loss: 0.36646798253059387\n",
            "\tBatch 22 loss: 0.4256058931350708\n",
            "\tBatch 23 loss: 0.40630409121513367\n",
            "\tBatch 24 loss: 0.2886395752429962\n",
            "\tBatch 25 loss: 0.5423407554626465\n",
            "\tBatch 26 loss: 0.3557056188583374\n",
            "\tBatch 27 loss: 0.3510250449180603\n",
            "\tBatch 28 loss: 0.4977506101131439\n",
            "\tBatch 29 loss: 0.23779740929603577\n",
            "\tBatch 30 loss: 0.31474363803863525\n",
            "\tBatch 31 loss: 0.2203330248594284\n",
            "\tBatch 32 loss: 0.3038921654224396\n",
            "\tBatch 33 loss: 0.5403814315795898\n",
            "\tBatch 34 loss: 0.30308863520622253\n",
            "\tBatch 35 loss: 0.2103477120399475\n",
            "\tBatch 36 loss: 0.4307304322719574\n",
            "\tBatch 37 loss: 0.5596116185188293\n",
            "\tBatch 38 loss: 0.25660809874534607\n",
            "\tBatch 39 loss: 0.3958388864994049\n",
            "\tBatch 40 loss: 0.2564951181411743\n",
            "\tBatch 41 loss: 0.3432658612728119\n",
            "\tBatch 42 loss: 0.3064967393875122\n",
            "\tBatch 43 loss: 0.42472559213638306\n",
            "\tBatch 44 loss: 0.38420259952545166\n",
            "\tBatch 45 loss: 0.3191676735877991\n",
            "\tBatch 46 loss: 0.43273091316223145\n",
            "\tBatch 47 loss: 0.4281684160232544\n",
            "\tBatch 48 loss: 0.5421452522277832\n",
            "\tBatch 49 loss: 0.3875020146369934\n",
            "\tBatch 50 loss: 0.43338334560394287\n",
            "\tBatch 51 loss: 0.17256398499011993\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9437346082648889\n",
            "        AUPRC: 0.9402593970298767\n",
            "        Sensitivity: 0.7052151560783386\n",
            "        Specificity: 0.9646167159080505\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7544710491816577\n",
            "        AUPRC: 0.7161049246788025\n",
            "        Sensitivity: 0.24963626265525818\n",
            "        Specificity: 0.921160101890564\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.36161482334136963\n",
            "\tBatch 1 loss: 0.3739256262779236\n",
            "\tBatch 2 loss: 0.2409573495388031\n",
            "\tBatch 3 loss: 0.34984830021858215\n",
            "\tBatch 4 loss: 0.3531593978404999\n",
            "\tBatch 5 loss: 0.36730876564979553\n",
            "\tBatch 6 loss: 0.33506104350090027\n",
            "\tBatch 7 loss: 0.18734553456306458\n",
            "\tBatch 8 loss: 0.3814525008201599\n",
            "\tBatch 9 loss: 0.26651883125305176\n",
            "\tBatch 10 loss: 0.36174237728118896\n",
            "\tBatch 11 loss: 0.7329443693161011\n",
            "\tBatch 12 loss: 0.5903040766716003\n",
            "\tBatch 13 loss: 0.5492869019508362\n",
            "\tBatch 14 loss: 0.2550235688686371\n",
            "\tBatch 15 loss: 0.42942455410957336\n",
            "\tBatch 16 loss: 0.23316901922225952\n",
            "\tBatch 17 loss: 0.5408165454864502\n",
            "\tBatch 18 loss: 0.5419360995292664\n",
            "\tBatch 19 loss: 0.5396925806999207\n",
            "\tBatch 20 loss: 0.40306687355041504\n",
            "\tBatch 21 loss: 0.3505496382713318\n",
            "\tBatch 22 loss: 0.4230194389820099\n",
            "\tBatch 23 loss: 0.4199739992618561\n",
            "\tBatch 24 loss: 0.2746751606464386\n",
            "\tBatch 25 loss: 0.5345508456230164\n",
            "\tBatch 26 loss: 0.34347087144851685\n",
            "\tBatch 27 loss: 0.3238673806190491\n",
            "\tBatch 28 loss: 0.5040032267570496\n",
            "\tBatch 29 loss: 0.21876201033592224\n",
            "\tBatch 30 loss: 0.30910125374794006\n",
            "\tBatch 31 loss: 0.20904961228370667\n",
            "\tBatch 32 loss: 0.29886046051979065\n",
            "\tBatch 33 loss: 0.5255391001701355\n",
            "\tBatch 34 loss: 0.2782283425331116\n",
            "\tBatch 35 loss: 0.20562918484210968\n",
            "\tBatch 36 loss: 0.4327201247215271\n",
            "\tBatch 37 loss: 0.5672425031661987\n",
            "\tBatch 38 loss: 0.24201203882694244\n",
            "\tBatch 39 loss: 0.3932608962059021\n",
            "\tBatch 40 loss: 0.2514899671077728\n",
            "\tBatch 41 loss: 0.3309614658355713\n",
            "\tBatch 42 loss: 0.3071172833442688\n",
            "\tBatch 43 loss: 0.4236859083175659\n",
            "\tBatch 44 loss: 0.37917810678482056\n",
            "\tBatch 45 loss: 0.3129481375217438\n",
            "\tBatch 46 loss: 0.42592519521713257\n",
            "\tBatch 47 loss: 0.42123448848724365\n",
            "\tBatch 48 loss: 0.5392717719078064\n",
            "\tBatch 49 loss: 0.3732686936855316\n",
            "\tBatch 50 loss: 0.4165663719177246\n",
            "\tBatch 51 loss: 0.16119424998760223\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9456002835832217\n",
            "        AUPRC: 0.9420995712280273\n",
            "        Sensitivity: 0.7165452241897583\n",
            "        Specificity: 0.9623542428016663\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7600780244302139\n",
            "        AUPRC: 0.721509575843811\n",
            "        Sensitivity: 0.2743202745914459\n",
            "        Specificity: 0.9213647246360779\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3462468981742859\n",
            "\tBatch 1 loss: 0.35559746623039246\n",
            "\tBatch 2 loss: 0.24365583062171936\n",
            "\tBatch 3 loss: 0.33031439781188965\n",
            "\tBatch 4 loss: 0.34927669167518616\n",
            "\tBatch 5 loss: 0.35734057426452637\n",
            "\tBatch 6 loss: 0.34872859716415405\n",
            "\tBatch 7 loss: 0.17501096427440643\n",
            "\tBatch 8 loss: 0.3678959608078003\n",
            "\tBatch 9 loss: 0.2514458894729614\n",
            "\tBatch 10 loss: 0.35841524600982666\n",
            "\tBatch 11 loss: 0.7390400171279907\n",
            "\tBatch 12 loss: 0.5948282480239868\n",
            "\tBatch 13 loss: 0.5501245260238647\n",
            "\tBatch 14 loss: 0.2564373016357422\n",
            "\tBatch 15 loss: 0.42501533031463623\n",
            "\tBatch 16 loss: 0.23418165743350983\n",
            "\tBatch 17 loss: 0.508739709854126\n",
            "\tBatch 18 loss: 0.5156511664390564\n",
            "\tBatch 19 loss: 0.5240131616592407\n",
            "\tBatch 20 loss: 0.39286738634109497\n",
            "\tBatch 21 loss: 0.34904617071151733\n",
            "\tBatch 22 loss: 0.4330574870109558\n",
            "\tBatch 23 loss: 0.4095711410045624\n",
            "\tBatch 24 loss: 0.2591187357902527\n",
            "\tBatch 25 loss: 0.5154932737350464\n",
            "\tBatch 26 loss: 0.3312041759490967\n",
            "\tBatch 27 loss: 0.31303805112838745\n",
            "\tBatch 28 loss: 0.5044483542442322\n",
            "\tBatch 29 loss: 0.22093963623046875\n",
            "\tBatch 30 loss: 0.3121716380119324\n",
            "\tBatch 31 loss: 0.20488306879997253\n",
            "\tBatch 32 loss: 0.2904776334762573\n",
            "\tBatch 33 loss: 0.49721449613571167\n",
            "\tBatch 34 loss: 0.260196328163147\n",
            "\tBatch 35 loss: 0.19579893350601196\n",
            "\tBatch 36 loss: 0.4297063946723938\n",
            "\tBatch 37 loss: 0.5502525568008423\n",
            "\tBatch 38 loss: 0.23909209668636322\n",
            "\tBatch 39 loss: 0.4046352803707123\n",
            "\tBatch 40 loss: 0.23300611972808838\n",
            "\tBatch 41 loss: 0.30317622423171997\n",
            "\tBatch 42 loss: 0.307848185300827\n",
            "\tBatch 43 loss: 0.3862321376800537\n",
            "\tBatch 44 loss: 0.36132699251174927\n",
            "\tBatch 45 loss: 0.3075011074542999\n",
            "\tBatch 46 loss: 0.4121870696544647\n",
            "\tBatch 47 loss: 0.42585426568984985\n",
            "\tBatch 48 loss: 0.5444610118865967\n",
            "\tBatch 49 loss: 0.3598911166191101\n",
            "\tBatch 50 loss: 0.4013460576534271\n",
            "\tBatch 51 loss: 0.15849944949150085\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9458555289937068\n",
            "        AUPRC: 0.9434384703636169\n",
            "        Sensitivity: 0.7262356281280518\n",
            "        Specificity: 0.9612858891487122\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7656537978261488\n",
            "        AUPRC: 0.7265700101852417\n",
            "        Sensitivity: 0.29234933853149414\n",
            "        Specificity: 0.9179176688194275\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3277600407600403\n",
            "\tBatch 1 loss: 0.3524203896522522\n",
            "\tBatch 2 loss: 0.22854962944984436\n",
            "\tBatch 3 loss: 0.3251517117023468\n",
            "\tBatch 4 loss: 0.34691470861434937\n",
            "\tBatch 5 loss: 0.367353618144989\n",
            "\tBatch 6 loss: 0.3585638403892517\n",
            "\tBatch 7 loss: 0.17597168684005737\n",
            "\tBatch 8 loss: 0.35785117745399475\n",
            "\tBatch 9 loss: 0.2556387782096863\n",
            "\tBatch 10 loss: 0.36048954725265503\n",
            "\tBatch 11 loss: 0.7264218926429749\n",
            "\tBatch 12 loss: 0.6019713878631592\n",
            "\tBatch 13 loss: 0.5612413287162781\n",
            "\tBatch 14 loss: 0.2531494200229645\n",
            "\tBatch 15 loss: 0.4334757626056671\n",
            "\tBatch 16 loss: 0.22314798831939697\n",
            "\tBatch 17 loss: 0.4897720217704773\n",
            "\tBatch 18 loss: 0.49293962121009827\n",
            "\tBatch 19 loss: 0.5116357803344727\n",
            "\tBatch 20 loss: 0.39666077494621277\n",
            "\tBatch 21 loss: 0.348239004611969\n",
            "\tBatch 22 loss: 0.43974435329437256\n",
            "\tBatch 23 loss: 0.4013763666152954\n",
            "\tBatch 24 loss: 0.23314012587070465\n",
            "\tBatch 25 loss: 0.5016010999679565\n",
            "\tBatch 26 loss: 0.3088347315788269\n",
            "\tBatch 27 loss: 0.31380027532577515\n",
            "\tBatch 28 loss: 0.49330875277519226\n",
            "\tBatch 29 loss: 0.2200235277414322\n",
            "\tBatch 30 loss: 0.3039448857307434\n",
            "\tBatch 31 loss: 0.20036715269088745\n",
            "\tBatch 32 loss: 0.2826434075832367\n",
            "\tBatch 33 loss: 0.4810006618499756\n",
            "\tBatch 34 loss: 0.25257712602615356\n",
            "\tBatch 35 loss: 0.18903356790542603\n",
            "\tBatch 36 loss: 0.43234121799468994\n",
            "\tBatch 37 loss: 0.5721552968025208\n",
            "\tBatch 38 loss: 0.2345336526632309\n",
            "\tBatch 39 loss: 0.4083971381187439\n",
            "\tBatch 40 loss: 0.21947941184043884\n",
            "\tBatch 41 loss: 0.28324371576309204\n",
            "\tBatch 42 loss: 0.3179115653038025\n",
            "\tBatch 43 loss: 0.37779831886291504\n",
            "\tBatch 44 loss: 0.3539913296699524\n",
            "\tBatch 45 loss: 0.30072686076164246\n",
            "\tBatch 46 loss: 0.4284060597419739\n",
            "\tBatch 47 loss: 0.4083379805088043\n",
            "\tBatch 48 loss: 0.5405793190002441\n",
            "\tBatch 49 loss: 0.3569715619087219\n",
            "\tBatch 50 loss: 0.3906913101673126\n",
            "\tBatch 51 loss: 0.14303770661354065\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9465534275188998\n",
            "        AUPRC: 0.9445041418075562\n",
            "        Sensitivity: 0.7461176514625549\n",
            "        Specificity: 0.9589337110519409\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7690489925886321\n",
            "        AUPRC: 0.729580819606781\n",
            "        Sensitivity: 0.31013578176498413\n",
            "        Specificity: 0.917787492275238\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3168250620365143\n",
            "\tBatch 1 loss: 0.33323776721954346\n",
            "\tBatch 2 loss: 0.23060891032218933\n",
            "\tBatch 3 loss: 0.3227447271347046\n",
            "\tBatch 4 loss: 0.35624879598617554\n",
            "\tBatch 5 loss: 0.3753998279571533\n",
            "\tBatch 6 loss: 0.3703381419181824\n",
            "\tBatch 7 loss: 0.17398220300674438\n",
            "\tBatch 8 loss: 0.35015857219696045\n",
            "\tBatch 9 loss: 0.23399768769741058\n",
            "\tBatch 10 loss: 0.3510594666004181\n",
            "\tBatch 11 loss: 0.7344774007797241\n",
            "\tBatch 12 loss: 0.6134182214736938\n",
            "\tBatch 13 loss: 0.5590747594833374\n",
            "\tBatch 14 loss: 0.2464228719472885\n",
            "\tBatch 15 loss: 0.4362504482269287\n",
            "\tBatch 16 loss: 0.22175706923007965\n",
            "\tBatch 17 loss: 0.47655028104782104\n",
            "\tBatch 18 loss: 0.4989708960056305\n",
            "\tBatch 19 loss: 0.5092760324478149\n",
            "\tBatch 20 loss: 0.3913601040840149\n",
            "\tBatch 21 loss: 0.3474135100841522\n",
            "\tBatch 22 loss: 0.45572927594184875\n",
            "\tBatch 23 loss: 0.40260058641433716\n",
            "\tBatch 24 loss: 0.22432702779769897\n",
            "\tBatch 25 loss: 0.500198483467102\n",
            "\tBatch 26 loss: 0.3123634159564972\n",
            "\tBatch 27 loss: 0.30494073033332825\n",
            "\tBatch 28 loss: 0.49982693791389465\n",
            "\tBatch 29 loss: 0.22323882579803467\n",
            "\tBatch 30 loss: 0.30378642678260803\n",
            "\tBatch 31 loss: 0.19853653013706207\n",
            "\tBatch 32 loss: 0.28346580266952515\n",
            "\tBatch 33 loss: 0.46586257219314575\n",
            "\tBatch 34 loss: 0.2478208690881729\n",
            "\tBatch 35 loss: 0.18798112869262695\n",
            "\tBatch 36 loss: 0.42608776688575745\n",
            "\tBatch 37 loss: 0.5668214559555054\n",
            "\tBatch 38 loss: 0.23249176144599915\n",
            "\tBatch 39 loss: 0.4061658978462219\n",
            "\tBatch 40 loss: 0.22696062922477722\n",
            "\tBatch 41 loss: 0.2705094814300537\n",
            "\tBatch 42 loss: 0.3102180063724518\n",
            "\tBatch 43 loss: 0.3646838665008545\n",
            "\tBatch 44 loss: 0.3437682092189789\n",
            "\tBatch 45 loss: 0.3156522214412689\n",
            "\tBatch 46 loss: 0.42630451917648315\n",
            "\tBatch 47 loss: 0.41155582666397095\n",
            "\tBatch 48 loss: 0.5547096729278564\n",
            "\tBatch 49 loss: 0.3544369637966156\n",
            "\tBatch 50 loss: 0.3786061406135559\n",
            "\tBatch 51 loss: 0.13439972698688507\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9480812870726327\n",
            "        AUPRC: 0.9466791152954102\n",
            "        Sensitivity: 0.747795820236206\n",
            "        Specificity: 0.9568531513214111\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7730399060443056\n",
            "        AUPRC: 0.7341983318328857\n",
            "        Sensitivity: 0.33412036299705505\n",
            "        Specificity: 0.9102634191513062\n",
            "====================================\n",
            "     Epoch #9\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3154098391532898\n",
            "\tBatch 1 loss: 0.33207055926322937\n",
            "\tBatch 2 loss: 0.22521629929542542\n",
            "\tBatch 3 loss: 0.32684606313705444\n",
            "\tBatch 4 loss: 0.35677772760391235\n",
            "\tBatch 5 loss: 0.3845987915992737\n",
            "\tBatch 6 loss: 0.37674665451049805\n",
            "\tBatch 7 loss: 0.1828414499759674\n",
            "\tBatch 8 loss: 0.34284645318984985\n",
            "\tBatch 9 loss: 0.2457730770111084\n",
            "\tBatch 10 loss: 0.34632834792137146\n",
            "\tBatch 11 loss: 0.7108175754547119\n",
            "\tBatch 12 loss: 0.6147964000701904\n",
            "\tBatch 13 loss: 0.5585039854049683\n",
            "\tBatch 14 loss: 0.24212884902954102\n",
            "\tBatch 15 loss: 0.4506310522556305\n",
            "\tBatch 16 loss: 0.22258609533309937\n",
            "\tBatch 17 loss: 0.4448034167289734\n",
            "\tBatch 18 loss: 0.4690438508987427\n",
            "\tBatch 19 loss: 0.48671793937683105\n",
            "\tBatch 20 loss: 0.3853185772895813\n",
            "\tBatch 21 loss: 0.33958470821380615\n",
            "\tBatch 22 loss: 0.45802754163742065\n",
            "\tBatch 23 loss: 0.4027360677719116\n",
            "\tBatch 24 loss: 0.20808829367160797\n",
            "\tBatch 25 loss: 0.4893760681152344\n",
            "\tBatch 26 loss: 0.29681798815727234\n",
            "\tBatch 27 loss: 0.3010402023792267\n",
            "\tBatch 28 loss: 0.4978426992893219\n",
            "\tBatch 29 loss: 0.21070927381515503\n",
            "\tBatch 30 loss: 0.306512713432312\n",
            "\tBatch 31 loss: 0.19998224079608917\n",
            "\tBatch 32 loss: 0.28053411841392517\n",
            "\tBatch 33 loss: 0.45360592007637024\n",
            "\tBatch 34 loss: 0.24020998179912567\n",
            "\tBatch 35 loss: 0.18785330653190613\n",
            "\tBatch 36 loss: 0.4169561564922333\n",
            "\tBatch 37 loss: 0.5526149272918701\n",
            "\tBatch 38 loss: 0.22797365486621857\n",
            "\tBatch 39 loss: 0.424121618270874\n",
            "\tBatch 40 loss: 0.22079595923423767\n",
            "\tBatch 41 loss: 0.2627474367618561\n",
            "\tBatch 42 loss: 0.3072124123573303\n",
            "\tBatch 43 loss: 0.34691205620765686\n",
            "\tBatch 44 loss: 0.335175484418869\n",
            "\tBatch 45 loss: 0.32083791494369507\n",
            "\tBatch 46 loss: 0.4135656952857971\n",
            "\tBatch 47 loss: 0.3928321301937103\n",
            "\tBatch 48 loss: 0.5552069544792175\n",
            "\tBatch 49 loss: 0.3566455543041229\n",
            "\tBatch 50 loss: 0.3752277195453644\n",
            "\tBatch 51 loss: 0.13306660950183868\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9489014493311392\n",
            "        AUPRC: 0.9469690918922424\n",
            "        Sensitivity: 0.7468830943107605\n",
            "        Specificity: 0.9589337110519409\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7749890351585784\n",
            "        AUPRC: 0.7369741201400757\n",
            "        Sensitivity: 0.34610164165496826\n",
            "        Specificity: 0.9064099788665771\n",
            "====================================\n",
            "     Epoch #10\n",
            "====================================\n",
            "\tBatch 0 loss: 0.29709282517433167\n",
            "\tBatch 1 loss: 0.3215499520301819\n",
            "\tBatch 2 loss: 0.22682327032089233\n",
            "\tBatch 3 loss: 0.3177357017993927\n",
            "\tBatch 4 loss: 0.3590850234031677\n",
            "\tBatch 5 loss: 0.3962496221065521\n",
            "\tBatch 6 loss: 0.38189420104026794\n",
            "\tBatch 7 loss: 0.17906072735786438\n",
            "\tBatch 8 loss: 0.3379368782043457\n",
            "\tBatch 9 loss: 0.23760981857776642\n",
            "\tBatch 10 loss: 0.3505513668060303\n",
            "\tBatch 11 loss: 0.7289677858352661\n",
            "\tBatch 12 loss: 0.6087662577629089\n",
            "\tBatch 13 loss: 0.5577383041381836\n",
            "\tBatch 14 loss: 0.2558004558086395\n",
            "\tBatch 15 loss: 0.4345375597476959\n",
            "\tBatch 16 loss: 0.22848789393901825\n",
            "\tBatch 17 loss: 0.4391189217567444\n",
            "\tBatch 18 loss: 0.4584347605705261\n",
            "\tBatch 19 loss: 0.48773321509361267\n",
            "\tBatch 20 loss: 0.3883236050605774\n",
            "\tBatch 21 loss: 0.342337429523468\n",
            "\tBatch 22 loss: 0.4534623622894287\n",
            "\tBatch 23 loss: 0.4074707329273224\n",
            "\tBatch 24 loss: 0.21133045852184296\n",
            "\tBatch 25 loss: 0.4802449345588684\n",
            "\tBatch 26 loss: 0.274063378572464\n",
            "\tBatch 27 loss: 0.29056525230407715\n",
            "\tBatch 28 loss: 0.4959847927093506\n",
            "\tBatch 29 loss: 0.21612083911895752\n",
            "\tBatch 30 loss: 0.309955358505249\n",
            "\tBatch 31 loss: 0.20746946334838867\n",
            "\tBatch 32 loss: 0.2940627932548523\n",
            "\tBatch 33 loss: 0.4326810836791992\n",
            "\tBatch 34 loss: 0.23110336065292358\n",
            "\tBatch 35 loss: 0.18424327671527863\n",
            "\tBatch 36 loss: 0.4263455271720886\n",
            "\tBatch 37 loss: 0.5773998498916626\n",
            "\tBatch 38 loss: 0.23549190163612366\n",
            "\tBatch 39 loss: 0.4345698952674866\n",
            "\tBatch 40 loss: 0.2169562429189682\n",
            "\tBatch 41 loss: 0.25399792194366455\n",
            "\tBatch 42 loss: 0.3241563141345978\n",
            "\tBatch 43 loss: 0.33321741223335266\n",
            "\tBatch 44 loss: 0.3262454867362976\n",
            "\tBatch 45 loss: 0.32319822907447815\n",
            "\tBatch 46 loss: 0.40937578678131104\n",
            "\tBatch 47 loss: 0.39655354619026184\n",
            "\tBatch 48 loss: 0.5629472136497498\n",
            "\tBatch 49 loss: 0.34968358278274536\n",
            "\tBatch 50 loss: 0.3484594225883484\n",
            "\tBatch 51 loss: 0.12609639763832092\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9493492943461703\n",
            "        AUPRC: 0.9482482075691223\n",
            "        Sensitivity: 0.7516499757766724\n",
            "        Specificity: 0.9609805345535278\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7788859273241259\n",
            "        AUPRC: 0.7387621402740479\n",
            "        Sensitivity: 0.3632870316505432\n",
            "        Specificity: 0.8967216610908508\n",
            "====================================\n",
            "     Epoch #11\n",
            "====================================\n",
            "\tBatch 0 loss: 0.291362464427948\n",
            "\tBatch 1 loss: 0.31115561723709106\n",
            "\tBatch 2 loss: 0.2203085869550705\n",
            "\tBatch 3 loss: 0.32116806507110596\n",
            "\tBatch 4 loss: 0.36685603857040405\n",
            "\tBatch 5 loss: 0.40643495321273804\n",
            "\tBatch 6 loss: 0.39624160528182983\n",
            "\tBatch 7 loss: 0.1874721497297287\n",
            "\tBatch 8 loss: 0.3232646584510803\n",
            "\tBatch 9 loss: 0.24501273036003113\n",
            "\tBatch 10 loss: 0.3328729569911957\n",
            "\tBatch 11 loss: 0.7141258120536804\n",
            "\tBatch 12 loss: 0.6001794338226318\n",
            "\tBatch 13 loss: 0.5749137997627258\n",
            "\tBatch 14 loss: 0.25515133142471313\n",
            "\tBatch 15 loss: 0.44725871086120605\n",
            "\tBatch 16 loss: 0.22580713033676147\n",
            "\tBatch 17 loss: 0.41829997301101685\n",
            "\tBatch 18 loss: 0.4315369129180908\n",
            "\tBatch 19 loss: 0.4688049554824829\n",
            "\tBatch 20 loss: 0.3792986273765564\n",
            "\tBatch 21 loss: 0.33320143818855286\n",
            "\tBatch 22 loss: 0.46495455503463745\n",
            "\tBatch 23 loss: 0.3915535509586334\n",
            "\tBatch 24 loss: 0.19592128694057465\n",
            "\tBatch 25 loss: 0.4754069149494171\n",
            "\tBatch 26 loss: 0.27224019169807434\n",
            "\tBatch 27 loss: 0.2750324308872223\n",
            "\tBatch 28 loss: 0.4798918068408966\n",
            "\tBatch 29 loss: 0.2206435352563858\n",
            "\tBatch 30 loss: 0.30367764830589294\n",
            "\tBatch 31 loss: 0.21102847158908844\n",
            "\tBatch 32 loss: 0.2993234097957611\n",
            "\tBatch 33 loss: 0.4034895598888397\n",
            "\tBatch 34 loss: 0.2141728401184082\n",
            "\tBatch 35 loss: 0.18184781074523926\n",
            "\tBatch 36 loss: 0.41246625781059265\n",
            "\tBatch 37 loss: 0.5916369557380676\n",
            "\tBatch 38 loss: 0.22611293196678162\n",
            "\tBatch 39 loss: 0.44516223669052124\n",
            "\tBatch 40 loss: 0.20939688384532928\n",
            "\tBatch 41 loss: 0.24644851684570312\n",
            "\tBatch 42 loss: 0.3175833225250244\n",
            "\tBatch 43 loss: 0.32794544100761414\n",
            "\tBatch 44 loss: 0.3243320882320404\n",
            "\tBatch 45 loss: 0.3222876191139221\n",
            "\tBatch 46 loss: 0.41364943981170654\n",
            "\tBatch 47 loss: 0.39087238907814026\n",
            "\tBatch 48 loss: 0.5797972679138184\n",
            "\tBatch 49 loss: 0.34999680519104004\n",
            "\tBatch 50 loss: 0.3513660430908203\n",
            "\tBatch 51 loss: 0.12868598103523254\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9498037077771867\n",
            "        AUPRC: 0.9486429691314697\n",
            "        Sensitivity: 0.7572681903839111\n",
            "        Specificity: 0.9567195177078247\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7816562198768651\n",
            "        AUPRC: 0.7417105436325073\n",
            "        Sensitivity: 0.38484787940979004\n",
            "        Specificity: 0.8888553977012634\n",
            "Training on data block 5!!!!\n",
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 1.22762930393219\n",
            "\tBatch 1 loss: 0.5470199584960938\n",
            "\tBatch 2 loss: 0.5439566373825073\n",
            "\tBatch 3 loss: 0.6515227556228638\n",
            "\tBatch 4 loss: 0.6916677951812744\n",
            "\tBatch 5 loss: 0.33358773589134216\n",
            "\tBatch 6 loss: 0.5535213947296143\n",
            "\tBatch 7 loss: 0.560150146484375\n",
            "\tBatch 8 loss: 0.38014331459999084\n",
            "\tBatch 9 loss: 0.5505157113075256\n",
            "\tBatch 10 loss: 0.4047965705394745\n",
            "\tBatch 11 loss: 0.5087064504623413\n",
            "\tBatch 12 loss: 0.5216934680938721\n",
            "\tBatch 13 loss: 0.5438559055328369\n",
            "\tBatch 14 loss: 0.783020555973053\n",
            "\tBatch 15 loss: 0.4876062572002411\n",
            "\tBatch 16 loss: 0.7261543273925781\n",
            "\tBatch 17 loss: 0.5000051856040955\n",
            "\tBatch 18 loss: 0.6815803647041321\n",
            "\tBatch 19 loss: 0.5114006996154785\n",
            "\tBatch 20 loss: 0.453714519739151\n",
            "\tBatch 21 loss: 0.5130021572113037\n",
            "\tBatch 22 loss: 0.6749585270881653\n",
            "\tBatch 23 loss: 0.8792997598648071\n",
            "\tBatch 24 loss: 0.5816142559051514\n",
            "\tBatch 25 loss: 1.0702615976333618\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8341394934911381\n",
            "        AUPRC: 0.8092250823974609\n",
            "        Sensitivity: 0.7598219513893127\n",
            "        Specificity: 0.7399853467941284\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7764033020324048\n",
            "        AUPRC: 0.7272759079933167\n",
            "        Sensitivity: 0.8516064882278442\n",
            "        Specificity: 0.5795608758926392\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.8317335844039917\n",
            "\tBatch 1 loss: 0.6240330934524536\n",
            "\tBatch 2 loss: 0.617226243019104\n",
            "\tBatch 3 loss: 0.6122713088989258\n",
            "\tBatch 4 loss: 0.6448790431022644\n",
            "\tBatch 5 loss: 0.35097628831863403\n",
            "\tBatch 6 loss: 0.5569251775741577\n",
            "\tBatch 7 loss: 0.5354208946228027\n",
            "\tBatch 8 loss: 0.3705065846443176\n",
            "\tBatch 9 loss: 0.5354152321815491\n",
            "\tBatch 10 loss: 0.4108838140964508\n",
            "\tBatch 11 loss: 0.4853588342666626\n",
            "\tBatch 12 loss: 0.533572256565094\n",
            "\tBatch 13 loss: 0.5397536754608154\n",
            "\tBatch 14 loss: 0.7848888635635376\n",
            "\tBatch 15 loss: 0.5040445923805237\n",
            "\tBatch 16 loss: 0.7549313902854919\n",
            "\tBatch 17 loss: 0.43707185983657837\n",
            "\tBatch 18 loss: 0.5612835884094238\n",
            "\tBatch 19 loss: 0.5019956827163696\n",
            "\tBatch 20 loss: 0.46977248787879944\n",
            "\tBatch 21 loss: 0.4830029606819153\n",
            "\tBatch 22 loss: 0.6227002739906311\n",
            "\tBatch 23 loss: 0.6417419910430908\n",
            "\tBatch 24 loss: 0.5387945771217346\n",
            "\tBatch 25 loss: 0.9247124195098877\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8334867176092795\n",
            "        AUPRC: 0.8081256151199341\n",
            "        Sensitivity: 0.841363787651062\n",
            "        Specificity: 0.6615457534790039\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7757459697028539\n",
            "        AUPRC: 0.7258092164993286\n",
            "        Sensitivity: 0.8851093649864197\n",
            "        Specificity: 0.5184943675994873\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.776133120059967\n",
            "\tBatch 1 loss: 0.5985036492347717\n",
            "\tBatch 2 loss: 0.5702275037765503\n",
            "\tBatch 3 loss: 0.5863686203956604\n",
            "\tBatch 4 loss: 0.5944786071777344\n",
            "\tBatch 5 loss: 0.3311096429824829\n",
            "\tBatch 6 loss: 0.5284051895141602\n",
            "\tBatch 7 loss: 0.5125229358673096\n",
            "\tBatch 8 loss: 0.38181036710739136\n",
            "\tBatch 9 loss: 0.5330356955528259\n",
            "\tBatch 10 loss: 0.40766045451164246\n",
            "\tBatch 11 loss: 0.4839138388633728\n",
            "\tBatch 12 loss: 0.5046534538269043\n",
            "\tBatch 13 loss: 0.5260515809059143\n",
            "\tBatch 14 loss: 0.7432072162628174\n",
            "\tBatch 15 loss: 0.49287843704223633\n",
            "\tBatch 16 loss: 0.6979324221611023\n",
            "\tBatch 17 loss: 0.4461793303489685\n",
            "\tBatch 18 loss: 0.559360682964325\n",
            "\tBatch 19 loss: 0.5070580244064331\n",
            "\tBatch 20 loss: 0.46694594621658325\n",
            "\tBatch 21 loss: 0.47122031450271606\n",
            "\tBatch 22 loss: 0.6117271184921265\n",
            "\tBatch 23 loss: 0.6364008188247681\n",
            "\tBatch 24 loss: 0.5339736938476562\n",
            "\tBatch 25 loss: 0.8973267078399658\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8378884985639435\n",
            "        AUPRC: 0.8166335821151733\n",
            "        Sensitivity: 0.823815643787384\n",
            "        Specificity: 0.6792410612106323\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7758098605847981\n",
            "        AUPRC: 0.7274192571640015\n",
            "        Sensitivity: 0.8742139339447021\n",
            "        Specificity: 0.5293351411819458\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.7384357452392578\n",
            "\tBatch 1 loss: 0.5886090993881226\n",
            "\tBatch 2 loss: 0.5674137473106384\n",
            "\tBatch 3 loss: 0.5774078965187073\n",
            "\tBatch 4 loss: 0.584917426109314\n",
            "\tBatch 5 loss: 0.3479216694831848\n",
            "\tBatch 6 loss: 0.5166677236557007\n",
            "\tBatch 7 loss: 0.5110775828361511\n",
            "\tBatch 8 loss: 0.38906246423721313\n",
            "\tBatch 9 loss: 0.525998055934906\n",
            "\tBatch 10 loss: 0.4143821597099304\n",
            "\tBatch 11 loss: 0.471252977848053\n",
            "\tBatch 12 loss: 0.49739667773246765\n",
            "\tBatch 13 loss: 0.5208848714828491\n",
            "\tBatch 14 loss: 0.7370824813842773\n",
            "\tBatch 15 loss: 0.48829594254493713\n",
            "\tBatch 16 loss: 0.6988972425460815\n",
            "\tBatch 17 loss: 0.45492085814476013\n",
            "\tBatch 18 loss: 0.5606415271759033\n",
            "\tBatch 19 loss: 0.505984365940094\n",
            "\tBatch 20 loss: 0.46016743779182434\n",
            "\tBatch 21 loss: 0.4743824601173401\n",
            "\tBatch 22 loss: 0.6035414934158325\n",
            "\tBatch 23 loss: 0.6464725136756897\n",
            "\tBatch 24 loss: 0.5249780416488647\n",
            "\tBatch 25 loss: 0.8658594489097595\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8385775790255816\n",
            "        AUPRC: 0.8184391856193542\n",
            "        Sensitivity: 0.8484127521514893\n",
            "        Specificity: 0.6682770848274231\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7757269770871161\n",
            "        AUPRC: 0.728725254535675\n",
            "        Sensitivity: 0.8719382882118225\n",
            "        Specificity: 0.5277317762374878\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.7362313270568848\n",
            "\tBatch 1 loss: 0.5823671221733093\n",
            "\tBatch 2 loss: 0.5499677658081055\n",
            "\tBatch 3 loss: 0.5642428398132324\n",
            "\tBatch 4 loss: 0.5887881517410278\n",
            "\tBatch 5 loss: 0.3633613586425781\n",
            "\tBatch 6 loss: 0.5307438373565674\n",
            "\tBatch 7 loss: 0.5040161609649658\n",
            "\tBatch 8 loss: 0.39301469922065735\n",
            "\tBatch 9 loss: 0.5258834362030029\n",
            "\tBatch 10 loss: 0.40876466035842896\n",
            "\tBatch 11 loss: 0.4778350591659546\n",
            "\tBatch 12 loss: 0.5068285465240479\n",
            "\tBatch 13 loss: 0.5237109661102295\n",
            "\tBatch 14 loss: 0.7287212610244751\n",
            "\tBatch 15 loss: 0.49824053049087524\n",
            "\tBatch 16 loss: 0.6977248787879944\n",
            "\tBatch 17 loss: 0.4498100280761719\n",
            "\tBatch 18 loss: 0.5418812036514282\n",
            "\tBatch 19 loss: 0.509385347366333\n",
            "\tBatch 20 loss: 0.4685611128807068\n",
            "\tBatch 21 loss: 0.474236398935318\n",
            "\tBatch 22 loss: 0.598308265209198\n",
            "\tBatch 23 loss: 0.6302082538604736\n",
            "\tBatch 24 loss: 0.5173432230949402\n",
            "\tBatch 25 loss: 0.8497214317321777\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8407263194250264\n",
            "        AUPRC: 0.8224664330482483\n",
            "        Sensitivity: 0.8512693643569946\n",
            "        Specificity: 0.658635675907135\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7745355875806705\n",
            "        AUPRC: 0.7259857058525085\n",
            "        Sensitivity: 0.8786744475364685\n",
            "        Specificity: 0.5198613405227661\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.715026319026947\n",
            "\tBatch 1 loss: 0.5935810804367065\n",
            "\tBatch 2 loss: 0.5638090968132019\n",
            "\tBatch 3 loss: 0.5614675879478455\n",
            "\tBatch 4 loss: 0.5753974318504333\n",
            "\tBatch 5 loss: 0.37105661630630493\n",
            "\tBatch 6 loss: 0.5348473191261292\n",
            "\tBatch 7 loss: 0.4977003335952759\n",
            "\tBatch 8 loss: 0.40526431798934937\n",
            "\tBatch 9 loss: 0.5219818353652954\n",
            "\tBatch 10 loss: 0.4162329435348511\n",
            "\tBatch 11 loss: 0.47500357031822205\n",
            "\tBatch 12 loss: 0.5022662878036499\n",
            "\tBatch 13 loss: 0.5157426595687866\n",
            "\tBatch 14 loss: 0.726540744304657\n",
            "\tBatch 15 loss: 0.48627692461013794\n",
            "\tBatch 16 loss: 0.6911240816116333\n",
            "\tBatch 17 loss: 0.4461890459060669\n",
            "\tBatch 18 loss: 0.5473312735557556\n",
            "\tBatch 19 loss: 0.5036737322807312\n",
            "\tBatch 20 loss: 0.4618406295776367\n",
            "\tBatch 21 loss: 0.4639453887939453\n",
            "\tBatch 22 loss: 0.5863831639289856\n",
            "\tBatch 23 loss: 0.6162833571434021\n",
            "\tBatch 24 loss: 0.5137981176376343\n",
            "\tBatch 25 loss: 0.816911518573761\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8403660159677386\n",
            "        AUPRC: 0.8224888443946838\n",
            "        Sensitivity: 0.8614977598190308\n",
            "        Specificity: 0.6539536714553833\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7729985282716834\n",
            "        AUPRC: 0.7265868186950684\n",
            "        Sensitivity: 0.8720672130584717\n",
            "        Specificity: 0.5232247114181519\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.696044921875\n",
            "\tBatch 1 loss: 0.5717789530754089\n",
            "\tBatch 2 loss: 0.5599027276039124\n",
            "\tBatch 3 loss: 0.5569816827774048\n",
            "\tBatch 4 loss: 0.5726080536842346\n",
            "\tBatch 5 loss: 0.37892651557922363\n",
            "\tBatch 6 loss: 0.5305277109146118\n",
            "\tBatch 7 loss: 0.4927269518375397\n",
            "\tBatch 8 loss: 0.39862117171287537\n",
            "\tBatch 9 loss: 0.5178465843200684\n",
            "\tBatch 10 loss: 0.41755497455596924\n",
            "\tBatch 11 loss: 0.48151522874832153\n",
            "\tBatch 12 loss: 0.5035761594772339\n",
            "\tBatch 13 loss: 0.5052205324172974\n",
            "\tBatch 14 loss: 0.7173100709915161\n",
            "\tBatch 15 loss: 0.4918006956577301\n",
            "\tBatch 16 loss: 0.6774167418479919\n",
            "\tBatch 17 loss: 0.45498374104499817\n",
            "\tBatch 18 loss: 0.5529446601867676\n",
            "\tBatch 19 loss: 0.5163919925689697\n",
            "\tBatch 20 loss: 0.46651050448417664\n",
            "\tBatch 21 loss: 0.480068564414978\n",
            "\tBatch 22 loss: 0.5913573503494263\n",
            "\tBatch 23 loss: 0.6189810037612915\n",
            "\tBatch 24 loss: 0.518342137336731\n",
            "\tBatch 25 loss: 0.8209848999977112\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8431467821923364\n",
            "        AUPRC: 0.8276709914207458\n",
            "        Sensitivity: 0.8570954203605652\n",
            "        Specificity: 0.6611464023590088\n",
            "    Test data metrics:\n",
            "        AUROC: 0.7702185033516007\n",
            "        AUPRC: 0.7232979536056519\n",
            "        Sensitivity: 0.8705286979675293\n",
            "        Specificity: 0.5259760618209839\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6884810328483582\n",
            "\tBatch 1 loss: 0.5725469589233398\n",
            "\tBatch 2 loss: 0.5506420135498047\n",
            "\tBatch 3 loss: 0.5499711632728577\n",
            "\tBatch 4 loss: 0.5791711211204529\n",
            "\tBatch 5 loss: 0.38383764028549194\n",
            "\tBatch 6 loss: 0.5286151766777039\n",
            "\tBatch 7 loss: 0.4864962697029114\n",
            "\tBatch 8 loss: 0.3992026150226593\n",
            "\tBatch 9 loss: 0.5232313871383667\n",
            "\tBatch 10 loss: 0.4116007089614868\n",
            "\tBatch 11 loss: 0.47948044538497925\n",
            "\tBatch 12 loss: 0.5077049136161804\n",
            "\tBatch 13 loss: 0.5124435424804688\n",
            "\tBatch 14 loss: 0.7218135595321655\n",
            "\tBatch 15 loss: 0.49569031596183777\n",
            "\tBatch 16 loss: 0.6932381987571716\n",
            "\tBatch 17 loss: 0.4511336088180542\n",
            "\tBatch 18 loss: 0.5480841398239136\n",
            "\tBatch 19 loss: 0.5142063498497009\n",
            "\tBatch 20 loss: 0.46677425503730774\n",
            "\tBatch 21 loss: 0.46334365010261536\n",
            "\tBatch 22 loss: 0.5729120969772339\n",
            "\tBatch 23 loss: 0.6146536469459534\n",
            "\tBatch 24 loss: 0.5123268961906433\n",
            "\tBatch 25 loss: 0.7911560535430908\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-86a16cf56ad0>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training on data block {idx}!!!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-57fe25927359>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_handle, test_data_handle, learning_rate, epochs, suspend_train_epochs_threshold, batch_size)\u001b[0m\n\u001b[1;32m     95\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     )\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Not using performance metrics yet in this function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Potential TODO: stop training once desired performance is reached (TBD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-2e12bf2a6296>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, eval_data, dataset_name, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4d7d3dbc5e1a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, abp, ecg, eeg)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m       \u001b[0mabp_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meeg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4d7d3dbc5e1a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m       \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-4d7d3dbc5e1a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2482\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get exact number of negative samples to match positive ones\n",
        "# abp_neg = abp_neg[0:167]\n",
        "\n",
        "# # Concat and shuffle together\n",
        "# all_abp = torch.concat([abp_neg, abp_pos])\n",
        "# all_labels = torch.concat([torch.zeros([abp_neg.size()[0]]), torch.ones([abp_pos.size()[0]])])\n",
        "# shuffled = shuffle_tensors([all_abp, all_labels])\n",
        "\n",
        "# train_set = [\n",
        "#     shuffled[0],\n",
        "#     torch.Tensor([]),\n",
        "#     torch.Tensor([]),\n",
        "#     shuffled[1]\n",
        "# ]\n",
        "\n",
        "# test_set = [\n",
        "#     shuffled[0].detach().clone(),\n",
        "#     torch.Tensor([]),\n",
        "#     torch.Tensor([]),\n",
        "#     shuffled[1].detach().clone()\n",
        "# ]"
      ],
      "metadata": {
        "id": "CRX9zlCMzOL9",
        "outputId": "63735263-95b5-4aa5-ed54-939a0d0f0511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'abp_neg' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-130a677314db>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get exact number of negative samples to match positive ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mabp_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabp_neg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m167\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Concat and shuffle together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_abp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabp_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabp_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'abp_neg' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain ABP data to train on\n",
        "#test_set = get_data(3, max_num_samples=1000)\n",
        "\n",
        "# Get cases with good data\n",
        "_download_vital_file('819')\n",
        "_download_vital_file('3704')\n",
        "\n",
        "all_data = get_data(3, from_dir='.')"
      ],
      "metadata": {
        "id": "--44au29-IRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34d2394-3d12-436a-fd30-766f99ec06e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 3704\n",
            "Statistics for case: 3704, 1028 total valid samples, 94 positive samples\n",
            "Getting track data for case: 819\n",
            "Statistics for case: 819, 737 total valid samples, 73 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:194: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  abps = FloatTensor(abps)\n",
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a good balanced dataset\n",
        "\n",
        "pos_indices = torch.nonzero(all_data[3] > 0.5).squeeze(-1)\n",
        "neg_indices = torch.nonzero(all_data[3] < 0.5).squeeze(-1)\n",
        "abp_pos = all_data[0].detach().clone()[pos_indices]\n",
        "abp_neg = all_data[0].detach().clone()[neg_indices]\n",
        "\n",
        "# Get exact number of negative samples to match positive ones\n",
        "abp_neg = abp_neg[0:167]\n",
        "\n",
        "# Concat and shuffle together\n",
        "all_abp = torch.concat([abp_neg, abp_pos])\n",
        "all_labels = torch.concat([torch.zeros([abp_neg.size()[0]]), torch.ones([abp_pos.size()[0]])])\n",
        "shuffled = shuffle_tensors([all_abp, all_labels])\n",
        "\n",
        "train_set = [\n",
        "    shuffled[0],\n",
        "    torch.Tensor([]),\n",
        "    torch.Tensor([]),\n",
        "    shuffled[1]\n",
        "]\n",
        "\n",
        "test_set = [\n",
        "    shuffled[0].detach().clone(),\n",
        "    torch.Tensor([]),\n",
        "    torch.Tensor([]),\n",
        "    shuffled[1].detach().clone()\n",
        "]\n"
      ],
      "metadata": {
        "id": "MYl8aTvL2PdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on ABP data\n",
        "\n",
        "# train_set = [\n",
        "#     all_data[0][0:4000].unsqueeze(1),\n",
        "#     all_data[1][0:4000].unsqueeze(1),\n",
        "#     all_data[2][0:4000].unsqueeze(1),\n",
        "#     all_data[3][0:4000]\n",
        "# ]\n",
        "\n",
        "# test_set = [\n",
        "#     all_data[0][4000:].unsqueeze(1),\n",
        "#     all_data[1][4000:].unsqueeze(1),\n",
        "#     all_data[2][4000:].unsqueeze(1),\n",
        "#     all_data[3][4000:]\n",
        "# ]\n",
        "\n",
        "# train(abp_model, train_set, test_set, batch_size=40, epochs=100, learning_rate=0.0001)\n",
        "\n",
        "# TRAIN ON ONLY CASE 819\n",
        "\n",
        "# new_train_set = (\n",
        "#     train_set[0].detach().clone(),\n",
        "#     train_set[1].detach().clone(),\n",
        "#     train_set[2].detach().clone(),\n",
        "#     torch.ones(train_set[3].size(), dtype=torch.float)\n",
        "# )\n",
        "\n",
        "# new_test_set = (\n",
        "#     train_set[0].detach().clone(),\n",
        "#     train_set[1].detach().clone(),\n",
        "#     train_set[2].detach().clone(),\n",
        "#     torch.ones(train_set[3].size(), dtype=torch.float)\n",
        "# )\n",
        "\n",
        "train(abp_model, train_set, test_set, batch_size=40, epochs=100)\n",
        "\n",
        "# TEST WITH RANDOM DATA\n",
        "# sample_size = 400\n",
        "# train_set_r = [\n",
        "#     torch.randn([sample_size, 1, 30000]),\n",
        "#     torch.randn([sample_size, 1, 30000]),\n",
        "#     torch.randn([sample_size, 1, 30000]),\n",
        "#     torch.where(torch.rand([sample_size]) > 0.5, 1.0, 0.0),\n",
        "# ]\n",
        "\n",
        "# train(abp_model, train_set_r, train_set_r, batch_size=40, epochs=3)\n"
      ],
      "metadata": {
        "id": "Uurpy92P9TXg",
        "outputId": "76cf63dd-ba70-42d3-b505-7e635d3ac2f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================\n",
            "     Epoch #1\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6892008185386658\n",
            "\tBatch 1 loss: 0.7042883038520813\n",
            "\tBatch 2 loss: 0.6887048482894897\n",
            "\tBatch 3 loss: 0.6910030245780945\n",
            "\tBatch 4 loss: 0.6846761703491211\n",
            "\tBatch 5 loss: 0.6989827752113342\n",
            "\tBatch 6 loss: 0.6940107345581055\n",
            "\tBatch 7 loss: 0.697780966758728\n",
            "\tBatch 8 loss: 0.6829289197921753\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 16\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 21\n",
            "y_hat_long sum: 40, target_long sum: 24\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 40, target_long sum: 19\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 14, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9068835883725715\n",
            "        AUPRC: 0.9036766886711121\n",
            "        Sensitivity: 1.0\n",
            "        Specificity: 0.0\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 16\n",
            "y_hat_long sum: 40, target_long sum: 22\n",
            "y_hat_long sum: 40, target_long sum: 21\n",
            "y_hat_long sum: 40, target_long sum: 24\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 40, target_long sum: 19\n",
            "y_hat_long sum: 40, target_long sum: 17\n",
            "y_hat_long sum: 14, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9084024251466533\n",
            "        AUPRC: 0.9060680866241455\n",
            "        Sensitivity: 1.0\n",
            "        Specificity: 0.0\n",
            "====================================\n",
            "     Epoch #2\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6869204640388489\n",
            "\tBatch 1 loss: 0.6975041627883911\n",
            "\tBatch 2 loss: 0.6867660880088806\n",
            "\tBatch 3 loss: 0.6886695027351379\n",
            "\tBatch 4 loss: 0.6862188577651978\n",
            "\tBatch 5 loss: 0.6922420263290405\n",
            "\tBatch 6 loss: 0.689597487449646\n",
            "\tBatch 7 loss: 0.6912027597427368\n",
            "\tBatch 8 loss: 0.6874047517776489\n",
            "y_hat_long sum: 35, target_long sum: 22\n",
            "y_hat_long sum: 34, target_long sum: 16\n",
            "y_hat_long sum: 38, target_long sum: 22\n",
            "y_hat_long sum: 36, target_long sum: 21\n",
            "y_hat_long sum: 36, target_long sum: 24\n",
            "y_hat_long sum: 36, target_long sum: 17\n",
            "y_hat_long sum: 36, target_long sum: 19\n",
            "y_hat_long sum: 35, target_long sum: 17\n",
            "y_hat_long sum: 13, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9146188580408359\n",
            "        AUPRC: 0.9116359353065491\n",
            "        Sensitivity: 0.984935998916626\n",
            "        Specificity: 0.17522545158863068\n",
            "y_hat_long sum: 36, target_long sum: 22\n",
            "y_hat_long sum: 33, target_long sum: 16\n",
            "y_hat_long sum: 38, target_long sum: 22\n",
            "y_hat_long sum: 37, target_long sum: 21\n",
            "y_hat_long sum: 37, target_long sum: 24\n",
            "y_hat_long sum: 35, target_long sum: 17\n",
            "y_hat_long sum: 36, target_long sum: 19\n",
            "y_hat_long sum: 36, target_long sum: 17\n",
            "y_hat_long sum: 13, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9119220383212456\n",
            "        AUPRC: 0.9160993099212646\n",
            "        Sensitivity: 0.984935998916626\n",
            "        Specificity: 0.16088983416557312\n",
            "====================================\n",
            "     Epoch #3\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6850563883781433\n",
            "\tBatch 1 loss: 0.6895695924758911\n",
            "\tBatch 2 loss: 0.6853010654449463\n",
            "\tBatch 3 loss: 0.6856827735900879\n",
            "\tBatch 4 loss: 0.688294529914856\n",
            "\tBatch 5 loss: 0.684249997138977\n",
            "\tBatch 6 loss: 0.6844044327735901\n",
            "\tBatch 7 loss: 0.6831811666488647\n",
            "\tBatch 8 loss: 0.6936203837394714\n",
            "y_hat_long sum: 15, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 9, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 21\n",
            "y_hat_long sum: 17, target_long sum: 24\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 11, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.914706176847865\n",
            "        AUPRC: 0.9165877103805542\n",
            "        Sensitivity: 0.6110977530479431\n",
            "        Specificity: 0.9234481453895569\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 8, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 21\n",
            "y_hat_long sum: 17, target_long sum: 24\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 19\n",
            "y_hat_long sum: 11, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9130499879447491\n",
            "        AUPRC: 0.9178506135940552\n",
            "        Sensitivity: 0.5832011699676514\n",
            "        Specificity: 0.9234481453895569\n",
            "====================================\n",
            "     Epoch #4\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6829558610916138\n",
            "\tBatch 1 loss: 0.6807898283004761\n",
            "\tBatch 2 loss: 0.683302640914917\n",
            "\tBatch 3 loss: 0.682981014251709\n",
            "\tBatch 4 loss: 0.6918870210647583\n",
            "\tBatch 5 loss: 0.6753025054931641\n",
            "\tBatch 6 loss: 0.6792897582054138\n",
            "\tBatch 7 loss: 0.6750096082687378\n",
            "\tBatch 8 loss: 0.7010712623596191\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9121559389770737\n",
            "        AUPRC: 0.9117672443389893\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9141807043692212\n",
            "        AUPRC: 0.9192715883255005\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #5\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6816198825836182\n",
            "\tBatch 1 loss: 0.671542227268219\n",
            "\tBatch 2 loss: 0.6819656491279602\n",
            "\tBatch 3 loss: 0.6805471181869507\n",
            "\tBatch 4 loss: 0.6955975294113159\n",
            "\tBatch 5 loss: 0.6670814752578735\n",
            "\tBatch 6 loss: 0.6745513677597046\n",
            "\tBatch 7 loss: 0.6669188737869263\n",
            "\tBatch 8 loss: 0.7087539434432983\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9151169204373849\n",
            "        AUPRC: 0.919556736946106\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.913498847896808\n",
            "        AUPRC: 0.9182300567626953\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #6\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6805179715156555\n",
            "\tBatch 1 loss: 0.6637954115867615\n",
            "\tBatch 2 loss: 0.6807087659835815\n",
            "\tBatch 3 loss: 0.6787312626838684\n",
            "\tBatch 4 loss: 0.6993579864501953\n",
            "\tBatch 5 loss: 0.6601194739341736\n",
            "\tBatch 6 loss: 0.6711148619651794\n",
            "\tBatch 7 loss: 0.660396933555603\n",
            "\tBatch 8 loss: 0.7158055305480957\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9158318365203783\n",
            "        AUPRC: 0.9196781516075134\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9125363991579238\n",
            "        AUPRC: 0.9177239537239075\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #7\n",
            "====================================\n",
            "\tBatch 0 loss: 0.679352879524231\n",
            "\tBatch 1 loss: 0.658550500869751\n",
            "\tBatch 2 loss: 0.6799470782279968\n",
            "\tBatch 3 loss: 0.6764718294143677\n",
            "\tBatch 4 loss: 0.7023816108703613\n",
            "\tBatch 5 loss: 0.6544641256332397\n",
            "\tBatch 6 loss: 0.6684986352920532\n",
            "\tBatch 7 loss: 0.65559983253479\n",
            "\tBatch 8 loss: 0.7181524634361267\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9137409679360537\n",
            "        AUPRC: 0.918603777885437\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9137770403610728\n",
            "        AUPRC: 0.9169428944587708\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #8\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6785613894462585\n",
            "\tBatch 1 loss: 0.6542757153511047\n",
            "\tBatch 2 loss: 0.677804172039032\n",
            "\tBatch 3 loss: 0.6745716333389282\n",
            "\tBatch 4 loss: 0.7036718726158142\n",
            "\tBatch 5 loss: 0.6512762308120728\n",
            "\tBatch 6 loss: 0.6665207147598267\n",
            "\tBatch 7 loss: 0.6510282754898071\n",
            "\tBatch 8 loss: 0.7187858819961548\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.914417685208369\n",
            "        AUPRC: 0.9208349585533142\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9128976986388897\n",
            "        AUPRC: 0.9209595918655396\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #9\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6773391366004944\n",
            "\tBatch 1 loss: 0.6524175405502319\n",
            "\tBatch 2 loss: 0.6764297485351562\n",
            "\tBatch 3 loss: 0.6725694537162781\n",
            "\tBatch 4 loss: 0.7027281522750854\n",
            "\tBatch 5 loss: 0.6483203768730164\n",
            "\tBatch 6 loss: 0.6648508310317993\n",
            "\tBatch 7 loss: 0.6481978297233582\n",
            "\tBatch 8 loss: 0.7167852520942688\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9142161130020442\n",
            "        AUPRC: 0.9227725267410278\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9159147711451077\n",
            "        AUPRC: 0.9241769313812256\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #10\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6744926571846008\n",
            "\tBatch 1 loss: 0.65020352602005\n",
            "\tBatch 2 loss: 0.6734442114830017\n",
            "\tBatch 3 loss: 0.6699119210243225\n",
            "\tBatch 4 loss: 0.7023297548294067\n",
            "\tBatch 5 loss: 0.6456762552261353\n",
            "\tBatch 6 loss: 0.6623522639274597\n",
            "\tBatch 7 loss: 0.6446765065193176\n",
            "\tBatch 8 loss: 0.7173344492912292\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9155457717968444\n",
            "        AUPRC: 0.9226319789886475\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9140720881000186\n",
            "        AUPRC: 0.9222228527069092\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #11\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6722265481948853\n",
            "\tBatch 1 loss: 0.6484382748603821\n",
            "\tBatch 2 loss: 0.6703135967254639\n",
            "\tBatch 3 loss: 0.6670130491256714\n",
            "\tBatch 4 loss: 0.7007306218147278\n",
            "\tBatch 5 loss: 0.6420892477035522\n",
            "\tBatch 6 loss: 0.6602762937545776\n",
            "\tBatch 7 loss: 0.6416604518890381\n",
            "\tBatch 8 loss: 0.715448796749115\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9147042660408164\n",
            "        AUPRC: 0.9239981770515442\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9133784312831218\n",
            "        AUPRC: 0.9213923215866089\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #12\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6702406406402588\n",
            "\tBatch 1 loss: 0.6452990174293518\n",
            "\tBatch 2 loss: 0.6670089364051819\n",
            "\tBatch 3 loss: 0.6637771725654602\n",
            "\tBatch 4 loss: 0.6979485750198364\n",
            "\tBatch 5 loss: 0.6389375925064087\n",
            "\tBatch 6 loss: 0.6578430533409119\n",
            "\tBatch 7 loss: 0.6369155049324036\n",
            "\tBatch 8 loss: 0.7131065726280212\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9145103661882721\n",
            "        AUPRC: 0.920425295829773\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9103260071371899\n",
            "        AUPRC: 0.9209523797035217\n",
            "        Sensitivity: 0.0\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #13\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6671538352966309\n",
            "\tBatch 1 loss: 0.6429203748703003\n",
            "\tBatch 2 loss: 0.6627871990203857\n",
            "\tBatch 3 loss: 0.6598578095436096\n",
            "\tBatch 4 loss: 0.6972920894622803\n",
            "\tBatch 5 loss: 0.633895993232727\n",
            "\tBatch 6 loss: 0.654870867729187\n",
            "\tBatch 7 loss: 0.6311982274055481\n",
            "\tBatch 8 loss: 0.7101749777793884\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9104366484368379\n",
            "        AUPRC: 0.9194555282592773\n",
            "        Sensitivity: 0.0030673397704958916\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 0, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 21\n",
            "y_hat_long sum: 0, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9120152979695652\n",
            "        AUPRC: 0.92055743932724\n",
            "        Sensitivity: 0.0030673397704958916\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #14\n",
            "====================================\n",
            "\tBatch 0 loss: 0.662881076335907\n",
            "\tBatch 1 loss: 0.639514148235321\n",
            "\tBatch 2 loss: 0.6594415903091431\n",
            "\tBatch 3 loss: 0.6560066342353821\n",
            "\tBatch 4 loss: 0.6943228840827942\n",
            "\tBatch 5 loss: 0.6285778284072876\n",
            "\tBatch 6 loss: 0.6510604619979858\n",
            "\tBatch 7 loss: 0.6250197291374207\n",
            "\tBatch 8 loss: 0.7063249945640564\n",
            "y_hat_long sum: 1, target_long sum: 22\n",
            "y_hat_long sum: 1, target_long sum: 16\n",
            "y_hat_long sum: 1, target_long sum: 22\n",
            "y_hat_long sum: 1, target_long sum: 21\n",
            "y_hat_long sum: 1, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 0, target_long sum: 19\n",
            "y_hat_long sum: 0, target_long sum: 17\n",
            "y_hat_long sum: 1, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.909001982577037\n",
            "        AUPRC: 0.9180669784545898\n",
            "        Sensitivity: 0.04630136117339134\n",
            "        Specificity: 1.0\n",
            "y_hat_long sum: 1, target_long sum: 22\n",
            "y_hat_long sum: 0, target_long sum: 16\n",
            "y_hat_long sum: 2, target_long sum: 22\n",
            "y_hat_long sum: 1, target_long sum: 21\n",
            "y_hat_long sum: 1, target_long sum: 24\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 1, target_long sum: 19\n",
            "y_hat_long sum: 1, target_long sum: 17\n",
            "y_hat_long sum: 1, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9120617249046163\n",
            "        AUPRC: 0.9225659966468811\n",
            "        Sensitivity: 0.046902552247047424\n",
            "        Specificity: 1.0\n",
            "====================================\n",
            "     Epoch #15\n",
            "====================================\n",
            "\tBatch 0 loss: 0.657463788986206\n",
            "\tBatch 1 loss: 0.6365253329277039\n",
            "\tBatch 2 loss: 0.6516498923301697\n",
            "\tBatch 3 loss: 0.6496671438217163\n",
            "\tBatch 4 loss: 0.691287636756897\n",
            "\tBatch 5 loss: 0.6214285492897034\n",
            "\tBatch 6 loss: 0.6463429927825928\n",
            "\tBatch 7 loss: 0.6186938285827637\n",
            "\tBatch 8 loss: 0.7003628611564636\n",
            "y_hat_long sum: 3, target_long sum: 22\n",
            "y_hat_long sum: 4, target_long sum: 16\n",
            "y_hat_long sum: 4, target_long sum: 22\n",
            "y_hat_long sum: 2, target_long sum: 21\n",
            "y_hat_long sum: 3, target_long sum: 24\n",
            "y_hat_long sum: 5, target_long sum: 17\n",
            "y_hat_long sum: 3, target_long sum: 19\n",
            "y_hat_long sum: 3, target_long sum: 17\n",
            "y_hat_long sum: 3, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9057255969914273\n",
            "        AUPRC: 0.9164742231369019\n",
            "        Sensitivity: 0.1636136770248413\n",
            "        Specificity: 0.9930555820465088\n",
            "y_hat_long sum: 3, target_long sum: 22\n",
            "y_hat_long sum: 3, target_long sum: 16\n",
            "y_hat_long sum: 4, target_long sum: 22\n",
            "y_hat_long sum: 3, target_long sum: 21\n",
            "y_hat_long sum: 4, target_long sum: 24\n",
            "y_hat_long sum: 5, target_long sum: 17\n",
            "y_hat_long sum: 2, target_long sum: 19\n",
            "y_hat_long sum: 5, target_long sum: 17\n",
            "y_hat_long sum: 3, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.907437916187036\n",
            "        AUPRC: 0.91530841588974\n",
            "        Sensitivity: 0.1635439395904541\n",
            "        Specificity: 0.9930555820465088\n",
            "====================================\n",
            "     Epoch #16\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6520121693611145\n",
            "\tBatch 1 loss: 0.6297735571861267\n",
            "\tBatch 2 loss: 0.6448286771774292\n",
            "\tBatch 3 loss: 0.6432048678398132\n",
            "\tBatch 4 loss: 0.6862486600875854\n",
            "\tBatch 5 loss: 0.6146045923233032\n",
            "\tBatch 6 loss: 0.642187774181366\n",
            "\tBatch 7 loss: 0.6073592305183411\n",
            "\tBatch 8 loss: 0.6975404024124146\n",
            "y_hat_long sum: 7, target_long sum: 22\n",
            "y_hat_long sum: 9, target_long sum: 16\n",
            "y_hat_long sum: 7, target_long sum: 22\n",
            "y_hat_long sum: 5, target_long sum: 21\n",
            "y_hat_long sum: 6, target_long sum: 24\n",
            "y_hat_long sum: 9, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 19\n",
            "y_hat_long sum: 9, target_long sum: 17\n",
            "y_hat_long sum: 4, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9064030583907068\n",
            "        AUPRC: 0.9148079752922058\n",
            "        Sensitivity: 0.3506799340248108\n",
            "        Specificity: 0.9861111044883728\n",
            "y_hat_long sum: 8, target_long sum: 22\n",
            "y_hat_long sum: 9, target_long sum: 16\n",
            "y_hat_long sum: 7, target_long sum: 22\n",
            "y_hat_long sum: 5, target_long sum: 21\n",
            "y_hat_long sum: 11, target_long sum: 24\n",
            "y_hat_long sum: 8, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 19\n",
            "y_hat_long sum: 9, target_long sum: 17\n",
            "y_hat_long sum: 4, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9054873441816186\n",
            "        AUPRC: 0.9133103489875793\n",
            "        Sensitivity: 0.38356393575668335\n",
            "        Specificity: 0.9861111044883728\n",
            "====================================\n",
            "     Epoch #17\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6449339389801025\n",
            "\tBatch 1 loss: 0.6251242160797119\n",
            "\tBatch 2 loss: 0.6351718306541443\n",
            "\tBatch 3 loss: 0.6345715522766113\n",
            "\tBatch 4 loss: 0.6811243891716003\n",
            "\tBatch 5 loss: 0.6064152717590332\n",
            "\tBatch 6 loss: 0.6332703232765198\n",
            "\tBatch 7 loss: 0.5971869230270386\n",
            "\tBatch 8 loss: 0.6914825439453125\n",
            "y_hat_long sum: 10, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 8, target_long sum: 22\n",
            "y_hat_long sum: 10, target_long sum: 21\n",
            "y_hat_long sum: 14, target_long sum: 24\n",
            "y_hat_long sum: 11, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 19\n",
            "y_hat_long sum: 10, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9079264219146551\n",
            "        AUPRC: 0.9156043529510498\n",
            "        Sensitivity: 0.49272775650024414\n",
            "        Specificity: 0.9463623762130737\n",
            "y_hat_long sum: 10, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 9, target_long sum: 22\n",
            "y_hat_long sum: 9, target_long sum: 21\n",
            "y_hat_long sum: 13, target_long sum: 24\n",
            "y_hat_long sum: 10, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 19\n",
            "y_hat_long sum: 10, target_long sum: 17\n",
            "y_hat_long sum: 5, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9036228943583592\n",
            "        AUPRC: 0.9119967222213745\n",
            "        Sensitivity: 0.4893557131290436\n",
            "        Specificity: 0.9533068537712097\n",
            "====================================\n",
            "     Epoch #18\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6358113288879395\n",
            "\tBatch 1 loss: 0.6193191409111023\n",
            "\tBatch 2 loss: 0.6239252686500549\n",
            "\tBatch 3 loss: 0.6201116442680359\n",
            "\tBatch 4 loss: 0.6790622472763062\n",
            "\tBatch 5 loss: 0.5962127447128296\n",
            "\tBatch 6 loss: 0.6256857514381409\n",
            "\tBatch 7 loss: 0.5815078020095825\n",
            "\tBatch 8 loss: 0.6803013682365417\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 12, target_long sum: 16\n",
            "y_hat_long sum: 11, target_long sum: 22\n",
            "y_hat_long sum: 10, target_long sum: 21\n",
            "y_hat_long sum: 18, target_long sum: 24\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9055317932246313\n",
            "        AUPRC: 0.916564404964447\n",
            "        Sensitivity: 0.5964581370353699\n",
            "        Specificity: 0.9341269731521606\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 11, target_long sum: 16\n",
            "y_hat_long sum: 11, target_long sum: 22\n",
            "y_hat_long sum: 10, target_long sum: 21\n",
            "y_hat_long sum: 19, target_long sum: 24\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 6, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9023378882092268\n",
            "        AUPRC: 0.9100298285484314\n",
            "        Sensitivity: 0.5903105139732361\n",
            "        Specificity: 0.9341269731521606\n",
            "====================================\n",
            "     Epoch #19\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6220640540122986\n",
            "\tBatch 1 loss: 0.6124768257141113\n",
            "\tBatch 2 loss: 0.6112731099128723\n",
            "\tBatch 3 loss: 0.6101856827735901\n",
            "\tBatch 4 loss: 0.6660743355751038\n",
            "\tBatch 5 loss: 0.5836362838745117\n",
            "\tBatch 6 loss: 0.6151502728462219\n",
            "\tBatch 7 loss: 0.5657191872596741\n",
            "\tBatch 8 loss: 0.6653093695640564\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 16\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 19, target_long sum: 24\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 12, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 7, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.90227949998207\n",
            "        AUPRC: 0.9132506251335144\n",
            "        Sensitivity: 0.6342509388923645\n",
            "        Specificity: 0.8857938051223755\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 14, target_long sum: 16\n",
            "y_hat_long sum: 13, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 13, target_long sum: 19\n",
            "y_hat_long sum: 13, target_long sum: 17\n",
            "y_hat_long sum: 7, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.8988305840619388\n",
            "        AUPRC: 0.9081506729125977\n",
            "        Sensitivity: 0.6373183131217957\n",
            "        Specificity: 0.868727445602417\n",
            "====================================\n",
            "     Epoch #20\n",
            "====================================\n",
            "\tBatch 0 loss: 0.6118425130844116\n",
            "\tBatch 1 loss: 0.6049607396125793\n",
            "\tBatch 2 loss: 0.591064453125\n",
            "\tBatch 3 loss: 0.59397292137146\n",
            "\tBatch 4 loss: 0.6609207987785339\n",
            "\tBatch 5 loss: 0.5649071931838989\n",
            "\tBatch 6 loss: 0.6034107804298401\n",
            "\tBatch 7 loss: 0.5433727502822876\n",
            "\tBatch 8 loss: 0.6561498641967773\n",
            "y_hat_long sum: 15, target_long sum: 22\n",
            "y_hat_long sum: 15, target_long sum: 16\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 15, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 8, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9020147318954375\n",
            "        AUPRC: 0.9098455905914307\n",
            "        Sensitivity: 0.6612875461578369\n",
            "        Specificity: 0.8577236533164978\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 13, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 15, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 8, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9025140054438228\n",
            "        AUPRC: 0.9109455943107605\n",
            "        Sensitivity: 0.686884343624115\n",
            "        Specificity: 0.8484643697738647\n",
            "====================================\n",
            "     Epoch #21\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5953096151351929\n",
            "\tBatch 1 loss: 0.5934098362922668\n",
            "\tBatch 2 loss: 0.5744529962539673\n",
            "\tBatch 3 loss: 0.5775696039199829\n",
            "\tBatch 4 loss: 0.6523662805557251\n",
            "\tBatch 5 loss: 0.5451484322547913\n",
            "\tBatch 6 loss: 0.5920466184616089\n",
            "\tBatch 7 loss: 0.5220590829849243\n",
            "\tBatch 8 loss: 0.6437917351722717\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 16, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 16, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 8, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8994549509687132\n",
            "        AUPRC: 0.9083201289176941\n",
            "        Sensitivity: 0.7393754720687866\n",
            "        Specificity: 0.8255500197410583\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 14, target_long sum: 22\n",
            "y_hat_long sum: 16, target_long sum: 21\n",
            "y_hat_long sum: 19, target_long sum: 24\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 17, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9002835431016211\n",
            "        AUPRC: 0.9113142490386963\n",
            "        Sensitivity: 0.6985442638397217\n",
            "        Specificity: 0.8341479301452637\n",
            "====================================\n",
            "     Epoch #22\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5775318741798401\n",
            "\tBatch 1 loss: 0.5729418992996216\n",
            "\tBatch 2 loss: 0.5543146133422852\n",
            "\tBatch 3 loss: 0.5552261471748352\n",
            "\tBatch 4 loss: 0.6446357369422913\n",
            "\tBatch 5 loss: 0.527985692024231\n",
            "\tBatch 6 loss: 0.578209638595581\n",
            "\tBatch 7 loss: 0.49590954184532166\n",
            "\tBatch 8 loss: 0.6254758834838867\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 16, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 17, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8995672395118522\n",
            "        AUPRC: 0.9081029891967773\n",
            "        Sensitivity: 0.739317774772644\n",
            "        Specificity: 0.8202590346336365\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 17, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.8977549728615387\n",
            "        AUPRC: 0.9066032767295837\n",
            "        Sensitivity: 0.7765670418739319\n",
            "        Specificity: 0.8202590346336365\n",
            "====================================\n",
            "     Epoch #23\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5586413145065308\n",
            "\tBatch 1 loss: 0.553929328918457\n",
            "\tBatch 2 loss: 0.5273860692977905\n",
            "\tBatch 3 loss: 0.5337191224098206\n",
            "\tBatch 4 loss: 0.6379135847091675\n",
            "\tBatch 5 loss: 0.5060160160064697\n",
            "\tBatch 6 loss: 0.5605292320251465\n",
            "\tBatch 7 loss: 0.47192078828811646\n",
            "\tBatch 8 loss: 0.6171318888664246\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 19, target_long sum: 17\n",
            "y_hat_long sum: 18, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.8999552290367203\n",
            "        AUPRC: 0.9075493216514587\n",
            "        Sensitivity: 0.7990299463272095\n",
            "        Specificity: 0.8092553019523621\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9035299355516708\n",
            "        AUPRC: 0.9131190776824951\n",
            "        Sensitivity: 0.781860888004303\n",
            "        Specificity: 0.8061688542366028\n",
            "====================================\n",
            "     Epoch #24\n",
            "====================================\n",
            "\tBatch 0 loss: 0.540097713470459\n",
            "\tBatch 1 loss: 0.5453150868415833\n",
            "\tBatch 2 loss: 0.5060952305793762\n",
            "\tBatch 3 loss: 0.514571487903595\n",
            "\tBatch 4 loss: 0.6237150430679321\n",
            "\tBatch 5 loss: 0.48014673590660095\n",
            "\tBatch 6 loss: 0.5526663064956665\n",
            "\tBatch 7 loss: 0.4371268153190613\n",
            "\tBatch 8 loss: 0.5707822442054749\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 17, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9096165155834309\n",
            "        AUPRC: 0.9152199625968933\n",
            "        Sensitivity: 0.7988736629486084\n",
            "        Specificity: 0.8145463466644287\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 17, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 19, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9059674847431809\n",
            "        AUPRC: 0.9114459156990051\n",
            "        Sensitivity: 0.778083086013794\n",
            "        Specificity: 0.8092553019523621\n",
            "====================================\n",
            "     Epoch #25\n",
            "====================================\n",
            "\tBatch 0 loss: 0.5129109621047974\n",
            "\tBatch 1 loss: 0.5320796370506287\n",
            "\tBatch 2 loss: 0.4859645366668701\n",
            "\tBatch 3 loss: 0.49320918321609497\n",
            "\tBatch 4 loss: 0.6200492978096008\n",
            "\tBatch 5 loss: 0.46122536063194275\n",
            "\tBatch 6 loss: 0.5245465636253357\n",
            "\tBatch 7 loss: 0.4072629511356354\n",
            "\tBatch 8 loss: 0.5563286542892456\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9074245551209363\n",
            "        AUPRC: 0.913905680179596\n",
            "        Sensitivity: 0.7802093625068665\n",
            "        Specificity: 0.8154281377792358\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 18, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9064246080279639\n",
            "        AUPRC: 0.9141232967376709\n",
            "        Sensitivity: 0.7802093625068665\n",
            "        Specificity: 0.8050857782363892\n",
            "====================================\n",
            "     Epoch #26\n",
            "====================================\n",
            "\tBatch 0 loss: 0.49690312147140503\n",
            "\tBatch 1 loss: 0.5002130270004272\n",
            "\tBatch 2 loss: 0.4583618640899658\n",
            "\tBatch 3 loss: 0.46201497316360474\n",
            "\tBatch 4 loss: 0.6252663731575012\n",
            "\tBatch 5 loss: 0.43490082025527954\n",
            "\tBatch 6 loss: 0.5175111889839172\n",
            "\tBatch 7 loss: 0.38453200459480286\n",
            "\tBatch 8 loss: 0.5224553942680359\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9108956653065243\n",
            "        AUPRC: 0.916391909122467\n",
            "        Sensitivity: 0.7933604717254639\n",
            "        Specificity: 0.8105972409248352\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.903133487921726\n",
            "        AUPRC: 0.9096946716308594\n",
            "        Sensitivity: 0.7993379235267639\n",
            "        Specificity: 0.8092553019523621\n",
            "====================================\n",
            "     Epoch #27\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4713056981563568\n",
            "\tBatch 1 loss: 0.4852832853794098\n",
            "\tBatch 2 loss: 0.43334245681762695\n",
            "\tBatch 3 loss: 0.45268183946609497\n",
            "\tBatch 4 loss: 0.6166642904281616\n",
            "\tBatch 5 loss: 0.4112322926521301\n",
            "\tBatch 6 loss: 0.5099173188209534\n",
            "\tBatch 7 loss: 0.3541201949119568\n",
            "\tBatch 8 loss: 0.4999852776527405\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9091484493691434\n",
            "        AUPRC: 0.9145628809928894\n",
            "        Sensitivity: 0.7830312848091125\n",
            "        Specificity: 0.8047492504119873\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.907201729012272\n",
            "        AUPRC: 0.9122954607009888\n",
            "        Sensitivity: 0.7875897884368896\n",
            "        Specificity: 0.7974799275398254\n",
            "====================================\n",
            "     Epoch #28\n",
            "====================================\n",
            "\tBatch 0 loss: 0.45703190565109253\n",
            "\tBatch 1 loss: 0.45623263716697693\n",
            "\tBatch 2 loss: 0.42124828696250916\n",
            "\tBatch 3 loss: 0.4326518177986145\n",
            "\tBatch 4 loss: 0.5946008563041687\n",
            "\tBatch 5 loss: 0.39314547181129456\n",
            "\tBatch 6 loss: 0.48073118925094604\n",
            "\tBatch 7 loss: 0.3258168697357178\n",
            "\tBatch 8 loss: 0.49800100922584534\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.915470572373176\n",
            "        AUPRC: 0.9206973314285278\n",
            "        Sensitivity: 0.7788721919059753\n",
            "        Specificity: 0.8044244050979614\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9155131609101826\n",
            "        AUPRC: 0.9178063869476318\n",
            "        Sensitivity: 0.8202460408210754\n",
            "        Specificity: 0.8047492504119873\n",
            "====================================\n",
            "     Epoch #29\n",
            "====================================\n",
            "\tBatch 0 loss: 0.44141101837158203\n",
            "\tBatch 1 loss: 0.44576215744018555\n",
            "\tBatch 2 loss: 0.411935955286026\n",
            "\tBatch 3 loss: 0.4177750051021576\n",
            "\tBatch 4 loss: 0.5975330471992493\n",
            "\tBatch 5 loss: 0.3745512068271637\n",
            "\tBatch 6 loss: 0.4692893922328949\n",
            "\tBatch 7 loss: 0.31565457582473755\n",
            "\tBatch 8 loss: 0.47539201378822327\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9173901206434635\n",
            "        AUPRC: 0.9202038645744324\n",
            "        Sensitivity: 0.7882551550865173\n",
            "        Specificity: 0.8071992993354797\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9175676107754693\n",
            "        AUPRC: 0.91779625415802\n",
            "        Sensitivity: 0.8040256500244141\n",
            "        Specificity: 0.8079267740249634\n",
            "====================================\n",
            "     Epoch #30\n",
            "====================================\n",
            "\tBatch 0 loss: 0.4238317012786865\n",
            "\tBatch 1 loss: 0.42817822098731995\n",
            "\tBatch 2 loss: 0.3993373513221741\n",
            "\tBatch 3 loss: 0.41644197702407837\n",
            "\tBatch 4 loss: 0.5928016304969788\n",
            "\tBatch 5 loss: 0.3553703725337982\n",
            "\tBatch 6 loss: 0.4571201205253601\n",
            "\tBatch 7 loss: 0.2850829064846039\n",
            "\tBatch 8 loss: 0.4402788281440735\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9223497176903298\n",
            "        AUPRC: 0.9256170988082886\n",
            "        Sensitivity: 0.7957514524459839\n",
            "        Specificity: 0.7952887415885925\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 24, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9205916544068877\n",
            "        AUPRC: 0.9243068695068359\n",
            "        Sensitivity: 0.8119109272956848\n",
            "        Specificity: 0.8133146166801453\n",
            "====================================\n",
            "     Epoch #31\n",
            "====================================\n",
            "\tBatch 0 loss: 0.40575703978538513\n",
            "\tBatch 1 loss: 0.40882283449172974\n",
            "\tBatch 2 loss: 0.3769167959690094\n",
            "\tBatch 3 loss: 0.38330045342445374\n",
            "\tBatch 4 loss: 0.6149474382400513\n",
            "\tBatch 5 loss: 0.348533570766449\n",
            "\tBatch 6 loss: 0.4583815932273865\n",
            "\tBatch 7 loss: 0.2713106572628021\n",
            "\tBatch 8 loss: 0.4211786091327667\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9256290322691668\n",
            "        AUPRC: 0.9285816550254822\n",
            "        Sensitivity: 0.8161505460739136\n",
            "        Specificity: 0.8216009736061096\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9238509482589762\n",
            "        AUPRC: 0.9263980388641357\n",
            "        Sensitivity: 0.7925508618354797\n",
            "        Specificity: 0.8276636004447937\n",
            "====================================\n",
            "     Epoch #32\n",
            "====================================\n",
            "\tBatch 0 loss: 0.40394920110702515\n",
            "\tBatch 1 loss: 0.39052993059158325\n",
            "\tBatch 2 loss: 0.37605565786361694\n",
            "\tBatch 3 loss: 0.39274030923843384\n",
            "\tBatch 4 loss: 0.5977856516838074\n",
            "\tBatch 5 loss: 0.3231494128704071\n",
            "\tBatch 6 loss: 0.4454365372657776\n",
            "\tBatch 7 loss: 0.26916977763175964\n",
            "\tBatch 8 loss: 0.38943150639533997\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.923293780403125\n",
            "        AUPRC: 0.924525260925293\n",
            "        Sensitivity: 0.8227059841156006\n",
            "        Specificity: 0.8080811500549316\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9255179297456037\n",
            "        AUPRC: 0.9279183149337769\n",
            "        Sensitivity: 0.8100029826164246\n",
            "        Specificity: 0.8207986950874329\n",
            "====================================\n",
            "     Epoch #33\n",
            "====================================\n",
            "\tBatch 0 loss: 0.38184699416160583\n",
            "\tBatch 1 loss: 0.38256606459617615\n",
            "\tBatch 2 loss: 0.3720191717147827\n",
            "\tBatch 3 loss: 0.36135175824165344\n",
            "\tBatch 4 loss: 0.5956486463546753\n",
            "\tBatch 5 loss: 0.3198111951351166\n",
            "\tBatch 6 loss: 0.41644731163978577\n",
            "\tBatch 7 loss: 0.25092363357543945\n",
            "\tBatch 8 loss: 0.3977125585079193\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9298979798138206\n",
            "        AUPRC: 0.9276628494262695\n",
            "        Sensitivity: 0.8074328899383545\n",
            "        Specificity: 0.8228327035903931\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9264019916782574\n",
            "        AUPRC: 0.9299004673957825\n",
            "        Sensitivity: 0.8425882458686829\n",
            "        Specificity: 0.8157530426979065\n",
            "====================================\n",
            "     Epoch #34\n",
            "====================================\n",
            "\tBatch 0 loss: 0.36873772740364075\n",
            "\tBatch 1 loss: 0.3749043345451355\n",
            "\tBatch 2 loss: 0.34286636114120483\n",
            "\tBatch 3 loss: 0.3531152606010437\n",
            "\tBatch 4 loss: 0.5901861190795898\n",
            "\tBatch 5 loss: 0.2955051064491272\n",
            "\tBatch 6 loss: 0.4149653911590576\n",
            "\tBatch 7 loss: 0.2443433701992035\n",
            "\tBatch 8 loss: 0.3669602572917938\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9366084554044496\n",
            "        AUPRC: 0.9346143007278442\n",
            "        Sensitivity: 0.8141166567802429\n",
            "        Specificity: 0.8095801472663879\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 22, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9322428934496172\n",
            "        AUPRC: 0.9293645024299622\n",
            "        Sensitivity: 0.8339041471481323\n",
            "        Specificity: 0.8392980098724365\n",
            "====================================\n",
            "     Epoch #35\n",
            "====================================\n",
            "\tBatch 0 loss: 0.36754292249679565\n",
            "\tBatch 1 loss: 0.3627736568450928\n",
            "\tBatch 2 loss: 0.33578401803970337\n",
            "\tBatch 3 loss: 0.3520323634147644\n",
            "\tBatch 4 loss: 0.6031167507171631\n",
            "\tBatch 5 loss: 0.29994362592697144\n",
            "\tBatch 6 loss: 0.41411834955215454\n",
            "\tBatch 7 loss: 0.23601683974266052\n",
            "\tBatch 8 loss: 0.35793250799179077\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9334972274834674\n",
            "        AUPRC: 0.9340277910232544\n",
            "        Sensitivity: 0.8142987489700317\n",
            "        Specificity: 0.8384660482406616\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9300985337680571\n",
            "        AUPRC: 0.9306390285491943\n",
            "        Sensitivity: 0.8129301071166992\n",
            "        Specificity: 0.8462731838226318\n",
            "====================================\n",
            "     Epoch #36\n",
            "====================================\n",
            "\tBatch 0 loss: 0.36972880363464355\n",
            "\tBatch 1 loss: 0.34340900182724\n",
            "\tBatch 2 loss: 0.3554220199584961\n",
            "\tBatch 3 loss: 0.3402032256126404\n",
            "\tBatch 4 loss: 0.6039629578590393\n",
            "\tBatch 5 loss: 0.2948702871799469\n",
            "\tBatch 6 loss: 0.4092229902744293\n",
            "\tBatch 7 loss: 0.23137831687927246\n",
            "\tBatch 8 loss: 0.39961346983909607\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9342961646714034\n",
            "        AUPRC: 0.934509813785553\n",
            "        Sensitivity: 0.8336307406425476\n",
            "        Specificity: 0.8144111037254333\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9324726064536354\n",
            "        AUPRC: 0.9316632747650146\n",
            "        Sensitivity: 0.7956344485282898\n",
            "        Specificity: 0.8144111037254333\n",
            "====================================\n",
            "     Epoch #37\n",
            "====================================\n",
            "\tBatch 0 loss: 0.34632936120033264\n",
            "\tBatch 1 loss: 0.34182876348495483\n",
            "\tBatch 2 loss: 0.3272763788700104\n",
            "\tBatch 3 loss: 0.3421041667461395\n",
            "\tBatch 4 loss: 0.5906454920768738\n",
            "\tBatch 5 loss: 0.2842285633087158\n",
            "\tBatch 6 loss: 0.39186030626296997\n",
            "\tBatch 7 loss: 0.21082703769207\n",
            "\tBatch 8 loss: 0.349656879901886\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 23, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9343020626985599\n",
            "        AUPRC: 0.931919515132904\n",
            "        Sensitivity: 0.7964224219322205\n",
            "        Specificity: 0.8354592323303223\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9363861074314968\n",
            "        AUPRC: 0.9349743723869324\n",
            "        Sensitivity: 0.8129301071166992\n",
            "        Specificity: 0.842941403388977\n",
            "====================================\n",
            "     Epoch #38\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3505795896053314\n",
            "\tBatch 1 loss: 0.3147304356098175\n",
            "\tBatch 2 loss: 0.3276650309562683\n",
            "\tBatch 3 loss: 0.3412100672721863\n",
            "\tBatch 4 loss: 0.6018750071525574\n",
            "\tBatch 5 loss: 0.2720131278038025\n",
            "\tBatch 6 loss: 0.39026397466659546\n",
            "\tBatch 7 loss: 0.2132374495267868\n",
            "\tBatch 8 loss: 0.3048141598701477\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9426085636670738\n",
            "        AUPRC: 0.9422325491905212\n",
            "        Sensitivity: 0.820276141166687\n",
            "        Specificity: 0.839763879776001\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 14, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9422427361324642\n",
            "        AUPRC: 0.9387847185134888\n",
            "        Sensitivity: 0.8175389766693115\n",
            "        Specificity: 0.8555324077606201\n",
            "====================================\n",
            "     Epoch #39\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3326621651649475\n",
            "\tBatch 1 loss: 0.30952587723731995\n",
            "\tBatch 2 loss: 0.33503231406211853\n",
            "\tBatch 3 loss: 0.3480437099933624\n",
            "\tBatch 4 loss: 0.5807898044586182\n",
            "\tBatch 5 loss: 0.26113712787628174\n",
            "\tBatch 6 loss: 0.4018990993499756\n",
            "\tBatch 7 loss: 0.19625884294509888\n",
            "\tBatch 8 loss: 0.3340538442134857\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9361394688148462\n",
            "        AUPRC: 0.9356534481048584\n",
            "        Sensitivity: 0.8249130845069885\n",
            "        Specificity: 0.8320727348327637\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9389322769339719\n",
            "        AUPRC: 0.9395716190338135\n",
            "        Sensitivity: 0.8337478637695312\n",
            "        Specificity: 0.8544052243232727\n",
            "====================================\n",
            "     Epoch #40\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3297193944454193\n",
            "\tBatch 1 loss: 0.3067370653152466\n",
            "\tBatch 2 loss: 0.31248071789741516\n",
            "\tBatch 3 loss: 0.33646783232688904\n",
            "\tBatch 4 loss: 0.5968549847602844\n",
            "\tBatch 5 loss: 0.25906163454055786\n",
            "\tBatch 6 loss: 0.36026841402053833\n",
            "\tBatch 7 loss: 0.20458146929740906\n",
            "\tBatch 8 loss: 0.3077736794948578\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 18, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9369504322414455\n",
            "        AUPRC: 0.934417188167572\n",
            "        Sensitivity: 0.8315937519073486\n",
            "        Specificity: 0.8453798294067383\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9362448211918493\n",
            "        AUPRC: 0.9334294199943542\n",
            "        Sensitivity: 0.8400299549102783\n",
            "        Specificity: 0.846138060092926\n",
            "====================================\n",
            "     Epoch #41\n",
            "====================================\n",
            "\tBatch 0 loss: 0.310249924659729\n",
            "\tBatch 1 loss: 0.30674347281455994\n",
            "\tBatch 2 loss: 0.3261305093765259\n",
            "\tBatch 3 loss: 0.31613868474960327\n",
            "\tBatch 4 loss: 0.5714409351348877\n",
            "\tBatch 5 loss: 0.25908395648002625\n",
            "\tBatch 6 loss: 0.38194718956947327\n",
            "\tBatch 7 loss: 0.1884789615869522\n",
            "\tBatch 8 loss: 0.31656986474990845\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 25, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 17, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.937899173883033\n",
            "        AUPRC: 0.9352991580963135\n",
            "        Sensitivity: 0.8247063159942627\n",
            "        Specificity: 0.8366583585739136\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 20, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.943189272221164\n",
            "        AUPRC: 0.9412679672241211\n",
            "        Sensitivity: 0.8124892711639404\n",
            "        Specificity: 0.8453913331031799\n",
            "====================================\n",
            "     Epoch #42\n",
            "====================================\n",
            "\tBatch 0 loss: 0.32345086336135864\n",
            "\tBatch 1 loss: 0.2985163629055023\n",
            "\tBatch 2 loss: 0.32061073184013367\n",
            "\tBatch 3 loss: 0.30522990226745605\n",
            "\tBatch 4 loss: 0.5791003108024597\n",
            "\tBatch 5 loss: 0.25887179374694824\n",
            "\tBatch 6 loss: 0.3558381199836731\n",
            "\tBatch 7 loss: 0.19695691764354706\n",
            "\tBatch 8 loss: 0.2668020725250244\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 25, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.946022257044334\n",
            "        AUPRC: 0.9433573484420776\n",
            "        Sensitivity: 0.83878093957901\n",
            "        Specificity: 0.8483560085296631\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 19, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9448428983397531\n",
            "        AUPRC: 0.9415054321289062\n",
            "        Sensitivity: 0.8369714617729187\n",
            "        Specificity: 0.8391274213790894\n",
            "====================================\n",
            "     Epoch #43\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3035673499107361\n",
            "\tBatch 1 loss: 0.27698826789855957\n",
            "\tBatch 2 loss: 0.3157745897769928\n",
            "\tBatch 3 loss: 0.3169941306114197\n",
            "\tBatch 4 loss: 0.5854333639144897\n",
            "\tBatch 5 loss: 0.2551666498184204\n",
            "\tBatch 6 loss: 0.370851993560791\n",
            "\tBatch 7 loss: 0.19506017863750458\n",
            "\tBatch 8 loss: 0.32528403401374817\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 11, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9475821523926982\n",
            "        AUPRC: 0.9429805278778076\n",
            "        Sensitivity: 0.8282347321510315\n",
            "        Specificity: 0.8197020888328552\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 20, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 9, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9440817262900839\n",
            "        AUPRC: 0.9403026103973389\n",
            "        Sensitivity: 0.8234438300132751\n",
            "        Specificity: 0.8618624806404114\n",
            "====================================\n",
            "     Epoch #44\n",
            "====================================\n",
            "\tBatch 0 loss: 0.2941048741340637\n",
            "\tBatch 1 loss: 0.29775479435920715\n",
            "\tBatch 2 loss: 0.30417245626449585\n",
            "\tBatch 3 loss: 0.32033535838127136\n",
            "\tBatch 4 loss: 0.5784574151039124\n",
            "\tBatch 5 loss: 0.2427477091550827\n",
            "\tBatch 6 loss: 0.367704302072525\n",
            "\tBatch 7 loss: 0.17950263619422913\n",
            "\tBatch 8 loss: 0.32741260528564453\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9426163783666655\n",
            "        AUPRC: 0.941786527633667\n",
            "        Sensitivity: 0.8434412479400635\n",
            "        Specificity: 0.8668420314788818\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9447772408077721\n",
            "        AUPRC: 0.9397960305213928\n",
            "        Sensitivity: 0.8139026165008545\n",
            "        Specificity: 0.8668420314788818\n",
            "====================================\n",
            "     Epoch #45\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3096926808357239\n",
            "\tBatch 1 loss: 0.263778418302536\n",
            "\tBatch 2 loss: 0.29553166031837463\n",
            "\tBatch 3 loss: 0.28504472970962524\n",
            "\tBatch 4 loss: 0.5808922052383423\n",
            "\tBatch 5 loss: 0.2509579062461853\n",
            "\tBatch 6 loss: 0.3385808765888214\n",
            "\tBatch 7 loss: 0.1774354726076126\n",
            "\tBatch 8 loss: 0.28821372985839844\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9471531111214097\n",
            "        AUPRC: 0.9455374479293823\n",
            "        Sensitivity: 0.845410168170929\n",
            "        Specificity: 0.8615509867668152\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 23, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.948195055469161\n",
            "        AUPRC: 0.9462856650352478\n",
            "        Sensitivity: 0.8498404026031494\n",
            "        Specificity: 0.8547167181968689\n",
            "====================================\n",
            "     Epoch #46\n",
            "====================================\n",
            "\tBatch 0 loss: 0.3007979691028595\n",
            "\tBatch 1 loss: 0.28622761368751526\n",
            "\tBatch 2 loss: 0.2894701361656189\n",
            "\tBatch 3 loss: 0.2870360016822815\n",
            "\tBatch 4 loss: 0.5581520795822144\n",
            "\tBatch 5 loss: 0.24446752667427063\n",
            "\tBatch 6 loss: 0.3361733853816986\n",
            "\tBatch 7 loss: 0.1588309407234192\n",
            "\tBatch 8 loss: 0.29155871272087097\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9510629643027093\n",
            "        AUPRC: 0.9481450319290161\n",
            "        Sensitivity: 0.8645387291908264\n",
            "        Specificity: 0.871471643447876\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9512859224850363\n",
            "        AUPRC: 0.9461638927459717\n",
            "        Sensitivity: 0.840217649936676\n",
            "        Specificity: 0.8505472540855408\n",
            "====================================\n",
            "     Epoch #47\n",
            "====================================\n",
            "\tBatch 0 loss: 0.28589928150177\n",
            "\tBatch 1 loss: 0.30362269282341003\n",
            "\tBatch 2 loss: 0.3023149073123932\n",
            "\tBatch 3 loss: 0.30406543612480164\n",
            "\tBatch 4 loss: 0.5755289196968079\n",
            "\tBatch 5 loss: 0.2294454276561737\n",
            "\tBatch 6 loss: 0.3225007653236389\n",
            "\tBatch 7 loss: 0.16644492745399475\n",
            "\tBatch 8 loss: 0.2828854024410248\n",
            "y_hat_long sum: 20, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 25, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9474305443041457\n",
            "        AUPRC: 0.9413825273513794\n",
            "        Sensitivity: 0.8670159578323364\n",
            "        Specificity: 0.8402708768844604\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9532593565682451\n",
            "        AUPRC: 0.9474031925201416\n",
            "        Sensitivity: 0.8374357223510742\n",
            "        Specificity: 0.8626474738121033\n",
            "====================================\n",
            "     Epoch #48\n",
            "====================================\n",
            "\tBatch 0 loss: 0.2669869661331177\n",
            "\tBatch 1 loss: 0.27287620306015015\n",
            "\tBatch 2 loss: 0.2839727997779846\n",
            "\tBatch 3 loss: 0.2775915563106537\n",
            "\tBatch 4 loss: 0.5511963367462158\n",
            "\tBatch 5 loss: 0.23044192790985107\n",
            "\tBatch 6 loss: 0.3559674620628357\n",
            "\tBatch 7 loss: 0.1623958796262741\n",
            "\tBatch 8 loss: 0.2960177958011627\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 21, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9520786878701338\n",
            "        AUPRC: 0.9482333064079285\n",
            "        Sensitivity: 0.8434412479400635\n",
            "        Specificity: 0.8716729283332825\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 20, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9563931491405278\n",
            "        AUPRC: 0.9526493549346924\n",
            "        Sensitivity: 0.8355838656425476\n",
            "        Specificity: 0.8771843314170837\n",
            "====================================\n",
            "     Epoch #49\n",
            "====================================\n",
            "\tBatch 0 loss: 0.2704758644104004\n",
            "\tBatch 1 loss: 0.26584023237228394\n",
            "\tBatch 2 loss: 0.31520920991897583\n",
            "\tBatch 3 loss: 0.27997085452079773\n",
            "\tBatch 4 loss: 0.586531937122345\n",
            "\tBatch 5 loss: 0.24528121948242188\n",
            "\tBatch 6 loss: 0.31344133615493774\n",
            "\tBatch 7 loss: 0.16601188480854034\n",
            "\tBatch 8 loss: 0.25708112120628357\n",
            "y_hat_long sum: 20, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 23, target_long sum: 24\n",
            "y_hat_long sum: 22, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 16, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9528879482185023\n",
            "        AUPRC: 0.9499600529670715\n",
            "        Sensitivity: 0.865164041519165\n",
            "        Specificity: 0.8551768064498901\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 20, target_long sum: 21\n",
            "y_hat_long sum: 22, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 24, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Test data metrics:\n",
            "        AUROC: 0.9557455565862397\n",
            "        AUPRC: 0.9526564478874207\n",
            "        Sensitivity: 0.8670191168785095\n",
            "        Specificity: 0.8571417927742004\n",
            "====================================\n",
            "     Epoch #50\n",
            "====================================\n",
            "\tBatch 0 loss: 0.25458478927612305\n",
            "\tBatch 1 loss: 0.27453333139419556\n",
            "\tBatch 2 loss: 0.2878008186817169\n",
            "\tBatch 3 loss: 0.2748916745185852\n",
            "\tBatch 4 loss: 0.5189120173454285\n",
            "\tBatch 5 loss: 0.24669179320335388\n",
            "\tBatch 6 loss: 0.3273248076438904\n",
            "\tBatch 7 loss: 0.14814820885658264\n",
            "\tBatch 8 loss: 0.26053696870803833\n",
            "y_hat_long sum: 18, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 16, target_long sum: 22\n",
            "y_hat_long sum: 19, target_long sum: 21\n",
            "y_hat_long sum: 24, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 22, target_long sum: 19\n",
            "y_hat_long sum: 15, target_long sum: 17\n",
            "y_hat_long sum: 10, target_long sum: 9\n",
            "    Train data metrics:\n",
            "        AUROC: 0.9571599552612877\n",
            "        AUPRC: 0.9531541466712952\n",
            "        Sensitivity: 0.8471001386642456\n",
            "        Specificity: 0.867723822593689\n",
            "y_hat_long sum: 19, target_long sum: 22\n",
            "y_hat_long sum: 21, target_long sum: 16\n",
            "y_hat_long sum: 17, target_long sum: 22\n",
            "y_hat_long sum: 18, target_long sum: 21\n",
            "y_hat_long sum: 24, target_long sum: 24\n",
            "y_hat_long sum: 21, target_long sum: 17\n",
            "y_hat_long sum: 21, target_long sum: 19\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c81e93f84bf1>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# TEST WITH RANDOM DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-57fe25927359>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_handle, test_data_handle, learning_rate, epochs, suspend_train_epochs_threshold, batch_size)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Not using performance metrics yet in this function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Potential TODO: stop training once desired performance is reached (TBD)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-0097cb82a268>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, eval_data, dataset_name, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mecg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c5a7eef6a396>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, abp, ecg, eeg)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m       \u001b[0mabp_o\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meeg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c5a7eef6a396>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m       \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresiduals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_ln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-c5a7eef6a396>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    304\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 306\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    307\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Get cases and find their data counts\n",
        "candy_cases = _get_candidate_cases()\n",
        "left_off = rare_candy_cases[-1]\n",
        "for case_id in candy_cases:\n",
        "  if int(case_id) <= int(left_off):\n",
        "    continue\n",
        "\n",
        "  # Download the case\n",
        "  _download_vital_file(case_id)\n",
        "\n",
        "  # Load the case\n",
        "  all_data = get_data(3, from_dir='.')\n",
        "\n",
        "  # Analyze positive vs negative ratio\n",
        "  length = len(torch.flatten(all_data[3]))\n",
        "  if length > 0:\n",
        "    pos = torch.sum(all_data[3])\n",
        "    if pos > 1:\n",
        "      rare_candy_cases.append(case_id)\n",
        "      case_distribution[case_id] = { 'positive': pos, 'negative': (length - pos) }\n",
        "\n",
        "  # Remove the case file\n",
        "  !rm ./*.vital\n"
      ],
      "metadata": {
        "id": "co7PHmJG0Cu_",
        "outputId": "ace8dd4e-d839-45ae-e47a-afe8cd569c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 152\n",
            "Statistics for case: 152, 428 total valid samples, 3 positive samples\n",
            "Getting track data for case: 156\n",
            "Statistics for case: 156, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 153\n",
            "Statistics for case: 153, 603 total valid samples, 6 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 156\n",
            "Statistics for case: 156, 2379 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 160\n",
            "Statistics for case: 160, 1277 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 161\n",
            "Statistics for case: 161, 1559 total valid samples, 43 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 163\n",
            "Statistics for case: 163, 265 total valid samples, 54 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 166\n",
            "Statistics for case: 166, 1466 total valid samples, 69 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 167\n",
            "Statistics for case: 167, 789 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 172\n",
            "Statistics for case: 172, 276 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 175\n",
            "Statistics for case: 175, 711 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 177\n",
            "Statistics for case: 177, 1751 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 178\n",
            "Statistics for case: 178, 1114 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 181\n",
            "Statistics for case: 181, 986 total valid samples, 17 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 183\n",
            "Statistics for case: 183, 259 total valid samples, 6 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 184\n",
            "Statistics for case: 184, 3096 total valid samples, 54 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 186\n",
            "Statistics for case: 186, 564 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 190\n",
            "Statistics for case: 190, 1171 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 191\n",
            "Statistics for case: 191, 691 total valid samples, 81 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 195\n",
            "Statistics for case: 195, 1430 total valid samples, 15 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 197\n",
            "Statistics for case: 197, 960 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 198\n",
            "Statistics for case: 198, 1257 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 199\n",
            "Statistics for case: 199, 267 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 200\n",
            "Statistics for case: 200, 677 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 202\n",
            "Statistics for case: 202, 2664 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 203\n",
            "Statistics for case: 203, 394 total valid samples, 130 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 206\n",
            "Statistics for case: 206, 642 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 207\n",
            "Statistics for case: 207, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 208\n",
            "Statistics for case: 208, 654 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 210\n",
            "Statistics for case: 210, 1509 total valid samples, 20 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 218\n",
            "Statistics for case: 218, 1237 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 221\n",
            "Statistics for case: 221, 217 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 222\n",
            "Statistics for case: 222, 627 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 229\n",
            "Statistics for case: 229, 1414 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 232\n",
            "Statistics for case: 232, 1016 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 233\n",
            "Statistics for case: 233, 958 total valid samples, 47 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 234\n",
            "Statistics for case: 234, 1232 total valid samples, 4 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 236\n",
            "Statistics for case: 236, 1764 total valid samples, 16 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 237\n",
            "Statistics for case: 237, 1859 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 239\n",
            "Statistics for case: 239, 1067 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 241\n",
            "Statistics for case: 241, 2333 total valid samples, 114 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 244\n",
            "Statistics for case: 244, 525 total valid samples, 138 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 247\n",
            "Statistics for case: 247, 1820 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 250\n",
            "Statistics for case: 250, 954 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 251\n",
            "Statistics for case: 251, 1663 total valid samples, 487 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 252\n",
            "Statistics for case: 252, 1551 total valid samples, 52 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 256\n",
            "Statistics for case: 256, 1166 total valid samples, 17 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 258\n",
            "Statistics for case: 258, 675 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 261\n",
            "Statistics for case: 261, 642 total valid samples, 27 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 263\n",
            "Statistics for case: 263, 673 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 266\n",
            "Statistics for case: 266, 1536 total valid samples, 9 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 268\n",
            "Statistics for case: 268, 418 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 269\n",
            "Statistics for case: 269, 921 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 270\n",
            "Statistics for case: 270, 184 total valid samples, 43 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 272\n",
            "Statistics for case: 272, 441 total valid samples, 10 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 279\n",
            "Statistics for case: 279, 569 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 281\n",
            "Statistics for case: 281, 843 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 282\n",
            "Statistics for case: 282, 757 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 283\n",
            "Statistics for case: 283, 1143 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 286\n",
            "Statistics for case: 286, 1002 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 287\n",
            "Statistics for case: 287, 657 total valid samples, 9 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 293\n",
            "Statistics for case: 293, 698 total valid samples, 12 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 295\n",
            "Statistics for case: 295, 912 total valid samples, 18 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 296\n",
            "Statistics for case: 296, 935 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 297\n",
            "Statistics for case: 297, 889 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 300\n",
            "Statistics for case: 300, 590 total valid samples, 42 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 302\n",
            "Statistics for case: 302, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 303\n",
            "Statistics for case: 303, 1452 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 304\n",
            "Statistics for case: 304, 1105 total valid samples, 17 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 306\n",
            "Statistics for case: 306, 1644 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 308\n",
            "Statistics for case: 308, 1551 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 309\n",
            "Statistics for case: 309, 666 total valid samples, 191 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 312\n",
            "Statistics for case: 312, 1113 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 316\n",
            "Statistics for case: 316, 355 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 318\n",
            "Statistics for case: 318, 613 total valid samples, 90 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 319\n",
            "Statistics for case: 319, 292 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 321\n",
            "Statistics for case: 321, 799 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 323\n",
            "Statistics for case: 323, 787 total valid samples, 106 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 325\n",
            "Statistics for case: 325, 796 total valid samples, 39 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 326\n",
            "Statistics for case: 326, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 327\n",
            "Statistics for case: 327, 1725 total valid samples, 97 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 330\n",
            "Statistics for case: 330, 572 total valid samples, 12 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 337\n",
            "Statistics for case: 337, 592 total valid samples, 45 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 338\n",
            "Statistics for case: 338, 809 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 342\n",
            "Statistics for case: 342, 1879 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 343\n",
            "Statistics for case: 343, 859 total valid samples, 14 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 345\n",
            "Statistics for case: 345, 1709 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 347\n",
            "Statistics for case: 347, 185 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 348\n",
            "Statistics for case: 348, 820 total valid samples, 7 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 349\n",
            "Statistics for case: 349, 856 total valid samples, 464 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 351\n",
            "Statistics for case: 351, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 353\n",
            "Statistics for case: 353, 1341 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 354\n",
            "Statistics for case: 354, 1005 total valid samples, 167 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 355\n",
            "Statistics for case: 355, 741 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 357\n",
            "Statistics for case: 357, 425 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 358\n",
            "Statistics for case: 358, 979 total valid samples, 1 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 359\n",
            "Statistics for case: 359, 1417 total valid samples, 184 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 362\n",
            "Statistics for case: 362, 1509 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 363\n",
            "Statistics for case: 363, 1340 total valid samples, 48 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 367\n",
            "Statistics for case: 367, 1159 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 369\n",
            "Statistics for case: 369, 1212 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 370\n",
            "Statistics for case: 370, 329 total valid samples, 32 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 371\n",
            "Statistics for case: 371, 683 total valid samples, 6 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 375\n",
            "Statistics for case: 375, 1395 total valid samples, 391 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 380\n",
            "Statistics for case: 380, 643 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 381\n",
            "Statistics for case: 381, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 382\n",
            "Statistics for case: 382, 1460 total valid samples, 9 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 383\n",
            "Statistics for case: 383, 430 total valid samples, 1 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 384\n",
            "Statistics for case: 384, 745 total valid samples, 19 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 386\n",
            "Statistics for case: 386, 0 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n",
            "Getting track data for case: 387\n",
            "Statistics for case: 387, 621 total valid samples, 26 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 388\n",
            "Statistics for case: 388, 1057 total valid samples, 12 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 390\n",
            "Statistics for case: 390, 1841 total valid samples, 24 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 397\n",
            "Statistics for case: 397, 1486 total valid samples, 451 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 398\n",
            "Statistics for case: 398, 738 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 402\n",
            "Statistics for case: 402, 1269 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 404\n",
            "Statistics for case: 404, 460 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 405\n",
            "Statistics for case: 405, 461 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 406\n",
            "Statistics for case: 406, 1606 total valid samples, 4 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 408\n",
            "Statistics for case: 408, 717 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 409\n",
            "Statistics for case: 409, 572 total valid samples, 69 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 413\n",
            "Statistics for case: 413, 434 total valid samples, 0 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 415\n",
            "Statistics for case: 415, 536 total valid samples, 39 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 416\n",
            "Statistics for case: 416, 511 total valid samples, 5 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 417\n",
            "Statistics for case: 417, 1576 total valid samples, 11 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 418\n",
            "Statistics for case: 418, 1567 total valid samples, 33 positive samples\n",
            "Max not reached but all available cases exhausted.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6f2b2df1b781>:197: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  hypotension_event_bools = BoolTensor(hypotension_event_bools).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting track data for case: 419\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-dd32d2310c12>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# Load the case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m# Analyze positive vs negative ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6f2b2df1b781>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(minutes_ahead, abp_and_ecg_sample_rate_per_second, eeg_sample_rate_per_second, max_num_samples, max_num_cases, from_dir)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0my_moving_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_numerator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mabp_data_in_two_seconds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mabp_data_in_two_seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mis_hypotension_event\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_moving_avg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m65\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0mhypotension_event_bools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_hypotension_event\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mcase_num_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcase_num_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/nanfunctions.py\u001b[0m in \u001b[0;36m_nanmax_dispatcher\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m def _nanmax_dispatcher(a, axis=None, out=None, keepdims=None,\n\u001b[0m\u001b[1;32m    365\u001b[0m                        initial=None, where=None):\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle the results of case analysis\n",
        "print(rare_candy_cases)\n",
        "\n",
        "afile = open(r'cases_with_positive_samples.pkl', 'wb')\n",
        "pickle.dump(rare_candy_cases, afile)\n",
        "afile.close()\n",
        "\n",
        "bfile = open(r'cases_with_positive_samples_statistics.pkl', 'wb')\n",
        "pickle.dump(case_distribution, bfile)\n",
        "bfile.close()"
      ],
      "metadata": {
        "id": "7mh4qd7mBzKS",
        "outputId": "0bce412a-3008-4f81-c354-eed433e8e5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '10', '12', '13', '16', '17', '19', '20', '24', '25', '27', '43', '49', '50', '52', '55', '58', '60', '61', '64', '66', '75', '79', '83', '84', '87', '92', '93', '94', '96', '97', '104', '105', '108', '111', '112', '116', '117', '118', '124', '135', '142', '143', '146', '148', '149', '152', '153', '161', '163', '166', '181', '183', '184', '191', '195', '197', '203', '208', '210', '233', '234', '236', '241', '244', '251', '252', '256', '261', '263', '266', '270', '272', '287', '293', '295', '300', '304', '309', '318', '323', '325', '327', '330', '337', '343', '348', '349', '354', '359', '363', '370', '371', '375', '382', '384', '387', '388', '390', '397', '406', '409', '415', '416', '417', '418']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(rare_candy_cases))"
      ],
      "metadata": {
        "id": "8hkXFQt7LyJA",
        "outputId": "d83a0bbc-e7fa-48b8-e457-86723dc0b755",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijm3trFE6UK6"
      },
      "outputs": [],
      "source": [
        "# Put together model as the paper describes (with all resnets)\n",
        "abp_resnet = WaveformResNet(\n",
        "    input_shape=30000,\n",
        "    output_size=32,\n",
        "    data_type='abp'\n",
        ")\n",
        "\n",
        "ecg_resnet = WaveformResNet(\n",
        "    input_shape=30000,\n",
        "    output_size=32,\n",
        "    data_type='ecg'\n",
        ")\n",
        "\n",
        "eeg_resnet = WaveformResNet(\n",
        "    input_shape=7680,\n",
        "    output_size=32,\n",
        "    data_type='eeg'\n",
        ")\n",
        "\n",
        "model = IntraoperativeHypotensionModel(\n",
        "    abp_resnet=abp_resnet,\n",
        "    ecg_resnet=ecg_resnet,\n",
        "    eeg_resnet=eeg_resnet\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFaXjyRytA24"
      },
      "source": [
        "## References\n",
        "Jo YY, Jang JH, Kwon Jm, Lee HC, Jung CW, et al. (2022) Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram: Retrospective study. PLOS ONE 17(8): e0272055. https://doi.org/10.1371/journal.pone.0272055\n",
        "\n",
        "## Acknowledgements\n",
        "* As mentioned in the introduction, this project leveraged the open [vitaldb dataset](https://vitaldb.net/dataset/), and without it would have been impossible in its current form.\n",
        "* Significant inspiration was drawn from [vital db examples](https://github.com/vitaldb/examples)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}